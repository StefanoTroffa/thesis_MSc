{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook was built with the purpose of understanding how to compute gradients and optimization routines with Graph Nets library. The compatibility between keras, tensorflow and Graph Nets is not fully supported. There are some grey areas which make all the standard implementation fails. Specifically, it is needed among other things to use the Adam optimizer of the Sonnet module. This was only discovered thanks to the notebook from the demo of graph nets 2 (the only example compatible with tensorflow2) called sort.py at the link:\n",
    "Minor adjustment in the source code are needed to run the notebook sort.ipynb. \n",
    "The pipeline for this notebook was: \n",
    "0) Understanding how the custom gradient implementation of tensorflow behaves\n",
    "1) taking the most naive version from the implementation of the main code\n",
    "2) Comparing step by step the sort.ipynb notebook with the structure needed for our work\n",
    "3) Finding out how to compute gradients (None values were ubiquitos initially)\n",
    "4) subsequently understanding how to apply the optimizer to this naive version \n",
    "5) Increasing the complexity in order to implement IT-SWO \n",
    "    Point 5 can be subsequently divided in subtasks:\n",
    "        5.1)\n",
    "        5.2)\n",
    "        5.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: \n",
    "Of course there are plenty of other examples, this is just a representative one for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 15:58:47.887063: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-24 15:58:47.892900: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-24 15:58:47.943764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 15:58:48.679160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-24 15:58:49.984806: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 15:58:49.985847: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "@tf.custom_gradient\n",
    "def custom_relu2(x):\n",
    "    def relu_grad(dy):\n",
    "        grad=  tf.cast( x > 0, dtype=tf.float32)*dy\n",
    "        return grad\n",
    "    return tf.maximum(x, 0.), relu_grad\n",
    "data = tf.Variable([0.5, -0.2, 0.0, -2.5, 3.0], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 0. 0. 2. 3.], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.94]\n",
      "tf.Tensor([0.94 0.   0.   1.88 2.82], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.88]\n",
      "tf.Tensor([0.88      0.        0.        1.76      2.6399999], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.82]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_relu(x):\n",
    "    def relu_forward(x):\n",
    "        return tf.maximum(x, 0)\n",
    "    \n",
    "    def relu_grad(dy):\n",
    "        grad = tf.cast(x > 0, dtype=tf.float32) * dy\n",
    "        return grad\n",
    "\n",
    "    return relu_forward(x), relu_grad\n",
    "\n",
    "# Define a trainable variable\n",
    "w = tf.Variable([1.0], dtype=tf.float32)  # A simple weight variable\n",
    "\n",
    "# Define an input\n",
    "x = tf.constant([1.0, -1.0, -2.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for step in range(3):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = custom_relu(w * x)  # Apply ReLU to a linear transformation of x\n",
    "        print(y, type(y))\n",
    "        # Compute gradients of y with respect to w\n",
    "        gradients = tape.gradient(y, w)\n",
    "        print(gradients)\n",
    "        # Apply gradients to update w\n",
    "        optimizer.apply_gradients(zip([gradients], [w]))\n",
    "\n",
    "        print(\"Updated w:\", w.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "from graph_nets import modules\n",
    "\n",
    "hidden_layer_size=4   #This will be 128\n",
    "output_emb_size=4      #This has to be 64 at the end\n",
    "# Define the MLP model\n",
    "class MLPModel_glob(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_glob, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='Glob_layer')\n",
    "       \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "        \n",
    "        return out\n",
    "class MLPModel_enc(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_enc, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='enc_layer')\n",
    "        \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the Encoder layer\n",
    "class Encoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_enc,\n",
    "            node_model_fn=MLPModel_enc,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Encoder, self).__call__(inputs)\n",
    "\n",
    "    \n",
    "class MLPModel_dec(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_dec, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='dec_layer')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Decoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_dec,\n",
    "            node_model_fn=MLPModel_dec,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Decoder, self).__call__(inputs)    \n",
    "class PoolingLayer_double(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingLayer_double, self).__init__()\n",
    "        self.linear = snt.Linear(output_size=2, name='linear_pool')\n",
    "        self.global_transform = snt.Linear(output_size=2, name='global_transform')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Sum-pooling over nodes and edges\n",
    "        pooled_nodes = tf.reduce_sum(inputs.nodes, axis=0)\n",
    "        pooled_edges = tf.reduce_sum(inputs.edges, axis=0)\n",
    "        pooled_features = tf.concat([pooled_nodes, pooled_edges], axis=0)\n",
    "        \n",
    "        transformed = self.linear(tf.expand_dims(pooled_features, axis=0))\n",
    "        \n",
    "        # Transform globals to match the shape of transformed\n",
    "        transformed_globals = self.global_transform(0.05 *inputs.globals)\n",
    "        #### THIS IS THE MOST RELEVANT PART, why again I can not use elu here? Is something related to the metric as well\n",
    "        out = tf.nn.elu(transformed + transformed_globals)\n",
    "        \n",
    "        return out    \n",
    "class GNN_double_output(snt.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN_double_output, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooling_layer = PoolingLayer_double()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "\n",
    "        output = self.pooling_layer(encoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.12018156051635742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefanotroffa/miniconda3/envs/gnets2/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "from compgraph.useful import node_to_index\n",
    "# Set a random seed\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = node_to_index(G)\n",
    "\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding = np.zeros((num_sites, 2))  # Two sublattices\n",
    "sublattice_encoding[::2, 0] = 1  # Sublattice 1\n",
    "sublattice_encoding[1::2, 1] = 1  # Sublattice 2\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "# Use the dictionary to set the node attributes in the graph\n",
    "#nx.set_node_attributes(G, node_dict)\n",
    "# Add 'features' to nodes\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # Replace with your actual edge features\n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge # Replace with your actual edge features\n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of configurations to generate\n",
    "n_configs = 6\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], len(basis_configs), axis=0)], axis=2)\n",
    "\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "bias_value=0.5\n",
    "# Create a list of graph dicts\n",
    "graph_tuples = []\n",
    "for i in range(n_graph):\n",
    "    graph_dict = {\n",
    "        'globals': np.array([0.05]),\n",
    "        'nodes': node_features[i],\n",
    "        'edges': np.full((edge_index_duplicated.shape[1], 1), bias_value),\n",
    "        'senders': edge_index_duplicated[0],\n",
    "        'receivers': edge_index_duplicated[1]\n",
    "    }\n",
    "    \n",
    "    # Convert to a GraphsTuple and append to the list\n",
    "    graph_tuples.append(utils_tf.data_dicts_to_graphs_tuple([graph_dict]))\n",
    "\n",
    "\n",
    "print(\"end time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs are equal: True\n"
     ]
    }
   ],
   "source": [
    "from useful import create_graph_tuples, compare_graph_tuples\n",
    "graph_tuples2=create_graph_tuples(basis_configs,G,sublattice_encoding)\n",
    "\n",
    "\n",
    "# Usage:\n",
    "result = compare_graph_tuples(graph_tuples2, graph_tuples)\n",
    "print(\"Graphs are equal:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.8513225677819865, shape=(), dtype=float64) tf.Tensor(1.0092861107726787, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "simple_gnn = GNN_double_output()\n",
    "for i in range(1):\n",
    "    a,b =simple_gnn(graph_tuples[i])[0]\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_loss_function(amplitude, phase):\n",
    "    \"\"\"A mock loss function for illustration purposes h\"\"\"\n",
    "    return tf.abs(1- phase*amplitude**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 to 4\n",
    "Most of the work here was done by trial and error and a lot of print statements among other strategies. \n",
    "The end result looks decently clean but the hidden work behind was quite intensive.\n",
    "Observations: sonnet modules seems to not be compatible with keras optimizers built on tensorflow. We need to use the sonnet optimizers.\n",
    "Furthermore, the sonnet optimizer does not allow for a learning schedule with Exponential decay as was intended to be used initially. \n",
    "The only useful implementation is of the standard Adam optimization routine. (Trivial SGD is inadequate for so many parameters, ca va sans dire)\n",
    "No fancy version of Adam are available, maybe it is for the best though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.optimizers.schedules.learning_rate_schedule.ExponentialDecay object at 0x7f109da0c560>\n"
     ]
    }
   ],
   "source": [
    "import sonnet as snt\n",
    "initial_learning_rate = 7e-3\n",
    "decay_steps = 8 * 1e5\n",
    "decay_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "print(lr_schedule)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.99, clipnorm=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " tf.Tensor([1.85 1.01], shape=(2,), dtype=float64)\n",
      "tf.Tensor(1.8513225677819865, shape=(), dtype=float64) tf.Tensor(1.0092861107726787, shape=(), dtype=float64) types <class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Is it lossing: \n",
      " tf.Tensor(2.459222421931945, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: 2.459222421931945\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 7e-3\n",
    "optimizer = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "\n",
    "def train_step(model, input_graph, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(input_graph)[0]\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "        amplitude, phase = output\n",
    "        print(amplitude, phase, \"types\", type(amplitude), type(phase))\n",
    "            \n",
    "        loss = mock_loss_function(amplitude,phase)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "# Training loop\n",
    "for step in range(1):\n",
    "    #print(graph_tuples[0])\n",
    "    outputs, loss = train_step(simple_gnn, graph_tuples[0], optimizer)\n",
    "    if step % 1 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "### 5.1\n",
    "In this section of the notebook we are going to implement the real loss function and attempt to train a simplified model of our GNN on a 2x2 square lattice. \n",
    "The following implementation will rely on scipy sparse.\n",
    "This implementation is suboptimal and is done for educational purpose. The reason is that implementing a Sparse Matrix is resource intensive regardless. The key reasoning is that one does not need to fully implement the matrix, just vector matrix multiplications, and those follow some specific patterns due to ortoghonality, plase look at step 5.2 for an implementation that does not need explicitly the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell block we generate the input graph and initialize the edges and node values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.08770251274108887\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "from compgraph.useful import neel_state, create_graph_tuples\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN, here they are set to 4 because the \n",
    "#total number of states is 2**4, and any value above is equivalent to optimizing the GNN on the whole hilbert space.\n",
    "# We are interested in the main pipeline, and want to generalize the network to larger scales thereafter\n",
    "batch_size= 4\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "# Create a square lattice with periodic boundary conditions. This is expecially useful in larger graphs where\n",
    "# having periodic boundary conditions allows for a more decent approximation of an infinite lattice, for small size graphs\n",
    "# periodic boundary conditions have a little effect.\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding= neel_state(G)\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "\n",
    "# Add 'features' to nodes, in this case the feature is a one hot vectore that represents the sublattice encoding of each node\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # \n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge \n",
    "\n",
    "\n",
    "# Number of batches of configurations to generate, here is set to 4 because 4x4=2**4= Hilbert space, and we don't need more than that\n",
    "n_configs = 4\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_tuples=create_graph_tuples(basis_configs, G, sublattice_encoding)\n",
    "print(\"end time:\", time.time()-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (0, 1), (1, 3), (2, 3)]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(G.edges)\n",
    "print(G.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from compgraph.gnn_src_code import GNN_double_output\n",
    "\n",
    "from compgraph.sparse_ham import create_spin_operators_qutipv5, construct_sparse_hamiltonian\n",
    "from compgraph.useful import sites_to_sparse\n",
    "\n",
    "configurations, value_list= sites_to_sparse(basis_configs)\n",
    "# Uncomment the following line to see the distribution of the initial random configurations\n",
    "#plt.plot(np.sort(value_list))\n",
    "#Here we noticed that the second environment created is not compatible for some obscure reasons, probably qutip compatibility. We are gonna stick to this one for now. ASK patrick\n",
    "\n",
    "J2=2.0\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, 0)\n",
    "less_trivial_gnn=GNN_double_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>\n",
      " <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      " \twith 1 stored elements in Compressed Sparse Row format>]\n",
      "4\n",
      "10\n",
      "7\n",
      "9\n",
      "13\n",
      "1\n",
      "7\n",
      "12\n",
      "9\n",
      "0\n",
      "tf.Tensor(\n",
      "[[ 4  0]\n",
      " [10  0]\n",
      " [ 7  0]\n",
      " [ 9  0]\n",
      " [13  0]\n",
      " [ 1  0]\n",
      " [ 7  0]\n",
      " [12  0]\n",
      " [ 9  0]\n",
      " [ 0  0]], shape=(10, 2), dtype=int64)\n",
      "[[-1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [-1  1  1  1]\n",
      " [ 1 -1 -1  1]\n",
      " [ 1  1 -1  1]\n",
      " [-1 -1 -1  1]\n",
      " [-1  1  1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1 -1 -1  1]\n",
      " [-1 -1 -1 -1]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m expected_energy \u001b[38;5;241m=\u001b[39m psi_total\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m@\u001b[39m Hamiltonian \u001b[38;5;241m@\u001b[39m psi_total\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Check if energies are close\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massertTrue(np\u001b[38;5;241m.\u001b[39mallclose(computed_energy\u001b[38;5;241m.\u001b[39mnumpy(), expected_energy))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: Computed energy matches expected energy.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "n, m = 2, 2\n",
    "num_sites = n * m\n",
    "lattice_size = (n, m)\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "mapping = node_to_index(G)\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "# Generate random configurations and amplitudes\n",
    "num_configs = 10  # Define how many random states you want to test\n",
    "configurations = np.random.choice([-1, 1], size=(num_configs, num_sites))\n",
    "amplitudes = np.random.rand(num_configs) + 1j * np.random.rand(num_configs)  # Random complex amplitudes\n",
    "\n",
    "# Create sparse vector using TensorFlow\n",
    "sparse_indices = sites_to_sparse(configurations)[0] # Get indices from your sites_to_sparse function\n",
    "indices = [[idx.indices[0], 0] for idx in sparse_indices]  # Format indices for tf.sparseTensor\n",
    "\n",
    "values_tensor = tf.stack(amplitudes, axis=0)\n",
    "indices_tensor = tf.constant(indices, dtype=tf.int64)\n",
    "print(indices_tensor)\n",
    "sparse_tensor = tf.sparse.SparseTensor(indices=indices_tensor, values=values_tensor, dense_shape=[2**num_sites, 1])\n",
    "\n",
    "# Compute energy using sparse_tensor_exp_energy\n",
    "J2 = 1.0  # Example J2 value\n",
    "computed_energy = sparse_tensor_exp_energy(sparse_tensor, G, J2)\n",
    "\n",
    "# Convert configurations to states and compute expected energy using Hamiltonian\n",
    "states = config_list_to_state_list(configurations)\n",
    "scaled_states = [amp * state for amp, state in zip(amplitudes, states)]\n",
    "psi_total = sum(scaled_states)  # Superposition of all states\n",
    "Hamiltonian = qu.ham_heis_2D(n, m, j=1.0, bz=0, cyclic=True)\n",
    "expected_energy = psi_total.H @ Hamiltonian @ psi_total\n",
    "\n",
    "# Check if energies are close\n",
    "self.assertTrue(np.allclose(computed_energy.numpy(), expected_energy))\n",
    "print(\"Test passed: Computed energy matches expected energy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(0.2532032102385702+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([ 0.01+0.25j  0.01+0.25j  0.  +0.28j -0.04+0.23j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9984529390193818+1.734723475976807e-18j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9984529390193818-1.734723475976807e-18j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: (-0.9984529390193818-1.734723475976807e-18j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([0.14+1.53j 0.14+1.53j 0.12+1.52j 0.02+1.55j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9983045617348313+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9983045617348313-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: (-0.9983045617348313-0j)\n",
      "4 8 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(60.91284703084328+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([0.51+2.86j 0.19+2.74j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9996666670912837-3.686287386450715e-18j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9996666670912837+3.686287386450715e-18j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: (-0.9996666670912837+3.686287386450715e-18j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([0.89+4.42j 0.57+4.32j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9996696384855851+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9996696384855851-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: (-0.9996696384855851-0j)\n",
      "8 12 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(294.14670190136843+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([1.17+6.27j 1.92+7.07j 1.97+7.17j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9992894696055656+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9992894696055656-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: (-0.9992894696055656-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([2.13 +8.87j 3.18 +9.94j 3.32+10.14j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9992900264495945+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9992900264495945-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: (-0.9992900264495945-0j)\n",
      "12 16 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(727.1103287538044+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([4.4 +13.1j  4.62+13.21j 4.62+13.21j 4.12+12.75j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9989613795072374-2.8650042407929455e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9989613795072374+2.8650042407929455e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: (-0.9989613795072374+2.8650042407929455e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([6.88+18.29j 7.13+18.38j 7.13+18.38j 6.59+17.86j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9989736692897578+1.9136168344369153e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9989736692897578-1.9136168344369153e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: (-0.9989736692897578-1.9136168344369153e-17j)\n",
      "0 4 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(3142.760211298451+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([10.8 +25.77j 10.8 +25.77j 10.5 +25.63j 11.82+27.4j ], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9983387675015883-2.6806898714704097e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9983387675015883+2.6806898714704097e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: (-0.9983387675015883+2.6806898714704097e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([16.06+36.57j 16.06+36.57j 15.74+36.36j 17.35+38.64j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9983367517350175+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9983367517350175-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: (-0.9983367517350175-0j)\n",
      "4 8 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(23104.58271572306+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([22.53+50.67j 22.03+50.12j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.999671371576563+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.999671371576563-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: (-0.999671371576563-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([33.03+71.45j 32.41+70.67j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9996713787433753+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9996713787433753-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: (-0.9996713787433753-0j)\n",
      "8 12 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(75947.2860122157+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([46.49 +97.9j  50.8 +104.33j 52.07+106.46j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9992868459425055+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9992868459425055-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: (-0.9992868459425055-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([66.22+134.93j 71.52+143.2j  73.25+146.18j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.999286627160779+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.999286627160779-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: (-0.999286627160779-0j)\n",
      "12 16 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(176882.80826915515+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([97.89+192.08j 98.44+192.73j 98.44+192.73j 96.66+190.06j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.998997367815669+2.0078068981715935e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.998997367815669-2.0078068981715935e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: (-0.998997367815669-2.0078068981715935e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([138.59+264.52j 139.28+265.39j 139.28+265.39j 137.28+262.39j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.999000235511562-4.1809546276472265e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.999000235511562+4.1809546276472265e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: (-0.999000235511562+4.1809546276472265e-17j)\n",
      "0 4 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(690657.3501144191+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([197.78+366.7j  197.78+366.7j  196.87+365.44j 205.93+380.75j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9983332801802185-2.0827269623562364e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9983332801802185+2.0827269623562364e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: (-0.9983332801802185+2.0827269623562364e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([283.79+512.93j 283.79+512.93j 282.7 +511.31j 295.19+532.33j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9983335220682961+2.1043686541585838e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9983335220682961-2.1043686541585838e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: (-0.9983335220682961-2.1043686541585838e-17j)\n",
      "4 8 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(4954669.272884695+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([399.12+705.1j  395.69+699.36j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9996714089223597+4.5832529259456564e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9996714089223597-4.5832529259456564e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: (-0.9996714089223597-4.5832529259456564e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([561.8 +969.94j 557.55+962.91j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9996714135894433-7.177545436796127e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9996714135894433+7.177545436796127e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: (-0.9996714135894433+7.177545436796127e-17j)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#reload(compgraph.tensor_wave_functions)\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr,  innerprod_sparse\n",
    "from compgraph.tensor_wave_functions import adjust_dtype_and_multiply, convert_csr_to_sparse_tensor, evolving_function, variational_wave_function_on_batch\n",
    "\n",
    "#THE inner training step should work for one configuration at a time if logic has any ground on reality\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "optimizer_snt = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "Hamiltonian_tensor = convert_csr_to_sparse_tensor(Hamiltonian)\n",
    "beta=0.005\n",
    "def evolving_function(wave, Ham_tensor,beta):\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    beta *= -1\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, beta)\n",
    "    #print(wave.indices,phi.indices)\n",
    "    \n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    #print(phi)\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    overlap=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    normalization=1/(norm_wave*norm_ito_wave)\n",
    "    #print(normalization, 'This is the coefficient at denominator')\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    loss=tf.reduce_sum(overlap_normalized.values)\n",
    "    print('Lossing final', loss)\n",
    "    return -loss\n",
    "def sparse_rep_inner_training_step(model, graph_batch, graph_batch_indices, target_phi, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = variational_wave_function_on_batch(model, graph_batch, graph_batch_indices)\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "            \n",
    "        loss = evolving_function(output, Hamiltonian_tensor, beta)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer_snt.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "start=0\n",
    "for step in range(10):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "    start =start%len(graph_tuples)\n",
    "    end=(start + batch_size)\n",
    "    graph_tuples_batch=graph_tuples[start:end]\n",
    "    graph_tuples_batch_indices= configurations[start:end]\n",
    "    print(start,end, len(configurations),len(graph_tuples))\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "    \n",
    "    beta = 0.05 #This parameter determines the amount of imaginary time evolution at each outer step\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    print(\"Inner product scipy sparse\", innerprod_sparse(phi_csr, phi_csr))\n",
    "    for innerstep in range(2):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "\n",
    "        outputs, loss = sparse_rep_inner_training_step(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices, phi_csr, optimizer)\n",
    "       \n",
    "        #print(outputs,loss)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "        \n",
    "        \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.b\n",
    "In the following we are taking the full Hilbert as configurations and graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 [[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import compgraph\n",
    "from compgraph.useful import sites_to_sparse\n",
    "from compgraph.useful import create_graph_from_ham\n",
    "\n",
    "lattice_size = (2, 2)\n",
    "sublattice_encoding = neel_state(nx.grid_2d_graph(*lattice_size, periodic=True)) \n",
    "G=create_graph_from_ham(geometric_structure=\"2d_square\", lattice_size=lattice_size, sub_lattice_encoding=sublattice_encoding)\n",
    "\n",
    "# Number of batches of configurations to generate, here is set to full Hilbert space\n",
    "full_size_configs = 2**(len(G.nodes))\n",
    "# Generate the basis configurations\n",
    "print(full_size_configs, sublattice_encoding)\n",
    "full_basis_configs = np.array([[int(x) for x in format(i, f'0{len(G.nodes)}b')] for i in range(full_size_configs)]) * 2 - 1\n",
    "\n",
    "\n",
    "#Initialization of edges, it should be 0, we set it to 0.05.\n",
    "bias_value=0.05\n",
    "# Create a list of graph dicts\n",
    "full_graph_tuples =create_graph_tuples(full_basis_configs, G, sublattice_encoding, 0.05, 0.5)\n",
    "\n",
    "full_configurations, value_list= sites_to_sparse(full_basis_configs)\n",
    "\n",
    "#for idx, config in enumerate(full_configurations):\n",
    "#    print(np.array(config), full_basis_configs[idx], value_list[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. -1.] [-1 -1  1 -1]\n",
      "  (0, 8)\t1\n",
      "[-1 -1  1 -1]\n",
      "2\n",
      "[-1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from compgraph.useful import graph_tuple_toconfig\n",
    "from compgraph.useful import sites_to_sparse\n",
    "from compgraph.useful import neel_state, config_list_to_state_list, graph_tuple_list_to_configs_list, config_to_state\n",
    "from compgraph.tensor_wave_functions import graph_tuple_to_config_hamiltonian_product\n",
    "import quimb as qu\n",
    "full_configurations_indices, valuelist= sites_to_sparse(full_basis_configs)\n",
    "\n",
    "config_from_batch=graph_tuple_toconfig(full_graph_tuples[2])\n",
    "\n",
    "print(config_from_batch,full_basis_configs[2])\n",
    "print(graph_tuples_batch_indices[2])\n",
    "print(full_basis_configs[2])\n",
    "print(sites_to_sparse([config_from_batch])[0][0].indices[0])\n",
    "\n",
    "print(config_from_batch)\n",
    "sparse, vt= sites_to_sparse([config_from_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f100cc990d0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((value_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE   (0, 0)\t(34623639.92992976+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[745.79+1262.5j  772.51+1306.1j  766.9 +1296.94j 789.57+1333.82j\n",
      " 772.51+1306.1j  787.09+1329.55j 793.62+1340.54j 804.15+1357.27j\n",
      " 766.9 +1296.94j 793.62+1340.54j 787.8 +1331.49j 810.47+1368.36j\n",
      " 789.57+1333.82j 804.15+1357.27j 810.47+1368.36j 821.01+1385.09j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-338.34+0.j   -362.42+0.j   -357.18-0.j   -377.96-0.j   -362.42+0.j\n",
      " -375.49-0.j   -382.15+0.j   -391.56+0.j   -357.18-0.j   -382.15+0.j\n",
      " -376.51-0.01j -398.1 -0.j   -377.96-0.j   -391.56+0.j   -398.1 -0.j\n",
      " -407.96-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: [-338.34+0.j   -362.42+0.j   -357.18-0.j   -377.96-0.j   -362.42+0.j\n",
      " -375.49-0.j   -382.15+0.j   -391.56+0.j   -357.18-0.j   -382.15+0.j\n",
      " -376.51-0.01j -398.1 -0.j   -377.96-0.j   -391.56+0.j   -398.1 -0.j\n",
      " -407.96-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[749.11+1268.05j 775.95+1311.85j 770.31+1302.65j 793.08+1339.69j\n",
      " 775.95+1311.85j 790.6 +1335.41j 797.14+1346.44j 807.72+1363.26j\n",
      " 770.31+1302.65j 797.14+1346.44j 791.3 +1337.34j 814.07+1374.38j\n",
      " 793.08+1339.69j 807.72+1363.26j 814.07+1374.38j 824.66+1391.2j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-339.84+0.j -364.02+0.j -358.75-0.j -379.63-0.j -364.02+0.j -377.16-0.j\n",
      " -383.84+0.j -393.3 +0.j -358.75-0.j -383.84+0.j -378.16-0.j -399.85-0.j\n",
      " -379.63-0.j -393.3 +0.j -399.85-0.j -409.77-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: [-339.84+0.j -364.02+0.j -358.75-0.j -379.63-0.j -364.02+0.j -377.16-0.j\n",
      " -383.84+0.j -393.3 +0.j -358.75-0.j -383.84+0.j -378.16-0.j -399.85-0.j\n",
      " -379.63-0.j -393.3 +0.j -399.85-0.j -409.77-0.j]\n",
      "WE   (0, 0)\t(35238750.698218934+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[752.45+1273.64j 779.4 +1317.62j 773.73+1308.37j 796.6 +1345.58j\n",
      " 779.4 +1317.62j 794.12+1341.31j 800.68+1352.36j 811.32+1369.26j\n",
      " 773.73+1308.37j 800.68+1352.36j 794.81+1343.22j 817.68+1380.42j\n",
      " 796.6 +1345.58j 811.32+1369.26j 817.68+1380.42j 828.32+1397.33j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-341.34-0.j -365.63+0.j -360.33-0.j -381.3 -0.j -365.63+0.j -378.83-0.j\n",
      " -385.53+0.j -395.04+0.j -360.33-0.j -385.53+0.j -379.83-0.j -401.61-0.j\n",
      " -381.3 -0.j -395.04+0.j -401.61-0.j -411.58-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: [-341.34-0.j -365.63+0.j -360.33-0.j -381.3 -0.j -365.63+0.j -378.83-0.j\n",
      " -385.53+0.j -395.04+0.j -360.33-0.j -385.53+0.j -379.83-0.j -401.61-0.j\n",
      " -381.3 -0.j -395.04+0.j -401.61-0.j -411.58-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[755.8 +1279.24j 782.86+1323.42j 777.17+1314.13j 800.13+1351.5j\n",
      " 782.86+1323.42j 797.65+1347.22j 804.23+1358.31j 814.92+1375.3j\n",
      " 777.17+1314.13j 804.23+1358.31j 798.34+1349.12j 821.31+1386.49j\n",
      " 800.13+1351.5j  814.92+1375.3j  821.31+1386.49j 832.  +1403.48j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-342.85-0.j -367.24+0.j -361.92-0.j -382.98-0.j -367.24+0.j -380.51-0.j\n",
      " -387.22+0.j -396.79+0.j -361.92-0.j -387.22+0.j -381.5 -0.j -403.38-0.j\n",
      " -382.98-0.j -396.79+0.j -403.38-0.j -413.4 -0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: [-342.85-0.j -367.24+0.j -361.92-0.j -382.98-0.j -367.24+0.j -380.51-0.j\n",
      " -387.22+0.j -396.79+0.j -361.92-0.j -387.22+0.j -381.5 -0.j -403.38-0.j\n",
      " -382.98-0.j -396.79+0.j -403.38-0.j -413.4 -0.j]\n",
      "WE   (0, 0)\t(35864571.138669804+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[759.16+1284.87j 786.34+1329.24j 780.62+1319.9j  803.69+1357.44j\n",
      " 786.34+1329.24j 801.2 +1353.16j 807.8 +1364.28j 818.55+1381.36j\n",
      " 780.62+1319.9j  807.8 +1364.28j 801.89+1355.04j 824.96+1392.58j\n",
      " 803.69+1357.44j 818.55+1381.36j 824.96+1392.58j 835.7 +1409.66j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-344.36+0.j -368.86+0.j -363.51-0.j -384.67-0.j -368.86+0.j -382.2 -0.j\n",
      " -388.93+0.j -398.54+0.j -363.51-0.j -388.93+0.j -383.18-0.j -405.15-0.j\n",
      " -384.67-0.j -398.54+0.j -405.15-0.j -415.23-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: [-344.36+0.j -368.86+0.j -363.51-0.j -384.67-0.j -368.86+0.j -382.2 -0.j\n",
      " -388.93+0.j -398.54+0.j -363.51-0.j -388.93+0.j -383.18-0.j -405.15-0.j\n",
      " -384.67-0.j -398.54+0.j -405.15-0.j -415.23-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[762.54+1290.52j 789.84+1335.09j 784.09+1325.7j  807.25+1363.41j\n",
      " 789.84+1335.09j 804.77+1359.13j 811.38+1370.27j 822.19+1387.44j\n",
      " 784.09+1325.7j  811.38+1370.27j 805.45+1361.j   828.61+1398.7j\n",
      " 807.25+1363.41j 822.19+1387.44j 828.61+1398.7j  839.42+1415.87j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-345.88+0.j -370.49+0.j -365.11-0.j -386.36-0.j -370.49+0.j -383.89-0.j\n",
      " -390.64+0.j -400.3 +0.j -365.11-0.j -390.64+0.j -384.86-0.j -406.93-0.j\n",
      " -386.36-0.j -400.3 +0.j -406.93-0.j -417.06-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: [-345.88+0.j -370.49+0.j -365.11-0.j -386.36-0.j -370.49+0.j -383.89-0.j\n",
      " -390.64+0.j -400.3 +0.j -365.11-0.j -390.64+0.j -384.86-0.j -406.93-0.j\n",
      " -386.36-0.j -400.3 +0.j -406.93-0.j -417.06-0.j]\n",
      "WE   (0, 0)\t(36501450.75644716+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[765.93+1296.2j  793.35+1340.97j 787.57+1331.53j 810.84+1369.4j\n",
      " 793.35+1340.97j 808.35+1365.13j 814.99+1376.29j 825.84+1393.56j\n",
      " 787.57+1331.53j 814.99+1376.29j 809.02+1366.98j 832.29+1404.85j\n",
      " 810.84+1369.4j  825.84+1393.56j 832.29+1404.85j 843.15+1422.11j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-347.41-0.j -372.13+0.j -366.72-0.j -388.06-0.j -372.13+0.j -385.59-0.j\n",
      " -392.36+0.j -402.07+0.j -366.72-0.j -392.36+0.j -386.56-0.j -408.72-0.j\n",
      " -388.06-0.j -402.07+0.j -408.72-0.j -418.91+0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: [-347.41-0.j -372.13+0.j -366.72-0.j -388.06-0.j -372.13+0.j -385.59-0.j\n",
      " -392.36+0.j -402.07+0.j -366.72-0.j -392.36+0.j -386.56-0.j -408.72-0.j\n",
      " -388.06-0.j -402.07+0.j -408.72-0.j -418.91+0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[769.34+1301.91j 796.88+1346.87j 791.07+1337.38j 814.44+1375.42j\n",
      " 796.88+1346.87j 811.95+1371.15j 818.6 +1382.34j 829.51+1399.7j\n",
      " 791.07+1337.38j 818.6 +1382.34j 812.61+1372.98j 835.98+1411.02j\n",
      " 814.44+1375.42j 829.51+1399.7j  835.98+1411.02j 846.89+1428.37j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-348.95-0.j -373.77+0.j -368.34-0.j -389.77-0.j -373.77+0.j -387.3 -0.j\n",
      " -394.09+0.j -403.85+0.j -368.34-0.j -394.09+0.j -388.26-0.j -410.52-0.j\n",
      " -389.77-0.j -403.85+0.j -410.52-0.j -420.76-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: [-348.95-0.j -373.77+0.j -368.34-0.j -389.77-0.j -373.77+0.j -387.3 -0.j\n",
      " -394.09+0.j -403.85+0.j -368.34-0.j -394.09+0.j -388.26-0.j -410.52-0.j\n",
      " -389.77-0.j -403.85+0.j -410.52-0.j -420.76-0.j]\n",
      "WE   (0, 0)\t(37149822.719439+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[772.77+1307.64j 800.42+1352.8j  794.59+1343.27j 818.06+1381.47j\n",
      " 800.42+1352.8j  815.57+1377.19j 822.24+1388.42j 833.2 +1405.86j\n",
      " 794.59+1343.27j 822.24+1388.42j 816.23+1379.02j 839.69+1417.22j\n",
      " 818.06+1381.47j 833.2 +1405.86j 839.69+1417.22j 850.66+1434.66j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-350.49-0.j -375.42+0.j -369.96-0.j -391.49-0.j -375.42+0.j -389.02-0.j\n",
      " -395.82+0.j -405.64+0.j -369.96-0.j -395.82+0.j -389.97-0.j -412.33-0.j\n",
      " -391.49-0.j -405.64+0.j -412.33-0.j -422.61-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: [-350.49-0.j -375.42+0.j -369.96-0.j -391.49-0.j -375.42+0.j -389.02-0.j\n",
      " -395.82+0.j -405.64+0.j -369.96-0.j -395.82+0.j -389.97-0.j -412.33-0.j\n",
      " -391.49-0.j -405.64+0.j -412.33-0.j -422.61-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[776.22+1313.41j 803.99+1358.76j 798.12+1349.18j 821.69+1387.54j\n",
      " 803.99+1358.76j 819.2 +1383.27j 825.89+1394.53j 836.91+1412.05j\n",
      " 798.12+1349.18j 825.89+1394.53j 819.86+1385.09j 843.42+1423.45j\n",
      " 821.69+1387.54j 836.91+1412.05j 843.42+1423.45j 854.44+1440.98j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-352.04+0.j -377.08+0.j -371.59-0.j -393.21-0.j -377.08+0.j -390.74-0.j\n",
      " -397.57+0.j -407.43+0.j -371.59-0.j -397.57+0.j -391.69-0.j -414.14-0.j\n",
      " -393.21-0.j -407.43+0.j -414.14-0.j -424.48-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: [-352.04+0.j -377.08+0.j -371.59-0.j -393.21-0.j -377.08+0.j -390.74-0.j\n",
      " -397.57+0.j -407.43+0.j -371.59-0.j -397.57+0.j -391.69-0.j -414.14-0.j\n",
      " -393.21-0.j -407.43+0.j -414.14-0.j -424.48-0.j]\n",
      "WE   (0, 0)\t(37810161.951441556+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[779.68+1319.2j  807.57+1364.75j 801.68+1355.12j 825.34+1393.65j\n",
      " 807.57+1364.75j 822.85+1389.37j 829.56+1400.67j 840.63+1418.28j\n",
      " 801.68+1355.12j 829.56+1400.67j 823.5 +1391.19j 847.17+1429.71j\n",
      " 825.34+1393.65j 840.63+1418.28j 847.17+1429.71j 858.23+1447.32j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-353.6 +0.j -378.75+0.j -373.24-0.j -394.95-0.j -378.75+0.j -392.47-0.j\n",
      " -399.32+0.j -409.23+0.j -373.24-0.j -399.32+0.j -393.42-0.j -415.97-0.j\n",
      " -394.95-0.j -409.23+0.j -415.97-0.j -426.35-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: [-353.6 +0.j -378.75+0.j -373.24-0.j -394.95-0.j -378.75+0.j -392.47-0.j\n",
      " -399.32+0.j -409.23+0.j -373.24-0.j -399.32+0.j -393.42-0.j -415.97-0.j\n",
      " -394.95-0.j -409.23+0.j -415.97-0.j -426.35-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[783.16+1325.02j 811.16+1370.76j 805.25+1361.09j 829.01+1399.78j\n",
      " 811.16+1370.76j 826.52+1395.51j 833.25+1406.83j 844.37+1424.53j\n",
      " 805.25+1361.09j 833.25+1406.83j 827.17+1397.32j 850.93+1436.j\n",
      " 829.01+1399.78j 844.37+1424.53j 850.93+1436.j   862.05+1453.7j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-355.17-0.j -380.42+0.j -374.89-0.j -396.69-0.j -380.42+0.j -394.21-0.j\n",
      " -401.08+0.j -411.04+0.j -374.89-0.j -401.08+0.j -395.16-0.j -417.8 -0.j\n",
      " -396.69-0.j -411.04+0.j -417.8 -0.j -428.23-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: [-355.17-0.j -380.42+0.j -374.89-0.j -396.69-0.j -380.42+0.j -394.21-0.j\n",
      " -401.08+0.j -411.04+0.j -374.89-0.j -401.08+0.j -395.16-0.j -417.8 -0.j\n",
      " -396.69-0.j -411.04+0.j -417.8 -0.j -428.23-0.j]\n",
      "WE   (0, 0)\t(38482767.06016232+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[786.66+1330.87j 814.78+1376.81j 808.83+1367.09j 832.7 +1405.95j\n",
      " 814.78+1376.81j 830.21+1401.67j 836.96+1413.03j 848.13+1430.81j\n",
      " 808.83+1367.09j 836.96+1413.03j 830.85+1403.47j 854.72+1442.32j\n",
      " 832.7 +1405.95j 848.13+1430.81j 854.72+1442.32j 865.88+1460.1j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-356.75-0.j -382.11+0.j -376.54-0.j -398.44-0.j -382.11+0.j -395.96-0.j\n",
      " -402.85+0.j -412.85+0.j -376.54-0.j -402.85+0.j -396.9 -0.j -419.64-0.j\n",
      " -398.44-0.j -412.85+0.j -419.64-0.j -430.12+0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: [-356.75-0.j -382.11+0.j -376.54-0.j -398.44-0.j -382.11+0.j -395.96-0.j\n",
      " -402.85+0.j -412.85+0.j -376.54-0.j -402.85+0.j -396.9 -0.j -419.64-0.j\n",
      " -398.44-0.j -412.85+0.j -419.64-0.j -430.12+0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[790.18+1336.75j 818.41+1382.89j 812.44+1373.12j 836.4 +1412.14j\n",
      " 818.41+1382.89j 833.91+1407.86j 840.68+1419.26j 851.9 +1437.12j\n",
      " 812.44+1373.12j 840.68+1419.26j 834.55+1409.66j 858.52+1448.68j\n",
      " 836.4 +1412.14j 851.9 +1437.12j 858.52+1448.68j 869.74+1466.54j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-358.33+0.j -383.8 +0.j -378.21-0.j -400.2 -0.j -383.8 +0.j -397.72-0.j\n",
      " -404.63+0.j -414.68+0.j -378.21-0.j -404.63+0.j -398.66-0.j -421.49-0.j\n",
      " -400.2 -0.j -414.68+0.j -421.49-0.j -432.02-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: [-358.33+0.j -383.8 +0.j -378.21-0.j -400.2 -0.j -383.8 +0.j -397.72-0.j\n",
      " -404.63+0.j -414.68+0.j -378.21-0.j -404.63+0.j -398.66-0.j -421.49-0.j\n",
      " -400.2 -0.j -414.68+0.j -421.49-0.j -432.02-0.j]\n",
      "WE   (0, 0)\t(39168067.36024149+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[793.71+1342.67j 822.07+1388.99j 816.07+1379.18j 840.13+1418.37j\n",
      " 822.07+1388.99j 837.64+1414.09j 844.42+1425.51j 855.7 +1443.46j\n",
      " 816.07+1379.18j 844.42+1425.51j 838.27+1415.88j 862.34+1455.06j\n",
      " 840.13+1418.37j 855.7 +1443.46j 862.34+1455.06j 873.61+1473.01j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-359.93-0.j -385.5 +0.j -379.88-0.j -401.96-0.j -385.5 +0.j -399.49-0.j\n",
      " -406.41+0.j -416.51+0.j -379.88-0.j -406.41+0.j -400.42-0.j -423.35-0.j\n",
      " -401.96-0.j -416.51+0.j -423.35-0.j -433.93-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: [-359.93-0.j -385.5 +0.j -379.88-0.j -401.96-0.j -385.5 +0.j -399.49-0.j\n",
      " -406.41+0.j -416.51+0.j -379.88-0.j -406.41+0.j -400.42-0.j -423.35-0.j\n",
      " -401.96-0.j -416.51+0.j -423.35-0.j -433.93-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[797.26+1348.61j 825.74+1395.13j 819.71+1385.27j 843.87+1424.62j\n",
      " 825.74+1395.13j 841.38+1420.34j 848.19+1431.8j  859.51+1449.83j\n",
      " 819.71+1385.27j 848.19+1431.8j  842.01+1422.13j 866.18+1461.47j\n",
      " 843.87+1424.62j 859.51+1449.83j 866.18+1461.47j 877.5 +1479.51j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-361.53-0.j -387.21+0.j -381.57-0.j -403.74-0.j -387.21+0.j -401.26-0.j\n",
      " -408.21+0.j -418.36+0.j -381.57-0.j -408.21+0.j -402.19-0.j -425.22-0.j\n",
      " -403.74-0.j -418.36+0.j -425.22-0.j -435.85-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: [-361.53-0.j -387.21+0.j -381.57-0.j -403.74-0.j -387.21+0.j -401.26-0.j\n",
      " -408.21+0.j -418.36+0.j -381.57-0.j -408.21+0.j -402.19-0.j -425.22-0.j\n",
      " -403.74-0.j -418.36+0.j -425.22-0.j -435.85-0.j]\n",
      "WE   (0, 0)\t(39866518.93699693+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[800.84+1354.58j 829.43+1401.3j  823.37+1391.4j  847.64+1430.91j\n",
      " 829.43+1401.3j  845.14+1426.63j 851.97+1438.12j 863.35+1456.24j\n",
      " 823.37+1391.4j  851.97+1438.12j 845.78+1428.41j 870.04+1467.92j\n",
      " 847.64+1430.91j 863.35+1456.24j 870.04+1467.92j 881.41+1486.04j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-363.14-0.j -388.93+0.j -383.26-0.j -405.52-0.j -388.93+0.j -403.05-0.j\n",
      " -410.02+0.j -420.21+0.j -383.26-0.j -410.02+0.j -403.98-0.j -427.1 -0.j\n",
      " -405.52-0.j -420.21+0.j -427.1 -0.j -437.78+0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: [-363.14-0.j -388.93+0.j -383.26-0.j -405.52-0.j -388.93+0.j -403.05-0.j\n",
      " -410.02+0.j -420.21+0.j -383.26-0.j -410.02+0.j -403.98-0.j -427.1 -0.j\n",
      " -405.52-0.j -420.21+0.j -427.1 -0.j -437.78+0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[804.43+1360.58j 833.14+1407.5j  827.06+1397.56j 851.42+1437.23j\n",
      " 833.14+1407.5j  848.92+1432.95j 855.77+1444.48j 867.2 +1462.68j\n",
      " 827.06+1397.56j 855.77+1444.48j 849.56+1434.73j 873.92+1474.41j\n",
      " 851.42+1437.23j 867.2 +1462.68j 873.92+1474.41j 885.35+1492.6j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-364.75-0.j -390.66+0.j -384.96-0.j -407.32-0.j -390.66+0.j -404.84-0.j\n",
      " -411.83+0.j -422.07+0.j -384.96-0.j -411.83+0.j -405.77-0.j -428.99-0.j\n",
      " -407.32-0.j -422.07+0.j -428.99-0.j -439.72-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: [-364.75-0.j -390.66+0.j -384.96-0.j -407.32-0.j -390.66+0.j -404.84-0.j\n",
      " -411.83+0.j -422.07+0.j -384.96-0.j -411.83+0.j -405.77-0.j -428.99-0.j\n",
      " -407.32-0.j -422.07+0.j -428.99-0.j -439.72-0.j]\n",
      "WE   (0, 0)\t(40578546.14410736+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[808.03+1366.61j 836.87+1413.74j 830.76+1403.74j 855.22+1443.58j\n",
      " 836.87+1413.74j 852.72+1439.3j  859.6 +1450.87j 871.07+1469.15j\n",
      " 830.76+1403.74j 859.6 +1450.87j 853.36+1441.09j 877.82+1480.93j\n",
      " 855.22+1443.58j 871.07+1469.15j 877.82+1480.93j 889.3 +1499.2j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-366.38-0.j -392.39+0.j -386.67-0.j -409.12-0.j -392.39+0.j -406.64-0.j\n",
      " -413.66+0.j -423.94+0.j -386.67-0.j -413.66+0.j -407.57-0.j -430.89-0.j\n",
      " -409.12-0.j -423.94+0.j -430.89-0.j -441.66-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: [-366.38-0.j -392.39+0.j -386.67-0.j -409.12-0.j -392.39+0.j -406.64-0.j\n",
      " -413.66+0.j -423.94+0.j -386.67-0.j -413.66+0.j -407.57-0.j -430.89-0.j\n",
      " -409.12-0.j -423.94+0.j -430.89-0.j -441.66-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[811.66+1372.68j 840.62+1420.j   834.48+1409.97j 859.04+1449.97j\n",
      " 840.62+1420.j   856.54+1445.69j 863.44+1457.29j 874.97+1475.65j\n",
      " 834.48+1409.97j 863.44+1457.29j 857.19+1447.48j 881.75+1487.48j\n",
      " 859.04+1449.97j 874.97+1475.65j 881.75+1487.48j 893.27+1505.84j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-368.02-0.j -394.14+0.j -388.39-0.j -410.93-0.j -394.14+0.j -408.46-0.j\n",
      " -415.49+0.j -425.82+0.j -388.39-0.j -415.49+0.j -409.39-0.j -432.8 -0.j\n",
      " -410.93-0.j -425.82+0.j -432.8 -0.j -443.62-0.j], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: [-368.02-0.j -394.14+0.j -388.39-0.j -410.93-0.j -394.14+0.j -408.46-0.j\n",
      " -415.49+0.j -425.82+0.j -388.39-0.j -415.49+0.j -409.39-0.j -432.8 -0.j\n",
      " -410.93-0.j -425.82+0.j -432.8 -0.j -443.62-0.j]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr,  innerprod_sparse\n",
    "from compgraph.tensor_wave_functions import adjust_dtype_and_multiply, convert_csr_to_sparse_tensor, evolving_function, variational_wave_function_on_batch\n",
    "initial_learning_rate = 7e-5\n",
    "\n",
    "#THE inner training step should work for one configuration at a time if logic has any ground on reality\n",
    "H_no_traversalfield=construct_sparse_hamiltonian(G, 0)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "optimizer_snt = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "Hamiltonian_tensor_no_traversalfield = convert_csr_to_sparse_tensor(H_no_traversalfield)\n",
    "beta=0.005\n",
    "def evolving_function(wave, Ham_tensor,beta):\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    beta *= -1\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, beta)\n",
    "    #print(wave.indices,phi.indices)\n",
    "    \n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    #print(phi)\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    overlap=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    normalization=1/tf.math.sqrt(norm_wave*norm_ito_wave)\n",
    "    #print(normalization, 'This is the coefficient at denominator')\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    return -overlap_normalized.values\n",
    "\n",
    "def sparse_rep_inner_training_step(model, graph_batch, graph_batch_indices, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = variational_wave_function_on_batch(model, graph_batch, graph_batch_indices)\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "            \n",
    "        loss = evolving_function(output, Hamiltonian_tensor, beta)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "start=0\n",
    "for step in range(10):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "\n",
    "    graph_tuples_batch=full_graph_tuples\n",
    "    graph_tuples_batch_indices= full_configurations\n",
    "\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "    \n",
    "    beta = 0.05 #This parameter determines the amount of imaginary time evolution at each outer step\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    print(\"WE\", innerprod_sparse(phi_csr, phi_csr))\n",
    "    for innerstep in range(2):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "\n",
    "        outputs, loss = sparse_rep_inner_training_step(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices, optimizer_snt)\n",
    "       \n",
    "        #print(outputs,loss)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "        \n",
    "        \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n",
      "tf.Tensor((1+0j), shape=(), dtype=complex128) \n",
      " tf.Tensor((1+0j), shape=(), dtype=complex128)\n",
      "tf.Tensor(\n",
      "[-0.05-0.22j -0.1 -0.26j -0.07-0.18j -0.11-0.19j -0.1 -0.26j -0.14-0.28j\n",
      " -0.11-0.19j -0.15-0.22j -0.07-0.18j -0.11-0.19j -0.12-0.2j  -0.15-0.22j\n",
      " -0.11-0.19j -0.15-0.22j -0.15-0.22j -0.18-0.23j], shape=(16,), dtype=complex128)\n",
      "tf.Tensor(\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j\n",
      " 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j], shape=(16,), dtype=complex128)\n",
      "[[-1 -1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [-1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1 -1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]\n",
      " [-1  1  1  1]\n",
      " [ 1 -1 -1 -1]\n",
      " [ 1 -1 -1  1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1  1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1  1 -1  1]\n",
      " [ 1  1  1 -1]\n",
      " [ 1  1  1  1]]\n",
      "[[-1 -1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [-1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1 -1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]\n",
      " [-1  1  1  1]\n",
      " [ 1 -1 -1 -1]\n",
      " [ 1 -1 -1  1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1  1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1  1 -1  1]\n",
      " [ 1  1  1 -1]\n",
      " [ 1  1  1  1]]\n",
      "tf.Tensor((2.203753837034054+0j), shape=(), dtype=complex128)\n",
      "tf.Tensor((1.9540899737473738+0j), shape=(), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from compgraph.tensor_wave_functions import time_evoluted_wave_function_on_batch,variational_wave_function_on_batch, variational_wave_function_on_batch_v2, sparse_tensor_exp_energy\n",
    "\n",
    "\n",
    "a=time_evoluted_wave_function_on_batch(less_trivial_gnn, 0.05, full_graph_tuples, G, sublattice_encoding)\n",
    "b=variational_wave_function_on_batch(less_trivial_gnn, full_graph_tuples,full_configurations)\n",
    "norm_b=np.array(b.values)/tf.norm(b.values)\n",
    "b2=variational_wave_function_on_batch_v2(less_trivial_gnn, full_graph_tuples)\n",
    "norm_a=np.array(a.values)/tf.norm(a.values)\n",
    "print(type(a),type(b))\n",
    "print(tf.norm(norm_b), \"\\n\", tf.norm(norm_a))\n",
    "print(a.values)\n",
    "print(b2.values-b.values)\n",
    "exp1=sparse_tensor_exp_energy(b,G,0)\n",
    "exp2=sparse_tensor_exp_energy(a,G,0)\n",
    "print(exp1/tf.norm(b.values))\n",
    "print(exp2/tf.norm(a.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  1. -2.  0.  0.  0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  1.  0.  0.  0. -2.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.]]\n",
      "[[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]]\n"
     ]
    }
   ],
   "source": [
    "Hamiltonian_qu= qu.ham_heis_2D(2, 2, j=1.0, bz=0, cyclic=True, parallel=False, ownership=None)\n",
    "print(Hamiltonian_qu)\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, 0)\n",
    "print(2*Hamiltonian.toarray()-Hamiltonian_qu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05-0.24j -0.11-0.28j -0.08-0.2j  -0.12-0.22j -0.11-0.28j -0.16-0.3j\n",
      " -0.13-0.25j -0.17-0.24j -0.08-0.2j  -0.13-0.25j -0.13-0.23j -0.16-0.24j\n",
      " -0.12-0.22j -0.17-0.24j -0.16-0.24j -0.2 -0.24j]\n",
      "tf.Tensor(\n",
      "[-0.05-0.24j -0.11-0.28j -0.08-0.2j  -0.12-0.22j -0.11-0.28j -0.16-0.3j\n",
      " -0.13-0.25j -0.17-0.24j -0.08-0.2j  -0.13-0.25j -0.13-0.23j -0.16-0.24j\n",
      " -0.12-0.22j -0.17-0.24j -0.16-0.24j -0.2 -0.24j], shape=(16,), dtype=complex128)\n",
      "[[-1 -1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [-1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1 -1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]\n",
      " [-1  1  1  1]\n",
      " [ 1 -1 -1 -1]\n",
      " [ 1 -1 -1  1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1  1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1  1 -1  1]\n",
      " [ 1  1  1 -1]\n",
      " [ 1  1  1  1]]\n",
      "[[-1 -1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [-1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1 -1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]\n",
      " [-1  1  1  1]\n",
      " [ 1 -1 -1 -1]\n",
      " [ 1 -1 -1  1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1  1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1  1 -1  1]\n",
      " [ 1  1  1 -1]\n",
      " [ 1  1  1  1]]\n",
      "[[-0.200768-0.240377j]\n",
      " [-0.163134-0.242837j]\n",
      " [-0.165104-0.242726j]\n",
      " [-0.122293-0.217229j]\n",
      " [-0.163134-0.242837j]\n",
      " [-0.129897-0.228832j]\n",
      " [-0.127389-0.247906j]\n",
      " [-0.084127-0.202308j]\n",
      " [-0.165104-0.242726j]\n",
      " [-0.127389-0.247906j]\n",
      " [-0.155245-0.301239j]\n",
      " [-0.106552-0.279945j]\n",
      " [-0.122293-0.217229j]\n",
      " [-0.084127-0.202308j]\n",
      " [-0.106552-0.279945j]\n",
      " [-0.0508  -0.235673j]]\n",
      "tf.Tensor((1.1130702098598402+0j), shape=(), dtype=complex128) [[1.11307+0.j]]\n",
      "tf.Tensor(\n",
      "[-0.05-0.24j -0.11-0.28j -0.08-0.2j  -0.12-0.22j -0.11-0.28j -0.16-0.3j\n",
      " -0.13-0.25j -0.17-0.24j -0.08-0.2j  -0.13-0.25j -0.13-0.23j -0.16-0.24j\n",
      " -0.12-0.22j -0.17-0.24j -0.16-0.24j -0.2 -0.24j], shape=(16,), dtype=complex128) [[-0.200768-0.240377j]\n",
      " [-0.163134-0.242837j]\n",
      " [-0.165104-0.242726j]\n",
      " [-0.122293-0.217229j]\n",
      " [-0.163134-0.242837j]\n",
      " [-0.129897-0.228832j]\n",
      " [-0.127389-0.247906j]\n",
      " [-0.084127-0.202308j]\n",
      " [-0.165104-0.242726j]\n",
      " [-0.127389-0.247906j]\n",
      " [-0.155245-0.301239j]\n",
      " [-0.106552-0.279945j]\n",
      " [-0.122293-0.217229j]\n",
      " [-0.084127-0.202308j]\n",
      " [-0.106552-0.279945j]\n",
      " [-0.0508  -0.235673j]]\n",
      "Expectation Value from sparse Hamiltonian:   (0, 0)\t(2.4529327260496596+0j)\n",
      "Expectation Value from CG notation: (2.4529327458669226+0j)\n",
      "Expecation value from quimb [[2.452933+0.j]]\n"
     ]
    }
   ],
   "source": [
    "from compgraph.useful import sparse_list_to_configs, config_list_to_state_list\n",
    "from compgraph.tensor_wave_functions import sparse_tensor_exp_energy\n",
    "import math\n",
    "less_trivial_gnn=GNN_double_output()\n",
    "\n",
    "b=variational_wave_function_on_batch(less_trivial_gnn, full_graph_tuples,full_configurations)\n",
    "b=tf.sparse.reorder(b)\n",
    "psi_csr = compute_wave_function_csr(full_graph_tuples, less_trivial_gnn, full_configurations)\n",
    "print(psi_csr.data)\n",
    "print(b.values)\n",
    "expectation_value=sparse_tensor_exp_energy(b,G,0)\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, 0)\n",
    "b_row_indices=np.array(b.indices)[:, 0]\n",
    "config_b=sparse_list_to_configs(b_row_indices, len(G.nodes))\n",
    "print(config_b)\n",
    "\n",
    "states=config_list_to_state_list(config_b)\n",
    "scaled_states=[coeff * state for coeff, state in zip(np.array(b.values), states)]\n",
    "\n",
    "fin_state=sum(scaled_states)\n",
    "print(fin_state)\n",
    "exp_value_quimb= fin_state.H@Hamiltonian_qu@fin_state\n",
    "\n",
    "print(tf.norm(b.values), np.sqrt(fin_state.H@fin_state))\n",
    "print(b.values, fin_state)\n",
    "# Calculate the expectation value using the Hamiltonian matrix\n",
    "sparse_expectation_value = psi_csr.conj().transpose().dot(Hamiltonian.dot((psi_csr)))\n",
    "# Output the expectation value for comparison\n",
    "print(\"Expectation Value from sparse Hamiltonian:\", 2*sparse_expectation_value)\n",
    "print(\"Expectation Value from CG notation:\", expectation_value)\n",
    "print(\"Expecation value from quimb\", exp_value_quimb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1] tf.Tensor((-0.2430237421353415-0.2877341324864011j), shape=(), dtype=complex128)\n",
      "[-1 -1 -1  1] tf.Tensor((-0.25903912119550654-0.30550446377904317j), shape=(), dtype=complex128)\n",
      "[-1 -1  1 -1] tf.Tensor((-0.3034278229372821-0.31644457991526376j), shape=(), dtype=complex128)\n",
      "[-1 -1  1  1] tf.Tensor((-0.2433042825583437-0.3074819824907743j), shape=(), dtype=complex128)\n",
      "[-1  1 -1 -1] tf.Tensor((-0.2590391211955064-0.30550446377904317j), shape=(), dtype=complex128)\n",
      "[-1  1 -1  1] tf.Tensor((-0.24595551734200494-0.3275928365301223j), shape=(), dtype=complex128)\n",
      "[-1  1  1 -1] tf.Tensor((-0.3187481702757292-0.3324781212986707j), shape=(), dtype=complex128)\n",
      "[-1  1  1  1] tf.Tensor((-0.23158380330625705-0.32512407273479926j), shape=(), dtype=complex128)\n",
      "[ 1 -1 -1 -1] tf.Tensor((-0.3034278229372821-0.31644457991526376j), shape=(), dtype=complex128)\n",
      "[ 1 -1 -1  1] tf.Tensor((-0.3187481702757293-0.3324781212986708j), shape=(), dtype=complex128)\n",
      "[ 1 -1  1 -1] tf.Tensor((-0.3747816509551245-0.37563788314769164j), shape=(), dtype=complex128)\n",
      "[ 1 -1  1  1] tf.Tensor((-0.3212646906312754-0.3670744083245453j), shape=(), dtype=complex128)\n",
      "[ 1  1 -1 -1] tf.Tensor((-0.2433042825583437-0.3074819824907744j), shape=(), dtype=complex128)\n",
      "[ 1  1 -1  1] tf.Tensor((-0.23158380330625705-0.32512407273479926j), shape=(), dtype=complex128)\n",
      "[ 1  1  1 -1] tf.Tensor((-0.3212646906312754-0.3670744083245453j), shape=(), dtype=complex128)\n",
      "[1 1 1 1] tf.Tensor((-0.23558177241965306-0.35620797185720077j), shape=(), dtype=complex128)\n",
      "[-0.24-0.29j -0.26-0.31j -0.3 -0.32j -0.24-0.31j -0.26-0.31j -0.25-0.33j\n",
      " -0.32-0.33j -0.23-0.33j -0.3 -0.32j -0.32-0.33j -0.37-0.38j -0.32-0.37j\n",
      " -0.24-0.31j -0.23-0.33j -0.32-0.37j -0.24-0.36j] [-0.24-0.29j -0.26-0.31j -0.3 -0.32j -0.24-0.31j -0.26-0.31j -0.25-0.33j\n",
      " -0.32-0.33j -0.23-0.33j -0.3 -0.32j -0.32-0.33j -0.37-0.38j -0.32-0.37j\n",
      " -0.24-0.31j -0.23-0.33j -0.32-0.37j -0.24-0.36j]\n",
      "coeff are [2.0] initial config [-1 -1 -1 -1] nonzero trans [[-1 -1 -1 -1]] config ket [-1 -1 -1 -1] used coeff 2.0\n",
      "coeff are [1.0, 1.0] initial config [-1 -1 -1  1] nonzero trans [[-1  1 -1 -1]\n",
      " [-1 -1  1 -1]] config ket [-1 -1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1 -1 -1  1] nonzero trans [[-1  1 -1 -1]\n",
      " [-1 -1  1 -1]] config ket [-1  1 -1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1 -1  1 -1] nonzero trans [[ 1 -1 -1 -1]\n",
      " [-1 -1 -1  1]] config ket [-1 -1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1 -1  1 -1] nonzero trans [[ 1 -1 -1 -1]\n",
      " [-1 -1 -1  1]] config ket [ 1 -1 -1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1 -1  1  1] nonzero trans [[ 1 -1 -1  1]\n",
      " [-1  1  1 -1]] config ket [-1  1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1 -1  1  1] nonzero trans [[ 1 -1 -1  1]\n",
      " [-1  1  1 -1]] config ket [ 1 -1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1  1 -1 -1] nonzero trans [[ 1 -1 -1 -1]\n",
      " [-1 -1 -1  1]] config ket [-1 -1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1  1 -1 -1] nonzero trans [[ 1 -1 -1 -1]\n",
      " [-1 -1 -1  1]] config ket [ 1 -1 -1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1  1 -1  1] nonzero trans [[ 1 -1 -1  1]\n",
      " [-1  1  1 -1]] config ket [-1  1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1  1 -1  1] nonzero trans [[ 1 -1 -1  1]\n",
      " [-1  1  1 -1]] config ket [ 1 -1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [-1  1  1 -1] nonzero trans [[ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]] config ket [-1 -1  1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [-1  1  1 -1] nonzero trans [[ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]] config ket [-1  1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [-1  1  1 -1] nonzero trans [[ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]] config ket [-1  1  1 -1] used coeff -2.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [-1  1  1 -1] nonzero trans [[ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]] config ket [ 1 -1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [-1  1  1 -1] nonzero trans [[ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [-1  1  1 -1]] config ket [ 1  1 -1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1  1  1  1] nonzero trans [[ 1  1 -1  1]\n",
      " [ 1 -1  1  1]] config ket [ 1 -1  1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [-1  1  1  1] nonzero trans [[ 1  1 -1  1]\n",
      " [ 1 -1  1  1]] config ket [ 1  1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1 -1 -1 -1] nonzero trans [[-1 -1  1 -1]\n",
      " [-1  1 -1 -1]] config ket [-1 -1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1 -1 -1 -1] nonzero trans [[-1 -1  1 -1]\n",
      " [-1  1 -1 -1]] config ket [-1  1 -1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [ 1 -1 -1  1] nonzero trans [[-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [-1 -1  1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [ 1 -1 -1  1] nonzero trans [[-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [-1  1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [ 1 -1 -1  1] nonzero trans [[-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [ 1 -1 -1  1] used coeff -2.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [ 1 -1 -1  1] nonzero trans [[-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [ 1 -1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0, 1.0, 1.0, -2.0] initial config [ 1 -1 -1  1] nonzero trans [[-1 -1  1  1]\n",
      " [-1  1 -1  1]\n",
      " [ 1  1 -1 -1]\n",
      " [ 1 -1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [ 1  1 -1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1 -1  1 -1] nonzero trans [[-1  1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [-1  1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1 -1  1 -1] nonzero trans [[-1  1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [ 1 -1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1 -1  1  1] nonzero trans [[-1  1  1  1]\n",
      " [ 1  1  1 -1]] config ket [-1  1  1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1 -1  1  1] nonzero trans [[-1  1  1  1]\n",
      " [ 1  1  1 -1]] config ket [ 1  1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1  1 -1 -1] nonzero trans [[-1  1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [-1  1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1  1 -1 -1] nonzero trans [[-1  1  1 -1]\n",
      " [ 1 -1 -1  1]] config ket [ 1 -1 -1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1  1 -1  1] nonzero trans [[-1  1  1  1]\n",
      " [ 1  1  1 -1]] config ket [-1  1  1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1  1 -1  1] nonzero trans [[-1  1  1  1]\n",
      " [ 1  1  1 -1]] config ket [ 1  1  1 -1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1  1  1 -1] nonzero trans [[ 1 -1  1  1]\n",
      " [ 1  1 -1  1]] config ket [ 1 -1  1  1] used coeff 1.0\n",
      "coeff are [1.0, 1.0] initial config [ 1  1  1 -1] nonzero trans [[ 1 -1  1  1]\n",
      " [ 1  1 -1  1]] config ket [ 1  1 -1  1] used coeff 1.0\n",
      "coeff are [2.0] initial config [1 1 1 1] nonzero trans [[1 1 1 1]] config ket [1 1 1 1] used coeff 2.0\n",
      "(-0.9835897813729695+5.850943135936622j)\n"
     ]
    }
   ],
   "source": [
    "from compgraph.cg_repr import config_hamiltonian_product\n",
    "for idx, config in enumerate(config_b):\n",
    "    print(config, b.values[idx])\n",
    "bra=np.array(b.values)\n",
    "ket=bra\n",
    "exp_value=0\n",
    "print(bra, ket)\n",
    "for idx_bra, config_bra in enumerate(config_b):\n",
    "    configurations_nonzero, coefficients = config_hamiltonian_product(config_bra, G)\n",
    "    for idx_ket,config_ket in enumerate(config_b):\n",
    "        # Using numpy to check if config_ket exists in configurations_nonzero\n",
    "        # and get the index if it exists\n",
    "        match_indices = np.where(np.all(configurations_nonzero == config_ket, axis=1))[0]\n",
    "        if match_indices.size > 0:\n",
    "            idx_nonzero = match_indices[0]  # Assuming the first match's index if multiple matches\n",
    "            coefficient = coefficients[idx_nonzero]  # Get the corresponding coefficient\n",
    "            exp_value += ket[idx_ket] * bra[idx_bra] * coefficient\n",
    "            print(\"coeff are\", coefficients, \"initial config\", config_bra, \n",
    "                  'nonzero trans', configurations_nonzero, \"config ket\",config_ket, 'used coeff', coefficient)\n",
    "\n",
    "print(exp_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs of psi [ 1 -1 -1  1] [-1  1 -1 -1]\n",
      "till here fine \n",
      "\n",
      "   (0, 4)\t1 [-1  1 -1 -1]\n",
      "end of square 2d function\n",
      "Expectation Value: (9.9-2.7j)\n",
      "Expectation Value from sparse Hamiltonian:   (0, 0)\t(9.9-2.7j)\n",
      "Expectation Value from CG notation: (9.9-2.7j)\n"
     ]
    }
   ],
   "source": [
    "from compgraph.cg_repr import square_2dham_exp\n",
    "# Define the psi and phi vectors with example amplitudes\n",
    "psi = np.array([2. +1j, 3.2 ])\n",
    "phi = np.array([1.5 +0.3j])\n",
    "print(\"Configs of psi\", basis_configs[2], basis_configs[4])\n",
    "# Define the configurations for psi and phi, they must be given as list of arrays or directly as multidimensional arrays\n",
    "configs_psi = np.array([basis_configs[2], basis_configs[4]])\n",
    "configs_phi = np.array([basis_configs[2]])\n",
    "print(\"till here fine \")\n",
    "print(\"\\n\", configurations[4],basis_configs[4])\n",
    "# Calculate the expectation value\n",
    "size=2**len(G.nodes)\n",
    "expectation_value = square_2dham_exp(psi, G, phi, J2, configs_psi, configs_phi)\n",
    "print(\"Expectation Value:\", expectation_value)\n",
    "psi_sparse=csr_matrix((psi, ([configurations[2].indices[0], configurations[4].indices[0]], [0,0])),shape=(size, 1))\n",
    "phi_sparse=csr_matrix((phi, ([configurations[2].indices[0]], [0])),shape=(size, 1))\n",
    "# Assuming `construct_sparse_hamiltonian` is already defined and returns the Hamiltonian matrix\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, J2)\n",
    "# Calculate the expectation value using the Hamiltonian matrix\n",
    "sparse_expectation_value = psi_sparse.conj().transpose().dot(Hamiltonian.dot((phi_sparse)))\n",
    "# Output the expectation value for comparison\n",
    "print(\"Expectation Value from sparse Hamiltonian:\", sparse_expectation_value)\n",
    "print(\"Expectation Value from CG notation:\", expectation_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(wave, Ham_tensor,beta):\n",
    "\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    beta *= -1\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, beta)\n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    norm_wave = 1/norm_wave\n",
    "    norm_ito_wave= 1/norm_ito_wave\n",
    "    normalized_wave=tf.sparse.map_values(tf.multiply,psi_conj,norm_wave)\n",
    "    normalized_ito_wave=tf.sparse.map_values(tf.multiply,phi,norm_ito_wave)\n",
    "    print('Are they normalized', tf.norm(normalized_wave.values), normalized_wave)\n",
    "    print(tf.norm(normalized_ito_wave.values), normalized_ito_wave)\n",
    "    normalization_factor=norm_wave*norm_ito_wave\n",
    "    overlap_n=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap_n,normalization_factor)\n",
    "\n",
    "    overlap=tf.sparse.map_values(tf.multiply,normalized_wave,normalized_ito_wave)\n",
    "    print('Final overlap', tf.norm(overlap.values),tf.norm(overlap_normalized.values))\n",
    "    overlap2= tf.sparse.map_values(tf.multiply, normalized_wave, normalized_ito_wave)\n",
    "    print(overlap, overlap2)\n",
    "#    print('wave norms', norm_wave, norm_ito_wave)\n",
    "#    normalization=1/((norm_wave*norm_ito_wave))\n",
    "    \n",
    "    #print(normalization, 'This is the coefficient at denominator')\n",
    "#    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    #TODO NORMALIZATION check\n",
    "#    print('overlap', tf.norm(overlap.values)*normalization, norm_ito_wave*norm_wave)\n",
    "#    print(tf.norm(overlap_normalized.values), 'This should be 1')\n",
    "    return -overlap.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "beta 0.05\n",
      "  (0, 0)\t(0.02789507433772087+0.2535555064678192j)\n",
      "  (1, 0)\t(-0.019511831924319267+0.16419753432273865j)\n",
      "  (2, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (3, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (4, 0)\t(0.14807215332984924+0.2852272391319275j)\n",
      "  (5, 0)\t(0.14807215332984924+0.2852272391319275j)\n",
      "  (6, 0)\t(-0.0013693792279809713+0.26433518528938293j)\n",
      "  (7, 0)\t(-0.0013693792279809713+0.26433518528938293j)\n",
      "  (8, 0)\t(-0.019511831924319267+0.16419753432273865j)\n",
      "  (9, 0)\t(-0.0013693792279809713+0.26433518528938293j)\n",
      "  (10, 0)\t(-0.019511831924319267+0.16419753432273865j)\n",
      "  (11, 0)\t(-0.002699091797694564+0.28862276673316956j)\n",
      "  (12, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (13, 0)\t(0.02789507433772087+0.2535555064678192j)\n",
      "  (14, 0)\t(-0.020448261871933937+0.13112111389636993j)\n",
      "  (15, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (0, 0)\t(0.020921305753290653+0.1901666298508644j)\n",
      "  (1, 0)\t(-0.019767962303012608+0.11818675883114338j)\n",
      "  (2, 0)\t(-0.010371121857315303+0.17636818140745164j)\n",
      "  (3, 0)\t(-0.003722334664780648+0.1868464931845665j)\n",
      "  (4, 0)\t(0.14539420148357748+0.22869266122579573j)\n",
      "  (5, 0)\t(0.17044796607224272+0.23240652531385422j)\n",
      "  (6, 0)\t(-0.005291366239544004+0.20136811286211015j)\n",
      "  (7, 0)\t(0.0020903735829051586+0.22455650568008423j)\n",
      "  (8, 0)\t(-0.019767962303012608+0.11818675883114338j)\n",
      "  (9, 0)\t(-0.005291366239544004+0.20136811286211015j)\n",
      "  (10, 0)\t(-0.030652816232759506+0.0871708795428276j)\n",
      "  (11, 0)\t(-0.007732665637740866+0.2280252579599619j)\n",
      "  (12, 0)\t(-0.003722334664780648+0.1868464931845665j)\n",
      "  (13, 0)\t(0.028980333724757657+0.18594454564154148j)\n",
      "  (14, 0)\t(-0.0208042855898384+0.06469962000846863j)\n",
      "  (15, 0)\t(0.013700787909328938+0.18121762946248055j)\n",
      "overlap tf.Tensor(\n",
      "[-0.07-0.j -0.03-0.j -0.06-0.j -0.06-0.j -0.11-0.j -0.11-0.j -0.07-0.j\n",
      " -0.07-0.j -0.03-0.j -0.07-0.j -0.03-0.j -0.08-0.j -0.06-0.j -0.07-0.j\n",
      " -0.02-0.j -0.06-0.j], shape=(16,), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "# First, load the autoreload extension if it hasn't been loaded already\n",
    "%load_ext autoreload\n",
    "# Set autoreload to reload modules automatically before executing code\n",
    "%autoreload 2\n",
    "print('beta', beta)\n",
    "\n",
    "\n",
    "psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "print(psi_csr)\n",
    "phi_csr = psi_csr - beta*Hamiltonian.dot(psi_csr)\n",
    "print(phi_csr)\n",
    "output = variational_wave_function_on_batch(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices)\n",
    "            \n",
    "overlap = evolving_function(output, Hamiltonian_tensor, 0)\n",
    "print(\"overlap\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[-0.01-0.03j -0.01-0.04j -0.  -0.03j -0.01-0.04j -0.01-0.04j -0.01-0.04j\n",
      " -0.01-0.04j -0.  -0.04j -0.  -0.03j -0.01-0.04j -0.  -0.03j -0.01-0.03j\n",
      " -0.01-0.04j -0.  -0.04j -0.01-0.03j -0.  -0.03j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64)) SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[0.17+0.66j 0.25+0.68j 0.06+0.65j 0.12+0.68j 0.25+0.68j 0.22+0.72j\n",
      " 0.14+0.67j 0.09+0.72j 0.06+0.65j 0.14+0.67j 0.05+0.55j 0.1 +0.58j\n",
      " 0.12+0.68j 0.09+0.72j 0.1 +0.58j 0.05+0.62j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64)) SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[-0.42+0.23j -0.42+0.35j -0.44+0.08j -0.47+0.17j -0.42+0.35j -0.5 +0.33j\n",
      " -0.46+0.19j -0.54+0.14j -0.44+0.08j -0.46+0.19j -0.31+0.06j -0.34+0.13j\n",
      " -0.47+0.17j -0.54+0.14j -0.34+0.13j -0.4 +0.07j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[-0.42+0.23j -0.42+0.35j -0.44+0.08j -0.47+0.17j -0.42+0.35j -0.5 +0.33j\n",
      " -0.46+0.19j -0.54+0.14j -0.44+0.08j -0.46+0.19j -0.31+0.06j -0.34+0.13j\n",
      " -0.47+0.17j -0.54+0.14j -0.34+0.13j -0.4 +0.07j], shape=(16,), dtype=complex128)\n",
      "tf.Tensor(\n",
      "[0.51+0.j 0.58+0.j 0.47+0.j 0.53+0.j 0.58+0.j 0.63+0.j 0.52+0.j 0.59+0.j\n",
      " 0.47+0.j 0.52+0.j 0.33+0.j 0.38+0.j 0.53+0.j 0.59+0.j 0.38+0.j 0.43+0.j], shape=(16,), dtype=complex128)\n",
      "  (0, 0)\t(0.17487379908561707+0.6912559866905212j)\n",
      "  (1, 0)\t(0.15978221036493778+0.6988106966018677j)\n",
      "  (2, 0)\t(0.15978221036493778+0.6988106966018677j)\n",
      "  (3, 0)\t(0.14292378723621368+0.7084795236587524j)\n",
      "  (4, 0)\t(0.15978221036493778+0.6988106966018677j)\n",
      "  (5, 0)\t(0.14292378723621368+0.7084795236587524j)\n",
      "  (6, 0)\t(0.12489217892289162+0.6753007769584656j)\n",
      "  (7, 0)\t(0.10256164893507957+0.6839965879917145j)\n",
      "  (8, 0)\t(0.15978221036493778+0.6988106966018677j)\n",
      "  (9, 0)\t(0.12489217892289162+0.6753007769584656j)\n",
      "  (10, 0)\t(0.14292378723621368+0.7084795236587524j)\n",
      "  (11, 0)\t(0.10256164893507957+0.6839965879917145j)\n",
      "  (12, 0)\t(0.14292378723621368+0.7084795236587524j)\n",
      "  (13, 0)\t(0.10256164893507957+0.6839965879917145j)\n",
      "  (14, 0)\t(0.10256164893507957+0.6839965879917145j)\n",
      "  (15, 0)\t(0.05347471684217453+0.6545992493629456j)\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[0.17+0.69j 0.16+0.7j  0.16+0.7j  0.14+0.71j 0.16+0.7j  0.14+0.71j\n",
      " 0.12+0.68j 0.1 +0.68j 0.16+0.7j  0.12+0.68j 0.14+0.71j 0.1 +0.68j\n",
      " 0.14+0.71j 0.1 +0.68j 0.1 +0.68j 0.05+0.65j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 0.+0.j -0.+0.j -0.+0.j  0.+0.j -0.+0.j  0.+0.j -0.-0.j  0.+0.j -0.+0.j\n",
      " -0.-0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j -0.+0.j], shape=(16,), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "import compgraph\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "indexing= np.column_stack((Hamiltonian.nonzero()))\n",
    "values_H= Hamiltonian.data\n",
    "shape_H = Hamiltonian.shape\n",
    "H_tensor= tf.sparse.SparseTensor(indexing,values_H,shape_H)\n",
    "H_tensor = tf.sparse.reorder(H_tensor)\n",
    "H2_tensor = convert_csr_to_sparse_tensor(Hamiltonian)\n",
    "output=  variational_wave_function_on_batch(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices)\n",
    "output_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "auxphi=tf.sparse.map_values(tf.multiply,output, -0.05)\n",
    "auxphi2=tf.sparse.map_values(tf.add,output,auxphi)\n",
    "prod=tf.sparse.map_values(tf.multiply,output,auxphi2)\n",
    "print(auxphi, auxphi2,prod) \n",
    "print(prod.values[:])\n",
    "conj_output = tf.sparse.map_values(tf.math.conj, output)\n",
    "prod=tf.sparse.map_values(tf.multiply,conj_output,output)\n",
    "print(prod.values[:])\n",
    "\n",
    "phi_csr = Hamiltonian.dot(output_csr)\n",
    "print(phi_csr)\n",
    "phi_sparse_coo = phi_csr.tocoo()\n",
    "indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex128)\n",
    "print(phi_sparse_tf)\n",
    "\n",
    "phi= adjust_dtype_and_multiply(H2_tensor,tf.sparse.reorder(output))\n",
    "\n",
    "print(tf.sparse.reorder(output).indices)\n",
    "print(phi.values-phi_sparse_tf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t(0.2+0j)\n",
      "  (1, 0)\t(0.6+0.5j)\n",
      "  (2, 0)\t(0.8+0.7j) \n",
      " ARTURO [[2.+0.j  0.+0.j  1.+0.2j 0.+0.j ]\n",
      " [0.+0.j  0.+0.j  3.+0.j  0.+0.j ]\n",
      " [4.+0.j  5.+0.j  6.+0.j  0.+0.j ]\n",
      " [0.+0.j  0.+0.j  0.+0.j  0.+0.j ]]\n",
      "  (0, 0)\t(1.06+0.86j)\n",
      "  (1, 0)\t(2.4000000000000004+2.0999999999999996j)\n",
      "  (2, 0)\t(8.600000000000001+6.699999999999999j)\n",
      "[[2.+0.j  0.+0.j  1.+0.2j 0.+0.j ]\n",
      " [0.+0.j  0.+0.j  3.+0.j  0.+0.j ]\n",
      " [4.+0.j  5.+0.j  6.+0.j  0.+0.j ]\n",
      " [0.+0.j  0.+0.j  0.+0.j  0.+0.j ]]\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [2 0]], shape=(3, 2), dtype=int64), values=tf.Tensor([0.2+0.j  0.6+0.5j 0.8+0.7j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([4 1], shape=(2,), dtype=int64))\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [2 0]], shape=(3, 2), dtype=int64), values=tf.Tensor([1.06+0.86j 2.4 +2.1j  8.6 +6.7j ], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([4 1], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([2, 0, 2, 0, 1, 2])\n",
    "data = np.array([1+ 0.2j, 2, 3, 4, 5, 6])\n",
    "arturo=csr_matrix((data, (row, col)), shape=(4, 4))\n",
    "data = [0.2, 0.6+0.5j, 0.8+0.7j ]  # List to store the non-zero entries\n",
    "row_indices = [0,1,2]  # List to store the row indices\n",
    "col_indices = [0]*3\n",
    "gomez=csr_matrix((data, (row_indices, col_indices)), shape=(4, 1))\n",
    "print(gomez,'\\n ARTURO',arturo.toarray())\n",
    "print(arturo.dot(gomez))\n",
    "indexing= np.column_stack((arturo.nonzero()))\n",
    "values_arturo= arturo.data\n",
    "shape_arturo = arturo.shape\n",
    "arturo_tensor= tf.sparse.SparseTensor(indexing,values_arturo,shape_arturo)\n",
    "arturo_tensor = tf.sparse.reorder(arturo_tensor)\n",
    "arturo_tensor = convert_csr_to_sparse_tensor(arturo)\n",
    "print(arturo_tensor._numpy())\n",
    "phi_sparse_coo = gomez.tocoo()\n",
    "indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex128)\n",
    "print(phi_sparse_tf)\n",
    "out_tensor = adjust_dtype_and_multiply(arturo_tensor,phi_sparse_tf)\n",
    "print(out_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 In the previous section we managed to perform all the operations we wanted to do while preserving the gradient. Hurra! \n",
    "In this section we will include the montecarlo update and try to stabilize the parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnets2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
