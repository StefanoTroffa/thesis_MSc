{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook was built with the purpose of understanding how to compute gradients and optimization routines with Graph Nets library. The compatibility between keras, tensorflow and Graph Nets is not fully supported. There are some grey areas which make all the standard implementation fails. Specifically, it is needed among other things to use the Adam optimizer of the Sonnet module. This was only discovered thanks to the notebook from the demo of graph nets 2 (the only example compatible with tensorflow2) called sort.py at the link:\n",
    "Minor adjustment in the source code are needed to run the notebook sort.ipynb. \n",
    "The pipeline for this notebook was: \n",
    "0) Understanding how the custom gradient implementation of tensorflow behaves\n",
    "1) taking the most naive version from the implementation of the main code\n",
    "2) Comparing step by step the sort.ipynb notebook with the structure needed for our work\n",
    "3) Finding out how to compute gradients (None values were ubiquitos initially)\n",
    "4) subsequently understanding how to apply the optimizer to this naive version \n",
    "5) Increasing the complexity in order to implement IT-SWO \n",
    "    Point 5 can be subsequently divided in subtasks:\n",
    "        5.1)\n",
    "        5.2)\n",
    "        5.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: \n",
    "Of course there are plenty of other examples, this is just a representative one for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 01:47:37.125055: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 01:47:39.306851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-04 01:47:39.307133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-04 01:47:39.307143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-04 01:47:41.079319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-04 01:47:41.117585: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-04 01:47:41.122960: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-04-04 01:47:41.122999: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-04-04 01:47:41.124752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "@tf.custom_gradient\n",
    "def custom_relu2(x):\n",
    "    def relu_grad(dy):\n",
    "        grad=  tf.cast( x > 0, dtype=tf.float32)*dy\n",
    "        return grad\n",
    "    return tf.maximum(x, 0.), relu_grad\n",
    "data = tf.Variable([0.5, -0.2, 0.0, -2.5, 3.0], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 0. 0. 2. 3.], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.94]\n",
      "tf.Tensor([0.94 0.   0.   1.88 2.82], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.88]\n",
      "tf.Tensor([0.88      0.        0.        1.76      2.6399999], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.82]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_relu(x):\n",
    "    def relu_forward(x):\n",
    "        return tf.maximum(x, 0)\n",
    "    \n",
    "    def relu_grad(dy):\n",
    "        grad = tf.cast(x > 0, dtype=tf.float32) * dy\n",
    "        return grad\n",
    "\n",
    "    return relu_forward(x), relu_grad\n",
    "\n",
    "# Define a trainable variable\n",
    "w = tf.Variable([1.0], dtype=tf.float32)  # A simple weight variable\n",
    "\n",
    "# Define an input\n",
    "x = tf.constant([1.0, -1.0, -2.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for step in range(3):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = custom_relu(w * x)  # Apply ReLU to a linear transformation of x\n",
    "        print(y, type(y))\n",
    "        # Compute gradients of y with respect to w\n",
    "        gradients = tape.gradient(y, w)\n",
    "        print(gradients)\n",
    "        # Apply gradients to update w\n",
    "        optimizer.apply_gradients(zip([gradients], [w]))\n",
    "\n",
    "        print(\"Updated w:\", w.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "from graph_nets import modules\n",
    "\n",
    "hidden_layer_size=4   #This will be 128\n",
    "output_emb_size=4      #This has to be 64 at the end\n",
    "# Define the MLP model\n",
    "class MLPModel_glob(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_glob, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='Glob_layer')\n",
    "       \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "        \n",
    "        return out\n",
    "class MLPModel_enc(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_enc, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='enc_layer')\n",
    "        \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the Encoder layer\n",
    "class Encoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_enc,\n",
    "            node_model_fn=MLPModel_enc,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Encoder, self).__call__(inputs)\n",
    "\n",
    "    \n",
    "class MLPModel_dec(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_dec, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='dec_layer')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Decoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_dec,\n",
    "            node_model_fn=MLPModel_dec,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Decoder, self).__call__(inputs)    \n",
    "class PoolingLayer_double(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingLayer_double, self).__init__()\n",
    "        self.linear = snt.Linear(output_size=2, name='linear_pool')\n",
    "        self.global_transform = snt.Linear(output_size=2, name='global_transform')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Sum-pooling over nodes and edges\n",
    "        pooled_nodes = tf.reduce_sum(inputs.nodes, axis=0)\n",
    "        pooled_edges = tf.reduce_sum(inputs.edges, axis=0)\n",
    "        pooled_features = tf.concat([pooled_nodes, pooled_edges], axis=0)\n",
    "        \n",
    "        transformed = self.linear(tf.expand_dims(pooled_features, axis=0))\n",
    "        \n",
    "        # Transform globals to match the shape of transformed\n",
    "        transformed_globals = self.global_transform(0.05 *inputs.globals)\n",
    "        #### THIS IS THE MOST RELEVANT PART, why again I can not use elu here? Is something related to the metric as well\n",
    "        out = tf.nn.elu(transformed + transformed_globals)\n",
    "        \n",
    "        return out    \n",
    "class GNN_double_output(snt.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN_double_output, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooling_layer = PoolingLayer_double()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "\n",
    "        output = self.pooling_layer(encoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.15899014472961426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "\n",
    "# Set a random seed\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding = np.zeros((num_sites, 2))  # Two sublattices\n",
    "sublattice_encoding[::2, 0] = 1  # Sublattice 1\n",
    "sublattice_encoding[1::2, 1] = -1  # Sublattice 2\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "# Use the dictionary to set the node attributes in the graph\n",
    "#nx.set_node_attributes(G, node_dict)\n",
    "# Add 'features' to nodes\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # Replace with your actual edge features\n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge # Replace with your actual edge features\n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of configurations to generate\n",
    "n_configs = 6\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], n_graph, axis=0)], axis=2)\n",
    "\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "bias_value=0.05\n",
    "# Create a list of graph dicts\n",
    "graph_tuples = []\n",
    "for i in range(n_graph):\n",
    "    graph_dict = {\n",
    "        'globals': np.array([0.05]),\n",
    "        'nodes': node_features[i],\n",
    "        'edges': np.full((edge_index_duplicated.shape[1], 1), bias_value),\n",
    "        'senders': edge_index_duplicated[0],\n",
    "        'receivers': edge_index_duplicated[1]\n",
    "    }\n",
    "    \n",
    "    # Convert to a GraphsTuple and append to the list\n",
    "    graph_tuples.append(utils_tf.data_dicts_to_graphs_tuple([graph_dict]))\n",
    "\n",
    "\n",
    "print(\"end time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.8287344387280773, shape=(), dtype=float64) tf.Tensor(-0.5772058972920167, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "simple_gnn = GNN_double_output()\n",
    "for i in range(1):\n",
    "    a,b =simple_gnn(graph_tuples[i])[0]\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_loss_function(amplitude, phase):\n",
    "    \"\"\"A mock loss function for illustration purposes h\"\"\"\n",
    "    return tf.abs(1- phase*amplitude**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 to 4\n",
    "Most of the work here was done by trial and error and a lot of print statements among other strategies. \n",
    "The end result looks decently clean but the hidden work behind was quite intensive.\n",
    "Observations: sonnet modules seems to not be compatible with keras optimizers built on tensorflow. We need to use the sonnet optimizers.\n",
    "Furthermore, the sonnet optimizer does not allow for a learning schedule with Exponential decay as was intended to be used initially. \n",
    "The only useful implementation is of the standard Adam optimization routine. (Trivial SGD is inadequate for so many parameters, ca va sans dire)\n",
    "No fancy version of Adam are available, maybe it is for the best though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.optimizers.schedules.learning_rate_schedule.ExponentialDecay object at 0x7f02540c3d90>\n"
     ]
    }
   ],
   "source": [
    "import sonnet as snt\n",
    "initial_learning_rate = 7e-3\n",
    "decay_steps = 8 * 1e5\n",
    "decay_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "print(lr_schedule)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.99, clipnorm=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " tf.Tensor([ 1.83 -0.58], shape=(2,), dtype=float64)\n",
      "tf.Tensor(1.8287344387280773, shape=(), dtype=float64) tf.Tensor(-0.5772058972920167, shape=(), dtype=float64) types <class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Is it lossing: \n",
      " tf.Tensor(2.9303321626082566, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: 2.9303321626082566\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 7e-3\n",
    "optimizer = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "\n",
    "def train_step(model, input_graph, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(input_graph)[0]\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "        amplitude, phase = output\n",
    "        print(amplitude, phase, \"types\", type(amplitude), type(phase))\n",
    "            \n",
    "        loss = mock_loss_function(amplitude,phase)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "# Training loop\n",
    "for step in range(1):\n",
    "    #print(graph_tuples[0])\n",
    "    outputs, loss = train_step(simple_gnn, graph_tuples[0], optimizer)\n",
    "    if step % 1 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "### 5.1\n",
    "In this section of the notebook we are going to implement the real loss function and attempt to train a simplified model of our GNN on a 2x2 square lattice. \n",
    "The following implementation will rely on scipy sparse.\n",
    "This implementation is suboptimal and is done for educational purpose. The reason is that implementing a Sparse Matrix is resource intensive regardless. The key reasoning is that one does not need to fully implement the matrix, just vector matrix multiplications, and those follow some specific patterns due to ortoghonality, plase look at step 5.2 for an implementation that does not need explicitly the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell block we generate the input graph and initialize the edges and node values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.10031771659851074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN, here they are set to 4 because the \n",
    "#total number of states is 2**4, and any value above is equivalent to optimizing the GNN on the whole hilbert space.\n",
    "# We are interested in the main pipeline, and want to generalize the network to larger scales thereafter\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice with periodic boundary conditions. This is expecially useful in larger graphs where\n",
    "# having periodic boundary conditions allows for a more decent approximation of an infinite lattice, for small size graphs\n",
    "# periodic boundary conditions have a little effect.\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding = np.zeros((num_sites, 2))  # Two sublattices\n",
    "sublattice_encoding[::2, 0] = 1  # Sublattice 1\n",
    "sublattice_encoding[1::2, 1] = -1  # Sublattice 2\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "\n",
    "# Add 'features' to nodes, in this case the feature is a one hot vectore that represents the sublattice encoding of each node\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # \n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge \n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of batches of configurations to generate, here is set to 4 because 4x4=2**4= Hilbert space, and we don't need more than that\n",
    "n_configs = 4\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features. I BELIEVE THIS WAY TO ENCODE THE NODE FEATURES IS NOT RIGHT\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], n_graph, axis=0)], axis=2)\n",
    "#print(node_features)\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "#Initialization of edges, it should be 0, we set it to 0.05.\n",
    "bias_value=0.05\n",
    "# Create a list of graph dicts\n",
    "graph_tuples = []\n",
    "for i in range(n_graph):\n",
    "    graph_dict = {\n",
    "        'globals': np.array([0.05]),\n",
    "        'nodes': node_features[i],\n",
    "        'edges': np.full((edge_index_duplicated.shape[1], 1), bias_value),\n",
    "        'senders': edge_index_duplicated[0],\n",
    "        'receivers': edge_index_duplicated[1]\n",
    "    }\n",
    "    \n",
    "    # Convert to a GraphsTuple and append to the list\n",
    "    graph_tuples.append(utils_tf.data_dicts_to_graphs_tuple([graph_dict]))\n",
    "\n",
    "\n",
    "print(\"end time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from compgraph.gnn_src_code import GNN_double_output\n",
    "\n",
    "from compgraph.sparse_ham import create_spin_operators, construct_sparse_hamiltonian, sites_to_sparse\n",
    "\n",
    "configurations, value_list= sites_to_sparse(basis_configs)\n",
    "# Uncomment the following line to see the distribution of the initial random configurations\n",
    "#plt.plot(np.sort(value_list))\n",
    "#Here we noticed that the second environment created is not compatible for some obscure reasons, probably qutip compatibility. We are gonna stick to this one for now. ASK patrick\n",
    "\n",
    "J2=2.0\n",
    "spin_operators=create_spin_operators(G)\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, spin_operators, J2)\n",
    "less_trivial_gnn=GNN_double_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 16 16\n",
      "WE   (0, 0)\t(0.47056779182690023+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-0.26-0.03j -0.33+0.04j -0.33+0.04j -0.43+0.09j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((0.6910316613287101+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor([-0.04+0.j  0.  +0.j  0.  +0.j -0.06-0.j -0.06-0.j  0.  +0.j -0.16+0.j], shape=(7,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: [-0.04+0.j  0.  +0.j  0.  +0.j -0.06-0.j -0.06-0.j  0.  +0.j -0.16+0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-0.69+0.38j -0.74+0.54j -0.74+0.54j -0.8 +0.67j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((1.7973742282104006+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-0.83+0.j    0.  +0.j    0.  +0.j   -1.25-0.j   -1.25-0.j    0.  +0.j\n",
      " -2.26+0.01j], shape=(7,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: [-0.83+0.j    0.  +0.j    0.  +0.j   -1.25-0.j   -1.25-0.j    0.  +0.j\n",
      " -2.26+0.01j]\n",
      "4 8 16 16\n",
      "WE   (0, 0)\t(34.50630851111905+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[13  0]\n",
      " [14  0]], shape=(2, 2), dtype=int64), values=tf.Tensor([-0.96+1.91j -0.95+1.78j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((2.9361310536871934+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor([  0.  +0.j     0.  +0.j   -13.05+0.01j -11.61-0.01j], shape=(4,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: [  0.  +0.j     0.  +0.j   -13.05+0.01j -11.61-0.01j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[13  0]\n",
      " [14  0]], shape=(2, 2), dtype=int64), values=tf.Tensor([-0.99+3.34j -0.98+3.02j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((4.710918290334819+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor([  0.  +0.j     0.  +0.j   -55.72+0.04j -46.15-0.04j], shape=(4,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: [  0.  +0.j     0.  +0.j   -55.72+0.04j -46.15-0.04j]\n",
      "8 12 16 16\n",
      "WE   (0, 0)\t(55.39509211211525+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 4  0]\n",
      " [14  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([-0.98+2.78j -0.99+3.42j -1.  +4.56j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((6.475747517885756+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[ -42.1 +0.j    0.  +0.j    0.  +0.j  -81.99+0.j    0.  +0.j    0.  +0.j\n",
      "    0.  +0.j    0.  +0.j -141.22-0.j], shape=(9,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: [ -42.1 +0.j    0.  +0.j    0.  +0.j  -81.99+0.j    0.  +0.j    0.  +0.j\n",
      "    0.  +0.j    0.  +0.j -141.22-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 4  0]\n",
      " [14  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([-0.99+3.97j -1.  +4.95j -1.  +6.69j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((9.260115328844416+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-116.31+0.j    0.  +0.j    0.  +0.j -236.29+0.j    0.  +0.j    0.  +0.j\n",
      "    0.  +0.j    0.  +0.j -423.26-0.j], shape=(9,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: [-116.31+0.j    0.  +0.j    0.  +0.j -236.29+0.j    0.  +0.j    0.  +0.j\n",
      "    0.  +0.j    0.  +0.j -423.26-0.j]\n",
      "12 16 16 16\n",
      "WE   (0, 0)\t(199.6115077213272+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+8.25j -1.+7.75j -1.+7.75j -1.+7.44j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((14.910807272437543+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-1002.51+2.79j     0.  +0.j    -727.78-0.07j  -727.78-0.07j\n",
      "  -778.9 -2.65j     0.  +0.j  ], shape=(6,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: [-1002.51+2.79j     0.  +0.j    -727.78-0.07j  -727.78-0.07j\n",
      "  -778.9 -2.65j     0.  +0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+11.1j  -1.+10.43j -1.+10.43j -1.+10.01j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((19.98694736884204+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-2414.16+5.02j     0.  +0.j   -1753.68-0.13j -1753.68-0.13j\n",
      " -1874.58-4.76j     0.  +0.j  ], shape=(6,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: [-2414.16+5.02j     0.  +0.j   -1753.68-0.13j -1753.68-0.13j\n",
      " -1874.58-4.76j     0.  +0.j  ]\n",
      "0 4 16 16\n",
      "WE   (0, 0)\t(668.286979688587+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+10.62j -1.+14.2j  -1.+14.2j  -1.+15.03j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((26.568414367737784+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-2266.81+0.j       0.  +0.j       0.  +0.j   -4431.37-0.55j\n",
      " -4431.37-0.55j     0.  +0.j   -6945.34+1.1j ], shape=(7,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: [-2266.81+0.j       0.  +0.j       0.  +0.j   -4431.37-0.55j\n",
      " -4431.37-0.55j     0.  +0.j   -6945.34+1.1j ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+14.67j -1.+19.5j  -1.+19.5j  -1.+20.5j ], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((36.3809634378524+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[ -5901.07+0.j       0.  +0.j       0.  +0.j  -11431.06-0.9j\n",
      " -11431.06-0.9j      0.  +0.j  -17654.67+1.8j], shape=(7,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: [ -5901.07+0.j       0.  +0.j       0.  +0.j  -11431.06-0.9j\n",
      " -11431.06-0.9j      0.  +0.j  -17654.67+1.8j]\n",
      "4 8 16 16\n",
      "WE   (0, 0)\t(8581.453772075904+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[13  0]\n",
      " [14  0]], shape=(2, 2), dtype=int64), values=tf.Tensor([-1.+34.14j -1.+31.22j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((46.3017634371986+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor([     0.  +0.j        0.  +0.j   -52783.1 +3.38j -43941.23-3.38j], shape=(4,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: [     0.  +0.j        0.  +0.j   -52783.1 +3.38j -43941.23-3.38j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[13  0]\n",
      " [14  0]], shape=(2, 2), dtype=int64), values=tf.Tensor([-1.+46.73j -1.+42.81j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((63.41474965683544+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor([      0.  +0.j         0.  +0.j   -135370.87+6.21j -113121.81-6.21j], shape=(4,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: [      0.  +0.j         0.  +0.j   -135370.87+6.21j -113121.81-6.21j]\n",
      "8 12 16 16\n",
      "WE   (0, 0)\t(9117.664740742559+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 4  0]\n",
      " [14  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([-1.+38.13j -1.+46.66j -1.+58.59j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((82.83446397435775+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[ -90407.89+0.j       0.  +0.j       0.  +0.j -180460.46+0.j\n",
      "       0.  +0.j       0.  +0.j       0.  +0.j       0.  +0.j\n",
      " -284397.33-0.j], shape=(9,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: [ -90407.89+0.j       0.  +0.j       0.  +0.j -180460.46+0.j\n",
      "       0.  +0.j       0.  +0.j       0.  +0.j       0.  +0.j\n",
      " -284397.33-0.j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 4  0]\n",
      " [14  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([-1.+52.03j -1.+64.05j -1.+80.54j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((113.66545679071241+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-230844.29+0.j       0.  +0.j       0.  +0.j -466406.97+0.j\n",
      "       0.  +0.j       0.  +0.j       0.  +0.j       0.  +0.j\n",
      " -737454.51-0.j], shape=(9,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: [-230844.29+0.j       0.  +0.j       0.  +0.j -466406.97+0.j\n",
      "       0.  +0.j       0.  +0.j       0.  +0.j       0.  +0.j\n",
      " -737454.51-0.j]\n",
      "12 16 16 16\n",
      "WE   (0, 0)\t(29499.896120086283+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+99.77j -1.+96.52j -1.+96.52j -1.+90.59j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((181.51391075241767+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-1753033.24+363.14j        0.    +0.j   -1354171.64 +12.18j\n",
      " -1354171.64 +12.18j -1380039.61-387.5j         0.    +0.j  ], shape=(6,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: [-1753033.24+363.14j        0.    +0.j   -1354171.64 +12.18j\n",
      " -1354171.64 +12.18j -1380039.61-387.5j         0.    +0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+133.03j -1.+128.89j -1.+128.89j -1.+120.67j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((242.12668991423143+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-4157387.27+648.74j        0.    +0.j   -3221130.67 +24.63j\n",
      " -3221130.67 +24.63j -3265535.  -698.j          0.    +0.j  ], shape=(6,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: [-4157387.27+648.74j        0.    +0.j   -3221130.67 +24.63j\n",
      " -3221130.67 +24.63j -3265535.  -698.j          0.    +0.j  ]\n",
      "0 4 16 16\n",
      "WE   (0, 0)\t(99269.13527989422+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+130.59j -1.+175.99j -1.+175.99j -1.+181.05j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((324.5614549980446+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[ -4151260.24 +0.j           0.   +0.j           0.   +0.j\n",
      "  -8286592.01-41.03j  -8286592.01-41.03j         0.   +0.j\n",
      " -12249751.4 +82.05j], shape=(7,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: [ -4151260.24 +0.j           0.   +0.j           0.   +0.j\n",
      "  -8286592.01-41.03j  -8286592.01-41.03j         0.   +0.j\n",
      " -12249751.4 +82.05j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-1.+180.72j -1.+243.29j -1.+243.29j -1.+249.44j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((448.1541022117579+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-10978247.53  +0.j           0.    +0.j           0.    +0.j\n",
      " -21868277.14 -68.89j -21868277.14 -68.89j         0.    +0.j\n",
      " -32102241.24+137.78j], shape=(7,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: [-10978247.53  +0.j           0.    +0.j           0.    +0.j\n",
      " -21868277.14 -68.89j -21868277.14 -68.89j         0.    +0.j\n",
      " -32102241.24+137.78j]\n",
      "4 8 16 16\n",
      "WE   (0, 0)\t(1229703.2300547296+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[13  0]\n",
      " [14  0]], shape=(2, 2), dtype=int64), values=tf.Tensor([-1.+405.85j -1.+377.22j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((554.2720044206252+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[        0.    +0.j           0.    +0.j   -89175854.65+396.77j\n",
      " -76747971.26-396.77j], shape=(4,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: [        0.    +0.j           0.    +0.j   -89175854.65+396.77j\n",
      " -76747971.26-396.77j]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[13  0]\n",
      " [14  0]], shape=(2, 2), dtype=int64), values=tf.Tensor([-1.+567.7j  -1.+528.37j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor((775.8037480733587+0j), shape=(), dtype=complex128) This is the coefficient at denominator\n",
      "Is it lossing: \n",
      " tf.Tensor([ 0.00e+00  +0.j    0.00e+00  +0.j   -2.44e+08+762.88j -2.11e+08-762.88j], shape=(4,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: [ 0.00e+00  +0.j    0.00e+00  +0.j   -2.44e+08+762.88j -2.11e+08-762.88j]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#reload(compgraph.tensor_wave_functions)\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr,  innerprod_sparse\n",
    "from compgraph.tensor_wave_functions import adjust_dtype_and_multiply, convert_csr_to_sparse_tensor\n",
    "\n",
    "#THE inner training step should work for one configuration at a time if logic has any ground on reality\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "optimizer_snt = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "Hamiltonian_tensor = convert_csr_to_sparse_tensor(Hamiltonian)\n",
    "beta=0.005\n",
    "def evolving_function(wave, Ham_tensor,beta):\n",
    "    \"\"\"This loss function is for illustrative purposes\"\"\"\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, -beta)\n",
    "    #print(wave.indices,phi.indices)\n",
    "    \n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    #print(phi)\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    overlap=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    normalization=tf.math.sqrt(norm_wave*norm_ito_wave)\n",
    "    print(normalization, 'This is the coefficient at denominator')\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    return -overlap_normalized.values[:]\n",
    "def wave_function(model, graph_batch, graph_batch_indices):\n",
    "    unique_data = {}  # Dictionary to store unique indices and their corresponding values\n",
    "    size=2**len(graph_batch[0].nodes)\n",
    "    # Compute the wave function components for each graph tuple\n",
    "    for idx, graph_tuple in enumerate(graph_batch):\n",
    "        #print(graph_batch_indices, type(graph_batch_indices))\n",
    "        # Extract the row index from the configuration\n",
    "        row_index = graph_batch_indices[idx].indices[0]\n",
    "        # Check if the index is already in the dictionary\n",
    "        if row_index in unique_data:\n",
    "            pass  # Sum up the values for repeated indices\n",
    "        else:\n",
    "            amplitude, phase = model(graph_tuple)[0]\n",
    "            # Convert amplitude to complex tensor with zero imaginary part\n",
    "            complex_coefficient=tf.complex(real=amplitude, imag=phase)\n",
    "\n",
    "        \n",
    "            unique_data[row_index] = complex_coefficient  # Add new index and value to the dictionary\n",
    "    \n",
    "    # Convert dictionary to lists\n",
    "    values = list(unique_data.values())\n",
    "    indices = [[key, 0] for key in unique_data.keys()]\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    values_tensor = tf.stack(values, axis=0)\n",
    "    indices_tensor = tf.constant(indices, dtype=tf.int64)\n",
    "    \n",
    "    # Create a sparse tensor\n",
    "    sparse_tensor = tf.sparse.SparseTensor(indices=indices_tensor, values=values_tensor, dense_shape=[size, 1])\n",
    "    \n",
    "    return tf.sparse.reorder(sparse_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sparse_rep_inner_training_step(model, graph_batch, graph_batch_indices, target_phi, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = wave_function(model, graph_batch, graph_batch_indices)\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "            \n",
    "        loss = evolving_function(output, Hamiltonian_tensor, beta)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer_snt.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "start=0\n",
    "for step in range(10):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "    start =start%len(graph_tuples)\n",
    "    end=(start + batch_size)\n",
    "    graph_tuples_batch=graph_tuples[start:end]\n",
    "    graph_tuples_batch_indices= configurations[start:end]\n",
    "    print(start,end, len(configurations),len(graph_tuples))\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "    \n",
    "    beta = 0.05 #This parameter determines the amount of imaginary time evolution at each outer step\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    print(\"WE\", innerprod_sparse(phi_csr, phi_csr))\n",
    "    for innerstep in range(2):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "\n",
    "        outputs, loss = sparse_rep_inner_training_step(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices, phi_csr, optimizer)\n",
    "       \n",
    "        #print(outputs,loss)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "        \n",
    "        \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[12  0]\n",
      " [ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-0.01-0.01j -0.01-0.01j -0.01-0.01j -0.01-0.01j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64)) SparseTensor(indices=tf.Tensor(\n",
      "[[12  0]\n",
      " [ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([0.1 +0.1j  0.13+0.15j 0.1 +0.11j 0.1 +0.11j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64)) SparseTensor(indices=tf.Tensor(\n",
      "[[12  0]\n",
      " [ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([-0.  +0.02j -0.01+0.04j -0.  +0.02j -0.  +0.02j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor([-0.  +0.02j -0.01+0.04j -0.  +0.02j -0.  +0.02j], shape=(4,), dtype=complex128)\n",
      "tf.Tensor([0.02+0.j 0.04+0.j 0.02+0.j 0.02+0.j], shape=(4,), dtype=complex128)\n",
      "  (0, 0)\t(0.6784907728433609+0.777226909995079j)\n",
      "  (3, 0)\t(0.11025918275117874+0.11102091521024704j)\n",
      "  (5, 0)\t(0.5167662873864174+0.5440657064318657j)\n",
      "  (6, 0)\t(0.38159093633294106+0.38719334453344345j)\n",
      "  (9, 0)\t(0.38159093633294106+0.38719334453344345j)\n",
      "  (10, 0)\t(0.5167662873864174+0.5440657064318657j)\n",
      "  (12, 0)\t(-0.2962479218840599-0.3220238760113716j)\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 3  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [12  0]], shape=(7, 2), dtype=int64), values=tf.Tensor(\n",
      "[ 0.68+0.78j  0.11+0.11j  0.52+0.54j  0.38+0.39j  0.38+0.39j  0.52+0.54j\n",
      " -0.3 -0.32j], shape=(7,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]], shape=(4, 2), dtype=int64)\n",
      "tf.Tensor([ 0.-0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j  0.+0.j -0.-0.j], shape=(7,), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "import compgraph\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "indexing= np.column_stack((Hamiltonian.nonzero()))\n",
    "values_H= Hamiltonian.data\n",
    "shape_H = Hamiltonian.shape\n",
    "H_tensor= tf.sparse.SparseTensor(indexing,values_H,shape_H)\n",
    "H_tensor = tf.sparse.reorder(H_tensor)\n",
    "H2_tensor = convert_csr_to_sparse_tensor(Hamiltonian)\n",
    "output=  wave_function(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices)\n",
    "output_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "auxphi=tf.sparse.map_values(tf.multiply,output, -0.05)\n",
    "auxphi2=tf.sparse.map_values(tf.add,output,auxphi)\n",
    "prod=tf.sparse.map_values(tf.multiply,output,auxphi2)\n",
    "print(auxphi, auxphi2,prod) \n",
    "print(prod.values[:])\n",
    "conj_output = tf.sparse.map_values(tf.math.conj, output)\n",
    "prod=tf.sparse.map_values(tf.multiply,conj_output,output)\n",
    "print(prod.values[:])\n",
    "\n",
    "phi_csr = Hamiltonian.dot(output_csr)\n",
    "print(phi_csr)\n",
    "phi_sparse_coo = phi_csr.tocoo()\n",
    "indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex128)\n",
    "print(phi_sparse_tf)\n",
    "\n",
    "phi= adjust_dtype_and_multiply(H2_tensor,tf.sparse.reorder(output))\n",
    "\n",
    "print(tf.sparse.reorder(output).indices)\n",
    "print(phi.values-phi_sparse_tf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([2, 0, 2, 0, 1, 2])\n",
    "data = np.array([1+ 0.2j, 2, 3, 4, 5, 6])\n",
    "arturo=csr_matrix((data, (row, col)), shape=(4, 4))\n",
    "data = [0.2, 0.6+0.5j, 0.8+0.7j ]  # List to store the non-zero entries\n",
    "row_indices = [0,1,2]  # List to store the row indices\n",
    "col_indices = [0]*3\n",
    "gomez=csr_matrix((data, (row_indices, col_indices)), shape=(4, 1))\n",
    "print(gomez,'\\n ARTURO',arturo.toarray())\n",
    "print(arturo.dot(gomez))\n",
    "indexing= np.column_stack((arturo.nonzero()))\n",
    "values_arturo= arturo.data\n",
    "shape_arturo = arturo.shape\n",
    "arturo_tensor= tf.sparse.SparseTensor(indexing,values_arturo,shape_arturo)\n",
    "arturo_tensor = tf.sparse.reorder(arturo_tensor)\n",
    "arturo_tensor = convert_csr_to_sparse_tensor(arturo)\n",
    "print(arturo_tensor._numpy())\n",
    "phi_sparse_coo = gomez.tocoo()\n",
    "indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex128)\n",
    "print(phi_sparse_tf)\n",
    "out_tensor = adjust_dtype_and_multiply(arturo_tensor,phi_sparse_tf)\n",
    "print(out_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 In the previous section we managed to perform all the operations we wanted to do while preserving the gradient. Hurra! \n",
    "In this section we will include the montecarlo update and try to stabilize the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphNet2",
   "language": "python",
   "name": "graphnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
