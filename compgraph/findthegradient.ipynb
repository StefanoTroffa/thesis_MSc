{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook was built with the purpose of understanding how to compute gradients and optimization routines with Graph Nets library. The compatibility between keras, tensorflow and Graph Nets is not fully supported. There are some grey areas which make all the standard implementation fails. Specifically, it is needed among other things to use the Adam optimizer of the Sonnet module. This was only discovered thanks to the notebook from the demo of graph nets 2 (the only example compatible with tensorflow2) called sort.py at the link:\n",
    "Minor adjustment in the source code are needed to run the notebook sort.ipynb. \n",
    "The pipeline for this notebook was: \n",
    "0) Understanding how the custom gradient implementation of tensorflow behaves\n",
    "1) taking the most naive version from the implementation of the main code\n",
    "2) Comparing step by step the sort.ipynb notebook with the structure needed for our work\n",
    "3) Finding out how to compute gradients (None values were ubiquitos initially)\n",
    "4) subsequently understanding how to apply the optimizer to this naive version \n",
    "5) Increasing the complexity in order to implement IT-SWO \n",
    "    Point 5 can be subsequently divided in subtasks:\n",
    "        5.1)\n",
    "        5.2)\n",
    "        5.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: \n",
    "Of course there are plenty of other examples, this is just a representative one for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "@tf.custom_gradient\n",
    "def custom_relu2(x):\n",
    "    def relu_grad(dy):\n",
    "        grad=  tf.cast( x > 0, dtype=tf.float32)*dy\n",
    "        return grad\n",
    "    return tf.maximum(x, 0.), relu_grad\n",
    "data = tf.Variable([0.5, -0.2, 0.0, -2.5, 3.0], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 0. 0. 2. 3.], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.94]\n",
      "tf.Tensor([0.94 0.   0.   1.88 2.82], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.88]\n",
      "tf.Tensor([0.88      0.        0.        1.76      2.6399999], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.82]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_relu(x):\n",
    "    def relu_forward(x):\n",
    "        return tf.maximum(x, 0)\n",
    "    \n",
    "    def relu_grad(dy):\n",
    "        grad = tf.cast(x > 0, dtype=tf.float32) * dy\n",
    "        return grad\n",
    "\n",
    "    return relu_forward(x), relu_grad\n",
    "\n",
    "# Define a trainable variable\n",
    "w = tf.Variable([1.0], dtype=tf.float32)  # A simple weight variable\n",
    "\n",
    "# Define an input\n",
    "x = tf.constant([1.0, -1.0, -2.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for step in range(3):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = custom_relu(w * x)  # Apply ReLU to a linear transformation of x\n",
    "        print(y, type(y))\n",
    "        # Compute gradients of y with respect to w\n",
    "        gradients = tape.gradient(y, w)\n",
    "        print(gradients)\n",
    "        # Apply gradients to update w\n",
    "        optimizer.apply_gradients(zip([gradients], [w]))\n",
    "\n",
    "        print(\"Updated w:\", w.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "from graph_nets import modules\n",
    "\n",
    "hidden_layer_size=4   #This will be 128\n",
    "output_emb_size=4      #This has to be 64 at the end\n",
    "# Define the MLP model\n",
    "class MLPModel_glob(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_glob, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='Glob_layer')\n",
    "       \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "        \n",
    "        return out\n",
    "class MLPModel_enc(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_enc, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='enc_layer')\n",
    "        \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the Encoder layer\n",
    "class Encoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_enc,\n",
    "            node_model_fn=MLPModel_enc,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Encoder, self).__call__(inputs)\n",
    "\n",
    "    \n",
    "class MLPModel_dec(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_dec, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='dec_layer')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Decoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_dec,\n",
    "            node_model_fn=MLPModel_dec,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Decoder, self).__call__(inputs)    \n",
    "class PoolingLayer_double(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingLayer_double, self).__init__()\n",
    "        self.linear = snt.Linear(output_size=2, name='linear_pool')\n",
    "        self.global_transform = snt.Linear(output_size=2, name='global_transform')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Sum-pooling over nodes and edges\n",
    "        pooled_nodes = tf.reduce_sum(inputs.nodes, axis=0)\n",
    "        pooled_edges = tf.reduce_sum(inputs.edges, axis=0)\n",
    "        pooled_features = tf.concat([pooled_nodes, pooled_edges], axis=0)\n",
    "        \n",
    "        transformed = self.linear(tf.expand_dims(pooled_features, axis=0))\n",
    "        \n",
    "        # Transform globals to match the shape of transformed\n",
    "        transformed_globals = self.global_transform(0.05 *inputs.globals)\n",
    "        #### THIS IS THE MOST RELEVANT PART, why again I can not use elu here? Is something related to the metric as well\n",
    "        out = tf.nn.elu(transformed + transformed_globals)\n",
    "        \n",
    "        return out    \n",
    "class GNN_double_output(snt.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN_double_output, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooling_layer = PoolingLayer_double()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "\n",
    "        output = self.pooling_layer(encoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.18071293830871582\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "\n",
    "# Set a random seed\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding = np.zeros((num_sites, 2))  # Two sublattices\n",
    "sublattice_encoding[::2, 0] = 1  # Sublattice 1\n",
    "sublattice_encoding[1::2, 1] = -1  # Sublattice 2\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "# Use the dictionary to set the node attributes in the graph\n",
    "#nx.set_node_attributes(G, node_dict)\n",
    "# Add 'features' to nodes\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # Replace with your actual edge features\n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge # Replace with your actual edge features\n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of configurations to generate\n",
    "n_configs = 6\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], n_graph, axis=0)], axis=2)\n",
    "\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "bias_value=0.05\n",
    "# Create a list of graph dicts\n",
    "graph_tuples = []\n",
    "for i in range(n_graph):\n",
    "    graph_dict = {\n",
    "        'globals': np.array([0.05]),\n",
    "        'nodes': node_features[i],\n",
    "        'edges': np.full((edge_index_duplicated.shape[1], 1), bias_value),\n",
    "        'senders': edge_index_duplicated[0],\n",
    "        'receivers': edge_index_duplicated[1]\n",
    "    }\n",
    "    \n",
    "    # Convert to a GraphsTuple and append to the list\n",
    "    graph_tuples.append(utils_tf.data_dicts_to_graphs_tuple([graph_dict]))\n",
    "\n",
    "\n",
    "print(\"end time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.8287344387280773, shape=(), dtype=float64) tf.Tensor(-0.5772058972920167, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "simple_gnn = GNN_double_output()\n",
    "for i in range(1):\n",
    "    a,b =simple_gnn(graph_tuples[i])[0]\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_loss_function(amplitude, phase):\n",
    "    \"\"\"A mock loss function for illustration purposes h\"\"\"\n",
    "    return tf.abs(1- phase*amplitude**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 to 4\n",
    "Most of the work here was done by trial and error and a lot of print statements among other strategies. \n",
    "The end result looks decently clean but the hidden work behind was quite intensive.\n",
    "Observations: sonnet modules seems to not be compatible with keras optimizers built on tensorflow. We need to use the sonnet optimizers.\n",
    "Furthermore, the sonnet optimizer does not allow for a learning schedule with Exponential decay as was intended to be used initially. \n",
    "The only useful implementation is of the standard Adam optimization routine. (Trivial SGD is inadequate for so many parameters, ca va sans dire)\n",
    "No fancy version of Adam are available, maybe it is for the best though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.optimizers.schedules.learning_rate_schedule.ExponentialDecay object at 0x7f5f7d776750>\n"
     ]
    }
   ],
   "source": [
    "import sonnet as snt\n",
    "initial_learning_rate = 7e-3\n",
    "decay_steps = 8 * 1e5\n",
    "decay_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "print(lr_schedule)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.99, clipnorm=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " tf.Tensor([ 1.83 -0.58], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(2.9303321626082566, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: 2.9303321626082566\n",
      "output: \n",
      " tf.Tensor([ 1.59 -0.54], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(2.3729231933832953, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: 2.3729231933832953\n",
      "output: \n",
      " tf.Tensor([ 1.36 -0.51], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.9366390097094244, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: 1.9366390097094244\n",
      "output: \n",
      " tf.Tensor([ 1.14 -0.47], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.609849140081862, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: 1.609849140081862\n",
      "output: \n",
      " tf.Tensor([ 0.93 -0.43], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.3734278567262748, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: 1.3734278567262748\n",
      "output: \n",
      " tf.Tensor([ 0.73 -0.4 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.2088565748776472, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: 1.2088565748776472\n",
      "output: \n",
      " tf.Tensor([ 0.54 -0.36], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.1039952063823248, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: 1.1039952063823248\n",
      "output: \n",
      " tf.Tensor([ 0.36 -0.32], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0426510552442942, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: 1.0426510552442942\n",
      "output: \n",
      " tf.Tensor([ 0.2  -0.29], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0117642700547094, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: 1.0117642700547094\n",
      "output: \n",
      " tf.Tensor([ 0.05 -0.26], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.000735016786883, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: 1.000735016786883\n",
      "output: \n",
      " tf.Tensor([-0.08 -0.24], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0013760023336082, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 10, Loss: 1.0013760023336082\n",
      "output: \n",
      " tf.Tensor([-0.18 -0.21], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0065229803743019, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 11, Loss: 1.0065229803743019\n",
      "output: \n",
      " tf.Tensor([-0.25 -0.19], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0117760909238864, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 12, Loss: 1.0117760909238864\n",
      "output: \n",
      " tf.Tensor([-0.31 -0.15], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0149721965652652, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 13, Loss: 1.0149721965652652\n",
      "output: \n",
      " tf.Tensor([-0.36 -0.12], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.015180298889238, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 14, Loss: 1.015180298889238\n",
      "output: \n",
      " tf.Tensor([-0.39 -0.08], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.0120282296272924, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 15, Loss: 1.0120282296272924\n",
      "output: \n",
      " tf.Tensor([-0.42 -0.03], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(1.005411879553728, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 16, Loss: 1.005411879553728\n",
      "output: \n",
      " tf.Tensor([-0.44  0.02], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.9953722938632975, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 17, Loss: 0.9953722938632975\n",
      "output: \n",
      " tf.Tensor([-0.45  0.09], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.9825134450720224, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 18, Loss: 0.9825134450720224\n",
      "output: \n",
      " tf.Tensor([-0.46  0.15], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.9672734968940133, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 19, Loss: 0.9672734968940133\n",
      "output: \n",
      " tf.Tensor([-0.47  0.22], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.9495530550707714, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 20, Loss: 0.9495530550707714\n",
      "output: \n",
      " tf.Tensor([-0.49  0.3 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.9288507770063802, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 21, Loss: 0.9288507770063802\n",
      "output: \n",
      " tf.Tensor([-0.5   0.38], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.9048317405322308, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 22, Loss: 0.9048317405322308\n",
      "output: \n",
      " tf.Tensor([-0.52  0.46], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.8755567256554021, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 23, Loss: 0.8755567256554021\n",
      "output: \n",
      " tf.Tensor([-0.55  0.54], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.8397643293354018, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 24, Loss: 0.8397643293354018\n",
      "output: \n",
      " tf.Tensor([-0.57  0.62], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.7956482193968756, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 25, Loss: 0.7956482193968756\n",
      "output: \n",
      " tf.Tensor([-0.6   0.71], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.7412000736011093, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 26, Loss: 0.7412000736011093\n",
      "output: \n",
      " tf.Tensor([-0.63  0.81], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.6742434961831599, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 27, Loss: 0.6742434961831599\n",
      "output: \n",
      " tf.Tensor([-0.67  0.92], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.5916466670845637, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 28, Loss: 0.5916466670845637\n",
      "output: \n",
      " tf.Tensor([-0.7   1.03], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.48960390125302733, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 29, Loss: 0.48960390125302733\n",
      "output: \n",
      " tf.Tensor([-0.74  1.16], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.3673990100008534, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 30, Loss: 0.3673990100008534\n",
      "output: \n",
      " tf.Tensor([-0.77  1.3 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.22354560837307436, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 31, Loss: 0.22354560837307436\n",
      "output: \n",
      " tf.Tensor([-0.8   1.46], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.057045178560223175, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 32, Loss: 0.057045178560223175\n",
      "output: \n",
      " tf.Tensor([-0.83  1.63], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.1348592017283099, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 33, Loss: 0.1348592017283099\n",
      "output: \n",
      " tf.Tensor([-0.85  1.74], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.26291897643520756, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 34, Loss: 0.26291897643520756\n",
      "output: \n",
      " tf.Tensor([-0.86  1.79], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.3316246922219417, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 35, Loss: 0.3316246922219417\n",
      "output: \n",
      " tf.Tensor([-0.87  1.79], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.3500367091549339, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 36, Loss: 0.3500367091549339\n",
      "output: \n",
      " tf.Tensor([-0.87  1.76], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.3281397766565104, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 37, Loss: 0.3281397766565104\n",
      "output: \n",
      " tf.Tensor([-0.87  1.7 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.2758042752436358, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 38, Loss: 0.2758042752436358\n",
      "output: \n",
      " tf.Tensor([-0.86  1.62], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.19924397713953668, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 39, Loss: 0.19924397713953668\n",
      "output: \n",
      " tf.Tensor([-0.85  1.52], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.1051671821937632, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 40, Loss: 0.1051671821937632\n",
      "output: \n",
      " tf.Tensor([-0.84  1.41], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.0005692809270193555, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 41, Loss: 0.0005692809270193555\n",
      "output: \n",
      " tf.Tensor([-0.83  1.34], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06615870956615066, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 42, Loss: 0.06615870956615066\n",
      "output: \n",
      " tf.Tensor([-0.83  1.3 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.10021168737195973, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 43, Loss: 0.10021168737195973\n",
      "output: \n",
      " tf.Tensor([-0.83  1.29], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.10872746763556362, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 44, Loss: 0.10872746763556362\n",
      "output: \n",
      " tf.Tensor([-0.83  1.3 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.09531505777751981, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 45, Loss: 0.09531505777751981\n",
      "output: \n",
      " tf.Tensor([-0.84  1.33], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06217983217482259, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 46, Loss: 0.06217983217482259\n",
      "output: \n",
      " tf.Tensor([-0.85  1.38], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.010558859722394454, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 47, Loss: 0.010558859722394454\n",
      "output: \n",
      " tf.Tensor([-0.85  1.45], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.058984533109739834, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 48, Loss: 0.058984533109739834\n",
      "output: \n",
      " tf.Tensor([-0.86  1.49], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.09889927891669825, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 49, Loss: 0.09889927891669825\n",
      "output: \n",
      " tf.Tensor([-0.86  1.5 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.11110702060440514, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 50, Loss: 0.11110702060440514\n",
      "output: \n",
      " tf.Tensor([-0.86  1.48], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.0984999742186643, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 51, Loss: 0.0984999742186643\n",
      "output: \n",
      " tf.Tensor([-0.86  1.45], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06451057460672005, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 52, Loss: 0.06451057460672005\n",
      "output: \n",
      " tf.Tensor([-0.85  1.4 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.012806369442459609, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 53, Loss: 0.012806369442459609\n",
      "output: \n",
      " tf.Tensor([-0.84  1.33], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.05292070240551239, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 54, Loss: 0.05292070240551239\n",
      "output: \n",
      " tf.Tensor([-0.84  1.29], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.09072316966449223, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 55, Loss: 0.09072316966449223\n",
      "output: \n",
      " tf.Tensor([-0.84  1.27], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.10526451809551152, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 56, Loss: 0.10526451809551152\n",
      "output: \n",
      " tf.Tensor([-0.84  1.28], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.09968212340542404, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 57, Loss: 0.09968212340542404\n",
      "output: \n",
      " tf.Tensor([-0.84  1.3 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.07600373648693026, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 58, Loss: 0.07600373648693026\n",
      "output: \n",
      " tf.Tensor([-0.85  1.34], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.035444066232865, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 59, Loss: 0.035444066232865\n",
      "output: \n",
      " tf.Tensor([-0.85  1.4 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.021381600706272375, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 60, Loss: 0.021381600706272375\n",
      "output: \n",
      " tf.Tensor([-0.86  1.43], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.05271484113436986, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 61, Loss: 0.05271484113436986\n",
      "output: \n",
      " tf.Tensor([-0.86  1.44], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06017787916807227, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 62, Loss: 0.06017787916807227\n",
      "output: \n",
      " tf.Tensor([-0.86  1.42], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.046137527163394365, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 63, Loss: 0.046137527163394365\n",
      "output: \n",
      " tf.Tensor([-0.85  1.39], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.013413771838818755, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 64, Loss: 0.013413771838818755\n",
      "output: \n",
      " tf.Tensor([-0.85  1.34], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.03494213429790938, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 65, Loss: 0.03494213429790938\n",
      "output: \n",
      " tf.Tensor([-0.85  1.31], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.05882512299125886, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 66, Loss: 0.05882512299125886\n",
      "output: \n",
      " tf.Tensor([-0.85  1.31], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.061733417425427706, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 67, Loss: 0.061733417425427706\n",
      "output: \n",
      " tf.Tensor([-0.85  1.33], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.04600765070003221, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 68, Loss: 0.04600765070003221\n",
      "output: \n",
      " tf.Tensor([-0.85  1.36], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.01312862939303594, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 69, Loss: 0.01312862939303594\n",
      "output: \n",
      " tf.Tensor([-0.86  1.41], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.036064647777819925, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 70, Loss: 0.036064647777819925\n",
      "output: \n",
      " tf.Tensor([-0.86  1.44], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06105377628272013, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 71, Loss: 0.06105377628272013\n",
      "output: \n",
      " tf.Tensor([-0.86  1.44], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06352718265749413, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 72, Loss: 0.06352718265749413\n",
      "output: \n",
      " tf.Tensor([-0.86  1.42], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.045801072743594906, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 73, Loss: 0.045801072743594906\n",
      "output: \n",
      " tf.Tensor([-0.85  1.38], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.01057569832842331, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 74, Loss: 0.01057569832842331\n",
      "output: \n",
      " tf.Tensor([-0.85  1.33], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.03925323700273209, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 75, Loss: 0.03925323700273209\n",
      "output: \n",
      " tf.Tensor([-0.85  1.31], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.06518761195627643, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 76, Loss: 0.06518761195627643\n",
      "output: \n",
      " tf.Tensor([-0.85  1.3 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.07065387104644916, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 77, Loss: 0.07065387104644916\n",
      "output: \n",
      " tf.Tensor([-0.85  1.31], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.05797517249912565, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 78, Loss: 0.05797517249912565\n",
      "output: \n",
      " tf.Tensor([-0.85  1.34], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.02864720639086793, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 79, Loss: 0.02864720639086793\n",
      "output: \n",
      " tf.Tensor([-0.86  1.39], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.016457651803437834, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 80, Loss: 0.016457651803437834\n",
      "output: \n",
      " tf.Tensor([-0.86  1.41], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.038617329243098686, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 81, Loss: 0.038617329243098686\n",
      "output: \n",
      " tf.Tensor([-0.86  1.41], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.03946333399425095, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 82, Loss: 0.03946333399425095\n",
      "output: \n",
      " tf.Tensor([-0.86  1.39], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.021193372064217453, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 83, Loss: 0.021193372064217453\n",
      "output: \n",
      " tf.Tensor([-0.85  1.36], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.013644387806243663, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 84, Loss: 0.013644387806243663\n",
      "output: \n",
      " tf.Tensor([-0.85  1.35], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.026494732901719664, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 85, Loss: 0.026494732901719664\n",
      "output: \n",
      " tf.Tensor([-0.85  1.35], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.02007251138151689, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 86, Loss: 0.02007251138151689\n",
      "output: \n",
      " tf.Tensor([-0.85  1.38], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.0038240859827438634, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 87, Loss: 0.0038240859827438634\n",
      "output: \n",
      " tf.Tensor([-0.85  1.38], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.007065286952506966, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 88, Loss: 0.007065286952506966\n",
      "output: \n",
      " tf.Tensor([-0.85  1.36], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.008326706584274368, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 89, Loss: 0.008326706584274368\n",
      "output: \n",
      " tf.Tensor([-0.85  1.37], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.004022918363162042, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 90, Loss: 0.004022918363162042\n",
      "output: \n",
      " tf.Tensor([-0.86  1.39], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.018076524962989282, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 91, Loss: 0.018076524962989282\n",
      "output: \n",
      " tf.Tensor([-0.86  1.39], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.019586624719469592, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 92, Loss: 0.019586624719469592\n",
      "output: \n",
      " tf.Tensor([-0.85  1.37], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.0025704920307116907, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 93, Loss: 0.0025704920307116907\n",
      "output: \n",
      " tf.Tensor([-0.85  1.34], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.030563368435367555, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 94, Loss: 0.030563368435367555\n",
      "output: \n",
      " tf.Tensor([-0.85  1.33], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.042500591450370884, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 95, Loss: 0.042500591450370884\n",
      "output: \n",
      " tf.Tensor([-0.85  1.33], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.03580640309281713, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 96, Loss: 0.03580640309281713\n",
      "output: \n",
      " tf.Tensor([-0.85  1.36], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.012182984737918012, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 97, Loss: 0.012182984737918012\n",
      "output: \n",
      " tf.Tensor([-0.86  1.4 ], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.02732206462046083, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 98, Loss: 0.02732206462046083\n",
      "output: \n",
      " tf.Tensor([-0.86  1.42], shape=(2,), dtype=float64)\n",
      "Is it lossing: \n",
      " tf.Tensor(0.04477673356173195, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 99, Loss: 0.04477673356173195\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 7e-3\n",
    "optimizer = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "\n",
    "def train_step(model, input_graph, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(input_graph)[0]\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "        amplitude, phase = output\n",
    "        #print(amplitude, phase, \"types\", type(amplitude), type(phase))\n",
    "            \n",
    "        loss = mock_loss_function(amplitude,phase)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "# Training loop\n",
    "for step in range(100):\n",
    "    #print(graph_tuples[0])\n",
    "    outputs, loss = train_step(simple_gnn, graph_tuples[0], optimizer)\n",
    "    if step % 1 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "### 5.1\n",
    "In this section of the notebook we are going to implement the real loss function and attempt to train a simplified model of our GNN on a 2x2 square lattice. \n",
    "The following implementation will rely on scipy sparse.\n",
    "This implementation is suboptimal and is done for educational purpose. The reason is that implementing a Sparse Matrix is resource intensive regardless. The key reasoning is that one does not need to fully implement the matrix, just vector matrix multiplications, and those follow some specific patterns due to ortoghonality, plase look at step 5.2 for an implementation that does not need explicitly the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell block we generate the input graph and initialize the edges and node values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.15488624572753906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN, here they are set to 4 because the \n",
    "#total number of states is 2**4, and any value above is equivalent to optimizing the GNN on the whole hilbert space.\n",
    "# We are interested in the main pipeline, and want to generalize the network to larger scales thereafter\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice with periodic boundary conditions. This is expecially useful in larger graphs where\n",
    "# having periodic boundary conditions allows for a more decent approximation of an infinite lattice, for small size graphs\n",
    "# periodic boundary conditions have a little effect.\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding = np.zeros((num_sites, 2))  # Two sublattices\n",
    "sublattice_encoding[::2, 0] = 1  # Sublattice 1\n",
    "sublattice_encoding[1::2, 1] = -1  # Sublattice 2\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "\n",
    "# Add 'features' to nodes, in this case the feature is a one hot vectore that represents the sublattice encoding of each node\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # \n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge \n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of batches of configurations to generate, here is set to 4 because 4x4=2**4= Hilbert space, and we don't need more than that\n",
    "n_configs = 4\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features. I BELIEVE THIS WAY TO ENCODE THE NODE FEATURES IS NOT RIGHT\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], n_graph, axis=0)], axis=2)\n",
    "#print(node_features)\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "#Initialization of edges, it should be 0, we set it to 0.05.\n",
    "bias_value=0.05\n",
    "# Create a list of graph dicts\n",
    "graph_tuples = []\n",
    "for i in range(n_graph):\n",
    "    graph_dict = {\n",
    "        'globals': np.array([0.05]),\n",
    "        'nodes': node_features[i],\n",
    "        'edges': np.full((edge_index_duplicated.shape[1], 1), bias_value),\n",
    "        'senders': edge_index_duplicated[0],\n",
    "        'receivers': edge_index_duplicated[1]\n",
    "    }\n",
    "    \n",
    "    # Convert to a GraphsTuple and append to the list\n",
    "    graph_tuples.append(utils_tf.data_dicts_to_graphs_tuple([graph_dict]))\n",
    "\n",
    "\n",
    "print(\"end time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from autoreload import reload #If needed just use reload(module)\n",
    "from compgraph.gnn_src_code import GNN_double_output\n",
    "\n",
    "from compgraph.sparse_ham import create_spin_operators, construct_sparse_hamiltonian, sites_to_sparse\n",
    "\n",
    "configurations, value_list= sites_to_sparse(basis_configs)\n",
    "# Uncomment the following line to see the distribution of the initial random configurations\n",
    "#plt.plot(np.sort(value_list))\n",
    "#Here we noticed that the second environment created is not compatible for some obscure reasons, probably qutip compatibility. We are gonna stick to this one for now. ASK patrick\n",
    "\n",
    "J2=2.0\n",
    "spin_operators=create_spin_operators(G)\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, spin_operators, J2)\n",
    "less_trivial_gnn=GNN_double_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE   (0, 0)\t(0.3125555033502897+3.469446951953614e-18j)\n",
      "output: \n",
      "   (0, 0)\t(0.14928362+0.16245468j)\n",
      "  (6, 0)\t(-0.10122831+0.3109828j)\n",
      "  (9, 0)\t(-0.10122831+0.3109828j)\n",
      "  (12, 0)\t(-0.083175875+0.28215635j)\n",
      "  (0, 0)\t(0.3125555033502897-3.469446951953614e-18j) <class 'scipy.sparse.csc.csc_matrix'>\n",
      "tf.Tensor((0.3229670333137129+0j), shape=(), dtype=complex128)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\nTraceback (most recent call last):\n\n  File \"/home/stefanotroffa/miniconda3/envs/GraphNet2/lib/python3.7/site-packages/scipy/sparse/base.py\", line 291, in __len__\n    raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\nTypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9062/3368659628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minnerstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Inner loop iterations: here we let psi approximate its ITO phi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_rep_inner_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mless_trivial_gnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_tuples_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_tuples_batch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_csr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9062/3368659628.py\u001b[0m in \u001b[0;36msparse_rep_inner_training_step\u001b[0;34m(model, graph_batch, graph_batch_indices, target_phi, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_sparse_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is it lossing: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9062/3368659628.py\u001b[0m in \u001b[0;36mloss_sparse_vectors\u001b[0;34m(psi_sparse, phi_sparse)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnorm_sqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_rep_inner_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_batch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GraphNet2/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GraphNet2/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\nTraceback (most recent call last):\n\n  File \"/home/stefanotroffa/miniconda3/envs/GraphNet2/lib/python3.7/site-packages/scipy/sparse/base.py\", line 291, in __len__\n    raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\nTypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#reload(compgraph.tensor_wave_functions)\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr,  innerprod_sparse\n",
    "#THE inner training step should work for one configuration at a time if logic has any ground on reality\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "def loss_sparse_vectors(psi_sparse, phi_sparse):\n",
    "    psi_norm= innerprod_sparse(psi_sparse,psi_sparse)\n",
    "    phi_norm= innerprod_sparse(phi_sparse,phi_sparse)\n",
    "    norm_sqrt = tf.math.sqrt(psi_norm[0,0] * phi_norm[0,0]);\n",
    "    numerator = innerprod_sparse(psi_sparse, phi_sparse)\n",
    "    print(numerator, type(numerator))\n",
    "    print(norm_sqrt)\n",
    "    loss= tf.constant(1.0, dtype=tf.float32)-numerator/norm_sqrt\n",
    "    return loss\n",
    "def sparse_rep_inner_training_step(model, graph_batch, graph_batch_indices, target_phi, optimizer):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = compute_wave_function_csr(graph_batch, less_trivial_gnn, graph_batch_indices)\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "        loss = loss_sparse_vectors(output,target_phi)\n",
    "            \n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "start=0\n",
    "for step in range(3):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "    graph_tuples_batch=graph_tuples[start:start + batch_size]\n",
    "    graph_tuples_batch_indices= configurations[start:start + batch_size]\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "    \n",
    "    beta = 0.05\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    print(\"WE\", innerprod_sparse(phi_csr, psi_csr))\n",
    "    for innerstep in range(5):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "\n",
    "        outputs, loss = sparse_rep_inner_training_step(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices, phi_csr, optimizer)\n",
    "        print(outputs,loss)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "        \n",
    "        print(gradients[0][0], \"step\", step)\n",
    "        \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.43  0.09], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.26 -0.03], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.33  0.04], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.33  0.04], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.56  0.2 ], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.56  0.2 ], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.51  0.21], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.51  0.21], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.26 -0.03], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.51  0.21], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.26 -0.03], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.37  0.06], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.33  0.04], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.43  0.09], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.32  0.05], shape=(2,), dtype=float64)\n",
      "tf.Tensor([-0.33  0.04], shape=(2,), dtype=float64)\n",
      "final_time 0.28047657012939453\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "reload(compgraph.tensor_wave_functions)\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr\n",
    "from compgraph.tensor_wave_functions import compute_wave_function_sparse_tensor\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "for step in range(3):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "    graph_tuples_batch=graph_tuples[start:start + batch_size]\n",
    "    graph_tuples_batch_indices= configurations[start:start + batch_size]\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, gnn, graph_tuples_batch_indices)\n",
    "    beta = 0.05\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    phi_sparse_coo = phi_csr.tocoo()\n",
    "    indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "    phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex64)\n",
    "\n",
    "    for innerstep in range(5):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            tape.watch(gnn.trainable_variables)\n",
    "            psi = compute_wave_function_sparse_tensor(graph_tuples_batch, gnn, graph_tuples_batch_indices)\n",
    "            #print(psi, phi_sparse_tf)\n",
    "            loss = compute_loss_tensor_updated(psi, phi_sparse_tf)\n",
    "            print(loss)\n",
    "        \n",
    "        time_s = time.time()\n",
    "        gradients = tape.gradient(loss, gnn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, gnn.trainable_variables))\n",
    "\n",
    "        # Update graph_tuples and configurations using Sequential Monte Carlo updates\n",
    "        updated_tuples_and_configs = sequential_monte_carlo_update(\n",
    "            graph_tuples[start:start + batch_size], gnn, N_sweeps\n",
    "        )\n",
    "        for i, (updated_graph_tuple, updated_configuration) in enumerate(updated_tuples_and_configs):\n",
    "            graph_tuples[start + i] = updated_graph_tuple\n",
    "            configurations[start + i] = updated_configuration\n",
    "\n",
    "        print(time.time() - time_s, \"Monte Carlo update with N_sweeps\", N_sweeps)\n",
    "    \n",
    "    print(gradients[0][0], \"step\", step)\n",
    "    \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphNet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
