{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook was built with the purpose of understanding how to compute gradients and optimization routines with Graph Nets library. The compatibility between keras, tensorflow and Graph Nets is not fully supported. There are some grey areas which make all the standard implementation fails. Specifically, it is needed among other things to use the Adam optimizer of the Sonnet module. This was only discovered thanks to the notebook from the demo of graph nets 2 (the only example compatible with tensorflow2) called sort.py at the link:\n",
    "Minor adjustment in the source code are needed to run the notebook sort.ipynb. \n",
    "The pipeline for this notebook was: \n",
    "0) Understanding how the custom gradient implementation of tensorflow behaves\n",
    "1) taking the most naive version from the implementation of the main code\n",
    "2) Comparing step by step the sort.ipynb notebook with the structure needed for our work\n",
    "3) Finding out how to compute gradients (None values were ubiquitos initially)\n",
    "4) subsequently understanding how to apply the optimizer to this naive version \n",
    "5) Increasing the complexity in order to implement IT-SWO \n",
    "    Point 5 can be subsequently divided in subtasks:\n",
    "        5.1)\n",
    "        5.2)\n",
    "        5.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: \n",
    "Of course there are plenty of other examples, this is just a representative one for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 01:24:55.891064: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 01:24:55.895044: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 01:24:55.958554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 01:24:56.725151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-23 01:24:58.105329: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-23 01:24:58.106713: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "@tf.custom_gradient\n",
    "def custom_relu2(x):\n",
    "    def relu_grad(dy):\n",
    "        grad=  tf.cast( x > 0, dtype=tf.float32)*dy\n",
    "        return grad\n",
    "    return tf.maximum(x, 0.), relu_grad\n",
    "data = tf.Variable([0.5, -0.2, 0.0, -2.5, 3.0], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 0. 0. 2. 3.], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.94]\n",
      "tf.Tensor([0.94 0.   0.   1.88 2.82], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.88]\n",
      "tf.Tensor([0.88      0.        0.        1.76      2.6399999], shape=(5,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([6.], shape=(1,), dtype=float32)\n",
      "Updated w: [0.82]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_relu(x):\n",
    "    def relu_forward(x):\n",
    "        return tf.maximum(x, 0)\n",
    "    \n",
    "    def relu_grad(dy):\n",
    "        grad = tf.cast(x > 0, dtype=tf.float32) * dy\n",
    "        return grad\n",
    "\n",
    "    return relu_forward(x), relu_grad\n",
    "\n",
    "# Define a trainable variable\n",
    "w = tf.Variable([1.0], dtype=tf.float32)  # A simple weight variable\n",
    "\n",
    "# Define an input\n",
    "x = tf.constant([1.0, -1.0, -2.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for step in range(3):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = custom_relu(w * x)  # Apply ReLU to a linear transformation of x\n",
    "        print(y, type(y))\n",
    "        # Compute gradients of y with respect to w\n",
    "        gradients = tape.gradient(y, w)\n",
    "        print(gradients)\n",
    "        # Apply gradients to update w\n",
    "        optimizer.apply_gradients(zip([gradients], [w]))\n",
    "\n",
    "        print(\"Updated w:\", w.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "from graph_nets import modules\n",
    "\n",
    "hidden_layer_size=4   #This will be 128\n",
    "output_emb_size=4      #This has to be 64 at the end\n",
    "# Define the MLP model\n",
    "class MLPModel_glob(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_glob, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='Glob_layer')\n",
    "       \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "        \n",
    "        return out\n",
    "class MLPModel_enc(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_enc, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='enc_layer')\n",
    "        \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the Encoder layer\n",
    "class Encoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_enc,\n",
    "            node_model_fn=MLPModel_enc,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Encoder, self).__call__(inputs)\n",
    "\n",
    "    \n",
    "class MLPModel_dec(snt.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(MLPModel_dec, self).__init__(name=name)\n",
    "        self.layer1 = snt.Linear(output_size=hidden_layer_size, name='dec_layer')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        out = tf.nn.relu(self.layer1(inputs))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Decoder(modules.GraphNetwork):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__(\n",
    "            edge_model_fn=MLPModel_dec,\n",
    "            node_model_fn=MLPModel_dec,\n",
    "            global_model_fn=MLPModel_glob\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return super(Decoder, self).__call__(inputs)    \n",
    "class PoolingLayer_double(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingLayer_double, self).__init__()\n",
    "        self.linear = snt.Linear(output_size=2, name='linear_pool')\n",
    "        self.global_transform = snt.Linear(output_size=2, name='global_transform')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Sum-pooling over nodes and edges\n",
    "        pooled_nodes = tf.reduce_sum(inputs.nodes, axis=0)\n",
    "        pooled_edges = tf.reduce_sum(inputs.edges, axis=0)\n",
    "        pooled_features = tf.concat([pooled_nodes, pooled_edges], axis=0)\n",
    "        \n",
    "        transformed = self.linear(tf.expand_dims(pooled_features, axis=0))\n",
    "        \n",
    "        # Transform globals to match the shape of transformed\n",
    "        transformed_globals = self.global_transform(0.05 *inputs.globals)\n",
    "        #### THIS IS THE MOST RELEVANT PART, why again I can not use elu here? Is something related to the metric as well\n",
    "        out = tf.nn.elu(transformed + transformed_globals)\n",
    "        \n",
    "        return out    \n",
    "class GNN_double_output(snt.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN_double_output, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooling_layer = PoolingLayer_double()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "\n",
    "        output = self.pooling_layer(encoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.10353255271911621\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "from compgraph.useful import node_to_index\n",
    "# Set a random seed\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = node_to_index(G)\n",
    "\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding = np.zeros((num_sites, 2))  # Two sublattices\n",
    "sublattice_encoding[::2, 0] = 1  # Sublattice 1\n",
    "sublattice_encoding[1::2, 1] = 1  # Sublattice 2\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "# Use the dictionary to set the node attributes in the graph\n",
    "#nx.set_node_attributes(G, node_dict)\n",
    "# Add 'features' to nodes\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # Replace with your actual edge features\n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge # Replace with your actual edge features\n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of configurations to generate\n",
    "n_configs = 6\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], len(basis_configs), axis=0)], axis=2)\n",
    "\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "bias_value=0.5\n",
    "# Create a list of graph dicts\n",
    "graph_tuples = []\n",
    "for i in range(n_graph):\n",
    "    graph_dict = {\n",
    "        'globals': np.array([0.05]),\n",
    "        'nodes': node_features[i],\n",
    "        'edges': np.full((edge_index_duplicated.shape[1], 1), bias_value),\n",
    "        'senders': edge_index_duplicated[0],\n",
    "        'receivers': edge_index_duplicated[1]\n",
    "    }\n",
    "    \n",
    "    # Convert to a GraphsTuple and append to the list\n",
    "    graph_tuples.append(utils_tf.data_dicts_to_graphs_tuple([graph_dict]))\n",
    "\n",
    "\n",
    "print(\"end time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs are equal: True\n"
     ]
    }
   ],
   "source": [
    "from useful import create_graph_tuples, compare_graph_tuples\n",
    "graph_tuples2=create_graph_tuples(basis_configs,G,sublattice_encoding)\n",
    "\n",
    "\n",
    "# Usage:\n",
    "result = compare_graph_tuples(graph_tuples2, graph_tuples)\n",
    "print(\"Graphs are equal:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.8513225677819865, shape=(), dtype=float64) tf.Tensor(1.0092861107726787, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "simple_gnn = GNN_double_output()\n",
    "for i in range(1):\n",
    "    a,b =simple_gnn(graph_tuples[i])[0]\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_loss_function(amplitude, phase):\n",
    "    \"\"\"A mock loss function for illustration purposes h\"\"\"\n",
    "    return tf.abs(1- phase*amplitude**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 to 4\n",
    "Most of the work here was done by trial and error and a lot of print statements among other strategies. \n",
    "The end result looks decently clean but the hidden work behind was quite intensive.\n",
    "Observations: sonnet modules seems to not be compatible with keras optimizers built on tensorflow. We need to use the sonnet optimizers.\n",
    "Furthermore, the sonnet optimizer does not allow for a learning schedule with Exponential decay as was intended to be used initially. \n",
    "The only useful implementation is of the standard Adam optimization routine. (Trivial SGD is inadequate for so many parameters, ca va sans dire)\n",
    "No fancy version of Adam are available, maybe it is for the best though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.optimizers.schedules.learning_rate_schedule.ExponentialDecay object at 0x7f6607457020>\n"
     ]
    }
   ],
   "source": [
    "import sonnet as snt\n",
    "initial_learning_rate = 7e-3\n",
    "decay_steps = 8 * 1e5\n",
    "decay_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "print(lr_schedule)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.99, clipnorm=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " tf.Tensor([1.85 1.01], shape=(2,), dtype=float64)\n",
      "tf.Tensor(1.8513225677819865, shape=(), dtype=float64) tf.Tensor(1.0092861107726787, shape=(), dtype=float64) types <class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Is it lossing: \n",
      " tf.Tensor(2.459222421931945, shape=(), dtype=float64) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: 2.459222421931945\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 7e-3\n",
    "optimizer = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "\n",
    "def train_step(model, input_graph, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(input_graph)[0]\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "        amplitude, phase = output\n",
    "        print(amplitude, phase, \"types\", type(amplitude), type(phase))\n",
    "            \n",
    "        loss = mock_loss_function(amplitude,phase)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "# Training loop\n",
    "for step in range(1):\n",
    "    #print(graph_tuples[0])\n",
    "    outputs, loss = train_step(simple_gnn, graph_tuples[0], optimizer)\n",
    "    if step % 1 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "### 5.1\n",
    "In this section of the notebook we are going to implement the real loss function and attempt to train a simplified model of our GNN on a 2x2 square lattice. \n",
    "The following implementation will rely on scipy sparse.\n",
    "This implementation is suboptimal and is done for educational purpose. The reason is that implementing a Sparse Matrix is resource intensive regardless. The key reasoning is that one does not need to fully implement the matrix, just vector matrix multiplications, and those follow some specific patterns due to ortoghonality, plase look at step 5.2 for an implementation that does not need explicitly the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell block we generate the input graph and initialize the edges and node values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 0.09561657905578613\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from graph_nets import utils_np, utils_tf\n",
    "from compgraph.useful import neel_state, create_graph_tuples\n",
    "start=time.time()\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define the lattice size\n",
    "lattice_size = (2,2)\n",
    "#Batch size, the number of graphs that will be computed simultaneously by the GNN, here they are set to 4 because the \n",
    "#total number of states is 2**4, and any value above is equivalent to optimizing the GNN on the whole hilbert space.\n",
    "# We are interested in the main pipeline, and want to generalize the network to larger scales thereafter\n",
    "batch_size= 4\n",
    "\n",
    "# Create a square lattice with periodic boundary conditions. This is expecially useful in larger graphs where\n",
    "# having periodic boundary conditions allows for a more decent approximation of an infinite lattice, for small size graphs\n",
    "# periodic boundary conditions have a little effect.\n",
    "G = nx.grid_2d_graph(*lattice_size, periodic=True)\n",
    "# Number of sites\n",
    "num_sites = lattice_size[0] * lattice_size[1]\n",
    "\n",
    "# Relabel the nodes to use integers\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "# Initialize the sublattice encoding\n",
    "sublattice_encoding= neel_state(G)\n",
    "# Create a dictionary where the keys are the node indices and the values are dictionaries\n",
    "# containing the 'features' field and the corresponding sublattice encoding\n",
    "node_dict = {i: {\"features\": sublattice_encoding[i]} for i in range(num_sites)}\n",
    "\n",
    "# Add 'features' to nodes, in this case the feature is a one hot vectore that represents the sublattice encoding of each node\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['features'] = sublattice_encoding[node]\n",
    "# Add 'features' to edges\n",
    "# Add 'features' to edges\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    G.edges[u, v]['features'] = [1.0]  # \n",
    "    G.edges[v, u]['features'] = [1.0]  # Add undirected edge \n",
    "# Now convert the networkx graph to a GraphsTuple\n",
    "graph_tuple = utils_np.networkxs_to_graphs_tuple([G])\n",
    "\n",
    "# Number of batches of configurations to generate, here is set to 4 because 4x4=2**4= Hilbert space, and we don't need more than that\n",
    "n_configs = 4\n",
    "n_graph=n_configs*batch_size\n",
    "# Generate the basis configurations\n",
    "basis_configs = np.random.randint(2, size=(n_graph, num_sites)) * 2 - 1  # Random spins (-1 or 1)\n",
    "\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features.\n",
    "node_features = np.concatenate([basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], n_graph, axis=0)], axis=2)\n",
    "#print(node_features)\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "#Initialization of edges, it should be 0, we set it to 0.05.\n",
    "\n",
    "\n",
    "\n",
    "graph_tuples=create_graph_tuples(basis_configs, G, sublattice_encoding)\n",
    "print(\"end time:\", time.time()-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "  (0, 1)\t(1+0j)\n",
      "  (0, 4)\t(1+0j)\n",
      "  (1, 5)\t(1+0j)\n",
      "  (2, 6)\t(1+0j)\n",
      "  (3, 7)\t(1+0j)\n",
      "  (8, 12)\t(1+0j)\n",
      "  (9, 13)\t(1+0j)\n",
      "  (10, 14)\t(1+0j)\n",
      "  (11, 15)\t(1+0j)\n",
      "[<1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>, <1x16 sparse matrix of type '<class 'numpy.int8'>'\n",
      "\twith 1 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from compgraph.gnn_src_code import GNN_double_output\n",
    "\n",
    "from compgraph.sparse_ham import create_spin_operators_qutipv5, construct_sparse_hamiltonian, sites_to_sparse\n",
    "\n",
    "configurations, value_list= sites_to_sparse(basis_configs)\n",
    "# Uncomment the following line to see the distribution of the initial random configurations\n",
    "#plt.plot(np.sort(value_list))\n",
    "#Here we noticed that the second environment created is not compatible for some obscure reasons, probably qutip compatibility. We are gonna stick to this one for now. ASK patrick\n",
    "\n",
    "J2=2.0\n",
    "Hamiltonian = construct_sparse_hamiltonian(G, J2)\n",
    "less_trivial_gnn=GNN_double_output()\n",
    "print(configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(0.1896035568142081+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([ 0.02+0.24j  0.02+0.24j  0.03+0.25j -0.02+0.16j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9637109104334056+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9637109104334056-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: (-0.9637109104334056-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([0.38+1.52j 0.38+1.52j 0.4 +1.46j 0.27+1.43j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9669446471228509+6.502615156263796e-18j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9669446471228509-6.502615156263796e-18j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: (-0.9669446471228509-6.502615156263796e-18j)\n",
      "4 8 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(64.0220253995611+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([1.24+2.69j 0.79+2.57j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9744651809194503+1.3877787807814457e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9744651809194503-1.3877787807814457e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: (-0.9744651809194503-1.3877787807814457e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([2.25+4.26j 1.81+4.16j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9744167570203957-2.0274580625478933e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9744167570203957+2.0274580625478933e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: (-0.9744167570203957+2.0274580625478933e-17j)\n",
      "8 12 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(261.33282268991513+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([3.34+6.28j 3.91+7.01j 3.91+6.9j ], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9762365679746219+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9762365679746219-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: (-0.9762365679746219-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([5.49 +9.24j 6.26+10.22j 6.32+10.1j ], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9762365558113661-1.8980381469212266e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9762365558113661+1.8980381469212266e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: (-0.9762365558113661+1.8980381469212266e-17j)\n",
      "12 16 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(883.943916531248+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([9.07+13.9j  9.15+14.32j 9.15+14.32j 8.6 +13.52j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9772459270157333+1.440633636690114e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9772459270157333-1.440633636690114e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: (-0.9772459270157333-1.440633636690114e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([13.28+19.71j 13.48+20.31j 13.48+20.31j 12.71+19.2j ], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9772460638222329-1.3952326707172835e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9772460638222329+1.3952326707172835e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: (-0.9772460638222329+1.3952326707172835e-17j)\n",
      "0 4 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(3915.935806723088+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([19.33+28.45j 19.33+28.45j 18.98+27.63j 19.74+28.49j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9671220301556629-2.6522295644426652e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9671220301556629+2.6522295644426652e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: (-0.9671220301556629+2.6522295644426652e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([27.57+39.89j 27.57+39.89j 27.04+38.84j 28.01+40.j  ], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9670849637267955-2.667580451213667e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9670849637267955+2.667580451213667e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: (-0.9670849637267955+2.667580451213667e-17j)\n",
      "4 8 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(33598.92653313513+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([37.56+53.16j 36.87+52.84j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.974391574547393+2.7091501784981542e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.974391574547393-2.7091501784981542e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: (-0.974391574547393-2.7091501784981542e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([53.5 +75.33j 52.69+74.85j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9743914851046331-2.6861108823328372e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9743914851046331+2.6861108823328372e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: (-0.9743914851046331+2.6861108823328372e-17j)\n",
      "8 12 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(79907.39211729231+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([75.13+105.93j 79.79+112.34j 79.69+112.13j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9762366942855015+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9762366942855015-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: (-0.9762366942855015-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 8  0]\n",
      " [13  0]\n",
      " [15  0]], shape=(3, 2), dtype=int64), values=tf.Tensor([106.93+149.62j 112.89+157.82j 112.85+157.71j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9762369364605803+1.7815473275934884e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9762369364605803-1.7815473275934884e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: (-0.9762369364605803-1.7815473275934884e-17j)\n",
      "12 16 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(230447.29779865066+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([156.85+217.42j 158.85+220.3j  158.85+220.3j  153.62+213.16j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9770310136307476+1.852884572118782e-21j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9770310136307476-1.852884572118782e-21j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: (-0.9770310136307476-1.852884572118782e-21j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 3  0]\n",
      " [ 6  0]\n",
      " [ 9  0]\n",
      " [10  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([221.56+304.24j 224.29+308.07j 224.29+308.07j 217.57+298.95j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9770108442435219-1.0058516248644817e-21j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9770108442435219+1.0058516248644817e-21j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: (-0.9770108442435219+1.0058516248644817e-21j)\n",
      "0 4 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(949942.5756948164+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([315.45+428.9j  315.45+428.9j  311.89+424.03j 318.11+432.7j ], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9667318390055575-1.4030261268684025e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9667318390055575+1.4030261268684025e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: (-0.9667318390055575+1.4030261268684025e-17j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 6  0]\n",
      " [ 9  0]\n",
      " [12  0]\n",
      " [15  0]], shape=(4, 2), dtype=int64), values=tf.Tensor([444.95+598.79j 444.95+598.79j 440.33+592.56j 449.94+605.74j], shape=(4,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9667215876066932+1.4266805933954102e-17j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9667215876066932-1.4266805933954102e-17j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: (-0.9667215876066932-1.4266805933954102e-17j)\n",
      "4 8 16 16\n",
      "Inner product scipy sparse   (0, 0)\t(8167102.582446312+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([608.32+810.13j 604.06+804.96j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9743913173947976+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9743913173947976-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: (-0.9743913173947976-0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[4 0]\n",
      " [8 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([853.83+1125.92j 847.92+1118.64j], shape=(2,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Lossing final tf.Tensor((0.9743913179949655+0j), shape=(), dtype=complex128)\n",
      "Is it lossing: \n",
      " tf.Tensor((-0.9743913179949655-0j), shape=(), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: (-0.9743913179949655-0j)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#reload(compgraph.tensor_wave_functions)\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr,  innerprod_sparse\n",
    "from compgraph.tensor_wave_functions import adjust_dtype_and_multiply, convert_csr_to_sparse_tensor, evolving_function, variational_wave_function_on_batch\n",
    "\n",
    "#THE inner training step should work for one configuration at a time if logic has any ground on reality\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "optimizer_snt = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "Hamiltonian_tensor = convert_csr_to_sparse_tensor(Hamiltonian)\n",
    "beta=0.005\n",
    "def evolving_function(wave, Ham_tensor,beta):\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    beta *= -1\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, beta)\n",
    "    #print(wave.indices,phi.indices)\n",
    "    \n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    #print(phi)\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    overlap=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    normalization=1/(norm_wave*norm_ito_wave)\n",
    "    #print(normalization, 'This is the coefficient at denominator')\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    loss=tf.reduce_sum(overlap_normalized.values)\n",
    "    print('Lossing final', loss)\n",
    "    return -loss\n",
    "def sparse_rep_inner_training_step(model, graph_batch, graph_batch_indices, target_phi, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = variational_wave_function_on_batch(model, graph_batch, graph_batch_indices)\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "            \n",
    "        loss = evolving_function(output, Hamiltonian_tensor, beta)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer_snt.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "start=0\n",
    "for step in range(10):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "    start =start%len(graph_tuples)\n",
    "    end=(start + batch_size)\n",
    "    graph_tuples_batch=graph_tuples[start:end]\n",
    "    graph_tuples_batch_indices= configurations[start:end]\n",
    "    print(start,end, len(configurations),len(graph_tuples))\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "    \n",
    "    beta = 0.05 #This parameter determines the amount of imaginary time evolution at each outer step\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    print(\"Inner product scipy sparse\", innerprod_sparse(phi_csr, phi_csr))\n",
    "    for innerstep in range(2):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "\n",
    "        outputs, loss = sparse_rep_inner_training_step(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices, phi_csr, optimizer)\n",
    "       \n",
    "        #print(outputs,loss)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "        \n",
    "        \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.b\n",
    "In the following we are taking the full Hilbert as configurations and graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1 [-1 -1 -1 -1] 1\n",
      "  (0, 1)\t1 [-1 -1 -1  1] 2\n",
      "  (0, 2)\t1 [-1 -1  1 -1] 3\n",
      "  (0, 3)\t1 [-1 -1  1  1] 4\n",
      "  (0, 4)\t1 [-1  1 -1 -1] 5\n",
      "  (0, 5)\t1 [-1  1 -1  1] 6\n",
      "  (0, 6)\t1 [-1  1  1 -1] 7\n",
      "  (0, 7)\t1 [-1  1  1  1] 8\n",
      "  (0, 8)\t1 [ 1 -1 -1 -1] 9\n",
      "  (0, 9)\t1 [ 1 -1 -1  1] 10\n",
      "  (0, 10)\t1 [ 1 -1  1 -1] 11\n",
      "  (0, 11)\t1 [ 1 -1  1  1] 12\n",
      "  (0, 12)\t1 [ 1  1 -1 -1] 13\n",
      "  (0, 13)\t1 [ 1  1 -1  1] 14\n",
      "  (0, 14)\t1 [ 1  1  1 -1] 15\n",
      "  (0, 15)\t1 [1 1 1 1] 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import compgraph\n",
    "from compgraph.sparse_ham import sites_to_sparse\n",
    "num_sites =len(G.nodes)\n",
    "sub_lattice_encoding= neel_state(G)\n",
    "# Number of batches of configurations to generate, here is set to full Hilbert space\n",
    "full_size_configs = 2**(len(G.nodes))\n",
    "# Generate the basis configurations\n",
    "full_basis_configs = np.array([[int(x) for x in format(i, f'0{num_sites}b')] for i in range(full_size_configs)]) * 2 - 1\n",
    "# Concatenate the basis configurations and the sublattice encoding to form the node features. I BELIEVE THIS WAY TO ENCODE THE NODE FEATURES IS NOT RIGHT\n",
    "full_node_features = np.concatenate([full_basis_configs[:, :, np.newaxis], np.repeat(sublattice_encoding[np.newaxis, :, :], n_graph, axis=0)], axis=2)\n",
    "\n",
    "# Get the edge indices\n",
    "edge_index = np.array(G.edges()).T\n",
    "edge_index_duplicated = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "#Initialization of edges, it should be 0, we set it to 0.05.\n",
    "bias_value=0.05\n",
    "# Create a list of graph dicts\n",
    "full_graph_tuples =create_graph_tuples(full_basis_configs, G, sublattice_encoding)\n",
    "\n",
    "\n",
    "\n",
    "full_configurations, value_list= sites_to_sparse(full_basis_configs)\n",
    "\n",
    "for idx, config in enumerate(full_configurations):\n",
    "    print(np.array(config), full_basis_configs[idx], value_list[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. -1.] [-1 -1  1 -1]\n",
      "  (0, 2)\t1\n",
      "[-1 -1  1 -1]\n",
      "2\n",
      "[-1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from compgraph.useful import graph_tuple_toconfig\n",
    "from compgraph.sparse_ham import sites_to_sparse\n",
    "from compgraph.useful import neel_state, config_list_to_state_list, graph_tuple_list_to_configs_list, config_to_state\n",
    "from compgraph.tensor_wave_functions import config_hamiltonian_product\n",
    "import quimb as qu\n",
    "full_configurations_indices, valuelist= sites_to_sparse(full_basis_configs)\n",
    "\n",
    "config_from_batch=graph_tuple_toconfig(full_graph_tuples[2])\n",
    "\n",
    "print(config_from_batch,full_basis_configs[2])\n",
    "print(graph_tuples_batch_indices[2])\n",
    "print(full_basis_configs[2])\n",
    "print(sites_to_sparse([config_from_batch])[0][0].indices[0])\n",
    "\n",
    "print(config_from_batch)\n",
    "sparse, vt= sites_to_sparse([config_from_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65489be5d0>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((value_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t(1+0j)\n",
      "  (0, 4)\t(1+0j)\n",
      "  (1, 5)\t(1+0j)\n",
      "  (2, 6)\t(1+0j)\n",
      "  (3, 7)\t(1+0j)\n",
      "  (8, 12)\t(1+0j)\n",
      "  (9, 13)\t(1+0j)\n",
      "  (10, 14)\t(1+0j)\n",
      "  (11, 15)\t(1+0j)\n",
      "WE   (0, 0)\t(35754209.01030278+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1211.41+1582.29j 1235.34+1613.79j 1224.97+1600.01j 1224.97+1600.01j\n",
      " 1189.43+1553.25j 1189.43+1553.25j 1181.13+1542.92j 1181.13+1542.92j\n",
      " 1235.34+1613.79j 1181.13+1542.92j 1235.34+1613.79j 1228.85+1605.24j\n",
      " 1224.97+1600.01j 1211.41+1582.29j 1201.18+1569.19j 1224.97+1600.01j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-431.39-0.j   -449.37-0.j   -444.28-0.01j -444.57-0.j   -411.36+0.02j\n",
      " -409.3 +0.03j -408.42-0.j   -407.39+0.j   -449.37-0.j   -408.42-0.j\n",
      " -452.03-0.02j -446.55-0.01j -444.57-0.j   -430.2 +0.01j -425.67-0.j\n",
      " -441.1 -0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: [-431.39-0.j   -449.37-0.j   -444.28-0.01j -444.57-0.j   -411.36+0.02j\n",
      " -409.3 +0.03j -408.42-0.j   -407.39+0.j   -449.37-0.j   -408.42-0.j\n",
      " -452.03-0.02j -446.55-0.01j -444.57-0.j   -430.2 +0.01j -425.67-0.j\n",
      " -441.1 -0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1216.48+1588.99j 1240.58+1620.72j 1230.1 +1606.79j 1230.1 +1606.79j\n",
      " 1194.39+1559.8j  1194.39+1559.8j  1186.06+1549.45j 1186.06+1549.45j\n",
      " 1240.58+1620.72j 1186.06+1549.45j 1240.58+1620.72j 1234.02+1612.08j\n",
      " 1230.1 +1606.79j 1216.48+1588.99j 1206.24+1575.88j 1230.1 +1606.79j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-433.2 -0.j   -451.31-0.j   -446.15-0.01j -446.44-0.j   -413.07+0.02j\n",
      " -410.99+0.03j -410.13-0.j   -409.09+0.j   -451.31-0.j   -410.13-0.j\n",
      " -453.99-0.02j -448.46-0.01j -446.44-0.j   -432.01+0.01j -427.49-0.j\n",
      " -442.96-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 0, Loss: [-433.2 -0.j   -451.31-0.j   -446.15-0.01j -446.44-0.j   -413.07+0.02j\n",
      " -410.99+0.03j -410.13-0.j   -409.09+0.j   -451.31-0.j   -410.13-0.j\n",
      " -453.99-0.02j -448.46-0.01j -446.44-0.j   -432.01+0.01j -427.49-0.j\n",
      " -442.96-0.j  ]\n",
      "WE   (0, 0)\t(36363018.34833353+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1221.57+1595.73j 1245.83+1627.68j 1235.25+1613.61j 1235.25+1613.61j\n",
      " 1199.36+1566.38j 1199.36+1566.38j 1191.02+1556.j   1191.02+1556.j\n",
      " 1245.83+1627.68j 1191.02+1556.j   1245.83+1627.68j 1239.21+1618.96j\n",
      " 1235.25+1613.61j 1221.57+1595.73j 1211.32+1582.61j 1235.25+1613.61j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-435.02-0.j   -453.26-0.j   -448.03-0.01j -448.32-0.j   -414.78+0.02j\n",
      " -412.7 +0.03j -411.85-0.j   -410.8 +0.j   -453.26-0.j   -411.85-0.j\n",
      " -455.97-0.02j -450.37-0.01j -448.32-0.j   -433.82+0.01j -429.31-0.j\n",
      " -444.83+0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: [-435.02-0.j   -453.26-0.j   -448.03-0.01j -448.32-0.j   -414.78+0.02j\n",
      " -412.7 +0.03j -411.85-0.j   -410.8 +0.j   -453.26-0.j   -411.85-0.j\n",
      " -455.97-0.02j -450.37-0.01j -448.32-0.j   -433.82+0.01j -429.31-0.j\n",
      " -444.83+0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1226.68+1602.49j 1251.12+1634.67j 1240.42+1620.45j 1240.42+1620.45j\n",
      " 1204.35+1572.99j 1204.35+1572.99j 1195.99+1562.59j 1195.99+1562.59j\n",
      " 1251.12+1634.67j 1195.99+1562.59j 1251.12+1634.67j 1244.43+1625.86j\n",
      " 1240.42+1620.45j 1226.68+1602.49j 1216.42+1589.36j 1240.42+1620.45j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-436.86-0.j   -455.22-0.j   -449.92-0.01j -450.2 -0.j   -416.51+0.02j\n",
      " -414.42+0.03j -413.57-0.j   -412.52+0.j   -455.22-0.j   -413.57-0.j\n",
      " -457.95-0.02j -452.29-0.01j -450.2 -0.j   -435.65+0.01j -431.15-0.j\n",
      " -446.7 -0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 1, Loss: [-436.86-0.j   -455.22-0.j   -449.92-0.01j -450.2 -0.j   -416.51+0.02j\n",
      " -414.42+0.03j -413.57-0.j   -412.52+0.j   -455.22-0.j   -413.57-0.j\n",
      " -457.95-0.02j -452.29-0.01j -450.2 -0.j   -435.65+0.01j -431.15-0.j\n",
      " -446.7 -0.j  ]\n",
      "WE   (0, 0)\t(36982092.177799866+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1231.82+1609.28j 1256.42+1641.68j 1245.61+1627.32j 1245.61+1627.32j\n",
      " 1209.37+1579.62j 1209.37+1579.62j 1200.99+1569.2j  1200.99+1569.2j\n",
      " 1256.42+1641.68j 1200.99+1569.2j  1256.42+1641.68j 1249.67+1632.79j\n",
      " 1245.61+1627.32j 1231.82+1609.28j 1221.54+1596.13j 1245.61+1627.32j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-438.69-0.j   -457.19-0.j   -451.81-0.01j -452.1 -0.j   -418.24+0.02j\n",
      " -416.14+0.03j -415.31-0.j   -414.25+0.j   -457.19-0.j   -415.31-0.j\n",
      " -459.94-0.02j -454.22-0.01j -452.1 -0.j   -437.48+0.01j -432.99-0.j\n",
      " -448.58-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: [-438.69-0.j   -457.19-0.j   -451.81-0.01j -452.1 -0.j   -418.24+0.02j\n",
      " -416.14+0.03j -415.31-0.j   -414.25+0.j   -457.19-0.j   -415.31-0.j\n",
      " -459.94-0.02j -454.22-0.01j -452.1 -0.j   -437.48+0.01j -432.99-0.j\n",
      " -448.58-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1236.97+1616.1j  1261.75+1648.73j 1250.83+1634.21j 1250.83+1634.21j\n",
      " 1214.4 +1586.28j 1214.4 +1586.28j 1206.01+1575.84j 1206.01+1575.84j\n",
      " 1261.75+1648.73j 1206.01+1575.84j 1261.75+1648.73j 1254.93+1639.75j\n",
      " 1250.83+1634.21j 1236.97+1616.1j  1226.68+1602.94j 1250.83+1634.21j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-440.54-0.j   -459.16-0.j   -453.72-0.01j -454.  -0.j   -419.98+0.02j\n",
      " -417.87+0.03j -417.05-0.j   -415.98+0.j   -459.16-0.j   -417.05-0.j\n",
      " -461.94-0.02j -456.15-0.01j -454.  -0.j   -439.31+0.01j -434.84-0.j\n",
      " -450.47-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 2, Loss: [-440.54-0.j   -459.16-0.j   -453.72-0.01j -454.  -0.j   -419.98+0.02j\n",
      " -417.87+0.03j -417.05-0.j   -415.98+0.j   -459.16-0.j   -417.05-0.j\n",
      " -461.94-0.02j -456.15-0.01j -454.  -0.j   -439.31+0.01j -434.84-0.j\n",
      " -450.47-0.j  ]\n",
      "WE   (0, 0)\t(37611683.57453923+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1242.15+1622.95j 1267.1 +1655.8j  1256.06+1641.14j 1256.06+1641.14j\n",
      " 1219.46+1592.98j 1219.46+1592.98j 1211.05+1582.51j 1211.05+1582.51j\n",
      " 1267.1 +1655.8j  1211.05+1582.51j 1267.1 +1655.8j  1260.21+1646.73j\n",
      " 1256.06+1641.14j 1242.15+1622.95j 1231.85+1609.77j 1256.06+1641.14j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-442.39-0.j   -461.15-0.j   -455.63-0.01j -455.9 -0.j   -421.72+0.02j\n",
      " -419.6 +0.03j -418.8 -0.j   -417.73+0.j   -461.15-0.j   -418.8 -0.j\n",
      " -463.95-0.02j -458.1 -0.01j -455.9 -0.j   -441.16+0.01j -436.69-0.j\n",
      " -452.36-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: [-442.39-0.j   -461.15-0.j   -455.63-0.01j -455.9 -0.j   -421.72+0.02j\n",
      " -419.6 +0.03j -418.8 -0.j   -417.73+0.j   -461.15-0.j   -418.8 -0.j\n",
      " -463.95-0.02j -458.1 -0.01j -455.9 -0.j   -441.16+0.01j -436.69-0.j\n",
      " -452.36-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1247.35+1629.83j 1272.47+1662.91j 1261.32+1648.1j  1261.32+1648.1j\n",
      " 1224.54+1599.7j  1224.54+1599.7j  1216.11+1589.21j 1216.11+1589.21j\n",
      " 1272.47+1662.91j 1216.11+1589.21j 1272.47+1662.91j 1265.52+1653.75j\n",
      " 1261.32+1648.1j  1247.35+1629.83j 1237.04+1616.64j 1261.32+1648.1j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-444.26-0.j   -463.14-0.j   -457.55-0.01j -457.82-0.j   -423.48+0.02j\n",
      " -421.35+0.03j -420.56-0.j   -419.47+0.j   -463.14-0.j   -420.56-0.j\n",
      " -465.97-0.02j -460.05-0.01j -457.82-0.j   -443.01+0.01j -438.56-0.j\n",
      " -454.27-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 3, Loss: [-444.26-0.j   -463.14-0.j   -457.55-0.01j -457.82-0.j   -423.48+0.02j\n",
      " -421.35+0.03j -420.56-0.j   -419.47+0.j   -463.14-0.j   -420.56-0.j\n",
      " -465.97-0.02j -460.05-0.01j -457.82-0.j   -443.01+0.01j -438.56-0.j\n",
      " -454.27-0.j  ]\n",
      "WE   (0, 0)\t(38252119.80595116+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1252.57+1636.74j 1277.87+1670.05j 1266.6 +1655.09j 1266.6 +1655.09j\n",
      " 1229.64+1606.45j 1229.64+1606.45j 1221.19+1595.94j 1221.19+1595.94j\n",
      " 1277.87+1670.05j 1221.19+1595.94j 1277.87+1670.05j 1270.85+1660.8j\n",
      " 1266.6 +1655.09j 1252.57+1636.74j 1242.25+1623.53j 1266.6 +1655.09j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-446.13-0.j   -465.14-0.j   -459.47-0.01j -459.74-0.j   -425.24+0.02j\n",
      " -423.1 +0.03j -422.32-0.j   -421.23+0.j   -465.14-0.j   -422.32-0.j\n",
      " -468.  -0.02j -462.01-0.01j -459.74-0.j   -444.87+0.01j -440.43-0.j\n",
      " -456.18-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: [-446.13-0.j   -465.14-0.j   -459.47-0.01j -459.74-0.j   -425.24+0.02j\n",
      " -423.1 +0.03j -422.32-0.j   -421.23+0.j   -465.14-0.j   -422.32-0.j\n",
      " -468.  -0.02j -462.01-0.01j -459.74-0.j   -444.87+0.01j -440.43-0.j\n",
      " -456.18-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1257.82+1643.68j 1283.29+1677.22j 1271.91+1662.1j  1271.91+1662.1j\n",
      " 1234.77+1613.23j 1234.77+1613.23j 1226.3 +1602.69j 1226.3 +1602.69j\n",
      " 1283.29+1677.22j 1226.3 +1602.69j 1283.29+1677.22j 1276.2 +1667.88j\n",
      " 1271.91+1662.1j  1257.82+1643.68j 1247.48+1630.45j 1271.91+1662.1j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-448.  +0.j   -467.15-0.j   -461.41-0.01j -461.67-0.j   -427.01+0.02j\n",
      " -424.86+0.03j -424.09-0.j   -422.99+0.j   -467.15-0.j   -424.09-0.j\n",
      " -470.03-0.02j -463.99-0.01j -461.67-0.j   -446.74+0.01j -442.31-0.j\n",
      " -458.1 +0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 4, Loss: [-448.  +0.j   -467.15-0.j   -461.41-0.01j -461.67-0.j   -427.01+0.02j\n",
      " -424.86+0.03j -424.09-0.j   -422.99+0.j   -467.15-0.j   -424.09-0.j\n",
      " -470.03-0.02j -463.99-0.01j -461.67-0.j   -446.74+0.01j -442.31-0.j\n",
      " -458.1 +0.j  ]\n",
      "WE   (0, 0)\t(38903737.58723883+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1263.09+1650.65j 1288.74+1684.42j 1277.24+1669.15j 1277.24+1669.15j\n",
      " 1239.91+1620.04j 1239.91+1620.04j 1231.43+1609.48j 1231.43+1609.48j\n",
      " 1288.74+1684.42j 1231.43+1609.48j 1288.74+1684.42j 1281.58+1675.j\n",
      " 1277.24+1669.15j 1263.09+1650.65j 1252.73+1637.41j 1277.24+1669.15j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-449.89-0.j   -469.17-0.j   -463.35-0.01j -463.61-0.j   -428.78+0.02j\n",
      " -426.63+0.03j -425.87-0.j   -424.77+0.j   -469.17-0.j   -425.87-0.j\n",
      " -472.08-0.02j -465.97-0.01j -463.61-0.j   -448.62+0.01j -444.2 -0.j\n",
      " -460.03-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: [-449.89-0.j   -469.17-0.j   -463.35-0.01j -463.61-0.j   -428.78+0.02j\n",
      " -426.63+0.03j -425.87-0.j   -424.77+0.j   -469.17-0.j   -425.87-0.j\n",
      " -472.08-0.02j -465.97-0.01j -463.61-0.j   -448.62+0.01j -444.2 -0.j\n",
      " -460.03-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1268.38+1657.66j 1294.21+1691.66j 1282.59+1676.24j 1282.59+1676.24j\n",
      " 1245.09+1626.88j 1245.09+1626.88j 1236.58+1616.3j  1236.58+1616.3j\n",
      " 1294.21+1691.66j 1236.58+1616.3j  1294.21+1691.66j 1286.98+1682.14j\n",
      " 1282.59+1676.24j 1268.38+1657.66j 1258.01+1644.4j  1282.59+1676.24j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-451.79-0.j   -471.2 -0.j   -465.31-0.01j -465.56-0.j   -430.57+0.02j\n",
      " -428.4 +0.03j -427.66-0.j   -426.55+0.j   -471.2 -0.j   -427.66-0.j\n",
      " -474.13-0.02j -467.95-0.01j -465.56-0.j   -450.51+0.01j -446.09-0.j\n",
      " -461.97-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 5, Loss: [-451.79-0.j   -471.2 -0.j   -465.31-0.01j -465.56-0.j   -430.57+0.02j\n",
      " -428.4 +0.03j -427.66-0.j   -426.55+0.j   -471.2 -0.j   -427.66-0.j\n",
      " -474.13-0.02j -467.95-0.01j -465.56-0.j   -450.51+0.01j -446.09-0.j\n",
      " -461.97-0.j  ]\n",
      "WE   (0, 0)\t(39566948.469463736+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1273.7 +1664.7j  1299.71+1698.93j 1287.97+1683.35j 1287.97+1683.35j\n",
      " 1250.28+1633.76j 1250.28+1633.76j 1241.76+1623.15j 1241.76+1623.15j\n",
      " 1299.71+1698.93j 1241.76+1623.15j 1299.71+1698.93j 1292.41+1689.32j\n",
      " 1287.97+1683.35j 1273.7 +1664.7j  1263.32+1651.41j 1287.97+1683.35j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-453.69-0.j   -473.24-0.j   -467.27-0.01j -467.52-0.j   -432.37+0.02j\n",
      " -430.19+0.03j -429.46-0.j   -428.34+0.j   -473.24-0.j   -429.46-0.j\n",
      " -476.2 -0.02j -469.95-0.01j -467.52-0.j   -452.41+0.01j -448.  -0.j\n",
      " -463.92-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: [-453.69-0.j   -473.24-0.j   -467.27-0.01j -467.52-0.j   -432.37+0.02j\n",
      " -430.19+0.03j -429.46-0.j   -428.34+0.j   -473.24-0.j   -429.46-0.j\n",
      " -476.2 -0.02j -469.95-0.01j -467.52-0.j   -452.41+0.01j -448.  -0.j\n",
      " -463.92-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1279.05+1671.77j 1305.23+1706.23j 1293.37+1690.5j  1293.37+1690.5j\n",
      " 1255.51+1640.67j 1255.51+1640.67j 1246.96+1630.04j 1246.96+1630.04j\n",
      " 1305.23+1706.23j 1246.96+1630.04j 1305.23+1706.23j 1297.86+1696.53j\n",
      " 1293.37+1690.5j  1279.05+1671.77j 1268.65+1658.47j 1293.37+1690.5j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-455.61-0.j   -475.29-0.j   -469.24-0.01j -469.49-0.j   -434.17+0.02j\n",
      " -431.98+0.03j -431.26-0.j   -430.13+0.j   -475.29-0.j   -431.26-0.j\n",
      " -478.27-0.02j -471.96-0.01j -469.49-0.j   -454.31+0.01j -449.91-0.j\n",
      " -465.87+0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 6, Loss: [-455.61-0.j   -475.29-0.j   -469.24-0.01j -469.49-0.j   -434.17+0.02j\n",
      " -431.98+0.03j -431.26-0.j   -430.13+0.j   -475.29-0.j   -431.26-0.j\n",
      " -478.27-0.02j -471.96-0.01j -469.49-0.j   -454.31+0.01j -449.91-0.j\n",
      " -465.87+0.j  ]\n",
      "WE   (0, 0)\t(40242130.45085846+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1284.42+1678.87j 1310.78+1713.57j 1298.8 +1697.68j 1298.8 +1697.68j\n",
      " 1260.75+1647.61j 1260.75+1647.61j 1252.18+1636.95j 1252.18+1636.95j\n",
      " 1310.78+1713.57j 1252.18+1636.95j 1310.78+1713.57j 1303.34+1703.78j\n",
      " 1298.8 +1697.68j 1284.42+1678.87j 1274.  +1665.55j 1298.8 +1697.68j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-457.53-0.j   -477.35-0.j   -471.22-0.01j -471.46-0.j   -435.98+0.02j\n",
      " -433.79+0.03j -433.08-0.j   -431.94+0.j   -477.35-0.j   -433.08-0.j\n",
      " -480.35-0.02j -473.97-0.01j -471.46-0.j   -456.23+0.01j -451.84-0.j\n",
      " -467.84-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: [-457.53-0.j   -477.35-0.j   -471.22-0.01j -471.46-0.j   -435.98+0.02j\n",
      " -433.79+0.03j -433.08-0.j   -431.94+0.j   -477.35-0.j   -433.08-0.j\n",
      " -480.35-0.02j -473.97-0.01j -471.46-0.j   -456.23+0.01j -451.84-0.j\n",
      " -467.84-0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1289.81+1686.01j 1316.35+1720.95j 1304.25+1704.9j  1304.25+1704.9j\n",
      " 1266.03+1654.59j 1266.03+1654.59j 1257.44+1643.91j 1257.44+1643.91j\n",
      " 1316.35+1720.95j 1257.44+1643.91j 1316.35+1720.95j 1308.84+1711.06j\n",
      " 1304.25+1704.9j  1289.81+1686.01j 1279.38+1672.67j 1304.25+1704.9j ], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-459.47-0.j   -479.41-0.j   -473.21-0.01j -473.45-0.j   -437.8 +0.02j\n",
      " -435.6 +0.03j -434.9 -0.j   -433.76+0.j   -479.41-0.j   -434.9 -0.j\n",
      " -482.45-0.02j -476.  -0.01j -473.45-0.j   -458.16+0.01j -453.77-0.j\n",
      " -469.81-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 7, Loss: [-459.47-0.j   -479.41-0.j   -473.21-0.01j -473.45-0.j   -437.8 +0.02j\n",
      " -435.6 +0.03j -434.9 -0.j   -433.76+0.j   -479.41-0.j   -434.9 -0.j\n",
      " -482.45-0.02j -476.  -0.01j -473.45-0.j   -458.16+0.01j -453.77-0.j\n",
      " -469.81-0.j  ]\n",
      "WE   (0, 0)\t(40929705.66037305+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1295.24+1693.19j 1321.96+1728.36j 1309.73+1712.15j 1309.73+1712.15j\n",
      " 1271.33+1661.6j  1271.33+1661.6j  1262.72+1650.89j 1262.72+1650.89j\n",
      " 1321.96+1728.36j 1262.72+1650.89j 1321.96+1728.36j 1314.37+1718.38j\n",
      " 1309.73+1712.15j 1295.24+1693.19j 1284.79+1679.83j 1309.73+1712.15j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-461.41-0.j   -481.49-0.j   -475.2 -0.01j -475.44-0.j   -439.64+0.02j\n",
      " -437.42+0.03j -436.73-0.j   -435.58+0.j   -481.49-0.j   -436.73-0.j\n",
      " -484.55-0.02j -478.03-0.01j -475.44-0.j   -460.09+0.01j -455.71-0.j\n",
      " -471.8 +0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: [-461.41-0.j   -481.49-0.j   -475.2 -0.01j -475.44-0.j   -439.64+0.02j\n",
      " -437.42+0.03j -436.73-0.j   -435.58+0.j   -481.49-0.j   -436.73-0.j\n",
      " -484.55-0.02j -478.03-0.01j -475.44-0.j   -460.09+0.01j -455.71-0.j\n",
      " -471.8 +0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1300.69+1700.4j  1327.59+1735.81j 1315.24+1719.43j 1315.24+1719.43j\n",
      " 1276.65+1668.65j 1276.65+1668.65j 1268.02+1657.91j 1268.02+1657.91j\n",
      " 1327.59+1735.81j 1268.02+1657.91j 1327.59+1735.81j 1319.93+1725.73j\n",
      " 1315.24+1719.43j 1300.69+1700.4j  1290.22+1687.02j 1315.24+1719.43j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-463.36+0.j   -483.58-0.j   -477.21-0.01j -477.45-0.j   -441.48+0.02j\n",
      " -439.25+0.03j -438.57-0.j   -437.41+0.j   -483.58-0.j   -438.57-0.j\n",
      " -486.67-0.02j -480.08-0.01j -477.45-0.j   -462.04+0.01j -457.66-0.j\n",
      " -473.79-0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 8, Loss: [-463.36+0.j   -483.58-0.j   -477.21-0.01j -477.45-0.j   -441.48+0.02j\n",
      " -439.25+0.03j -438.57-0.j   -437.41+0.j   -483.58-0.j   -438.57-0.j\n",
      " -486.67-0.02j -480.08-0.01j -477.45-0.j   -462.04+0.01j -457.66-0.j\n",
      " -473.79-0.j  ]\n",
      "WE   (0, 0)\t(41629987.2574738+0j)\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1306.17+1707.65j 1333.24+1743.29j 1320.77+1726.75j 1320.77+1726.75j\n",
      " 1282.  +1675.73j 1282.  +1675.73j 1273.35+1664.97j 1273.35+1664.97j\n",
      " 1333.24+1743.29j 1273.35+1664.97j 1333.24+1743.29j 1325.52+1733.12j\n",
      " 1320.77+1726.75j 1306.17+1707.65j 1295.69+1694.25j 1320.77+1726.75j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-465.32+0.j   -485.68-0.j   -479.23-0.01j -479.46-0.j   -443.32+0.02j\n",
      " -441.09+0.03j -440.43-0.j   -439.26+0.j   -485.68-0.j   -440.43-0.j\n",
      " -488.8 -0.02j -482.13-0.01j -479.46-0.j   -463.99+0.01j -459.62-0.j\n",
      " -475.79+0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: [-465.32+0.j   -485.68-0.j   -479.23-0.01j -479.46-0.j   -443.32+0.02j\n",
      " -441.09+0.03j -440.43-0.j   -439.26+0.j   -485.68-0.j   -440.43-0.j\n",
      " -488.8 -0.02j -482.13-0.01j -479.46-0.j   -463.99+0.01j -459.62-0.j\n",
      " -475.79+0.j  ]\n",
      "output: \n",
      " SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[1311.67+1714.93j 1338.93+1750.81j 1326.33+1734.11j 1326.33+1734.11j\n",
      " 1287.38+1682.84j 1287.38+1682.84j 1278.71+1672.06j 1278.71+1672.06j\n",
      " 1338.93+1750.81j 1278.71+1672.06j 1338.93+1750.81j 1331.13+1740.54j\n",
      " 1326.33+1734.11j 1311.67+1714.93j 1301.17+1701.51j 1326.33+1734.11j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "Is it lossing: \n",
      " tf.Tensor(\n",
      "[-467.3 -0.j   -487.79-0.j   -481.26-0.01j -481.48-0.j   -445.18+0.02j\n",
      " -442.94+0.03j -442.29-0.j   -441.11+0.j   -487.79-0.j   -442.29-0.j\n",
      " -490.93-0.02j -484.2 -0.j   -481.48-0.j   -465.95+0.01j -461.6 -0.j\n",
      " -477.8 +0.j  ], shape=(16,), dtype=complex128) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Step 9, Loss: [-467.3 -0.j   -487.79-0.j   -481.26-0.01j -481.48-0.j   -445.18+0.02j\n",
      " -442.94+0.03j -442.29-0.j   -441.11+0.j   -487.79-0.j   -442.29-0.j\n",
      " -490.93-0.02j -484.2 -0.j   -481.48-0.j   -465.95+0.01j -461.6 -0.j\n",
      " -477.8 +0.j  ]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from compgraph.sparse_ham import compute_wave_function_csr,  innerprod_sparse\n",
    "from compgraph.tensor_wave_functions import adjust_dtype_and_multiply, convert_csr_to_sparse_tensor, evolving_function, variational_wave_function_on_batch\n",
    "initial_learning_rate = 7e-5\n",
    "\n",
    "#THE inner training step should work for one configuration at a time if logic has any ground on reality\n",
    "H_no_traversalfield=construct_sparse_hamiltonian(G, 0)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "optimizer_snt = snt.optimizers.Adam(initial_learning_rate,0.9)\n",
    "Hamiltonian_tensor_no_traversalfield = convert_csr_to_sparse_tensor(H_no_traversalfield)\n",
    "beta=0.005\n",
    "def evolving_function(wave, Ham_tensor,beta):\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    beta *= -1\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, beta)\n",
    "    #print(wave.indices,phi.indices)\n",
    "    \n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    #print(phi)\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    overlap=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    normalization=1/tf.math.sqrt(norm_wave*norm_ito_wave)\n",
    "    #print(normalization, 'This is the coefficient at denominator')\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    return -overlap_normalized.values\n",
    "\n",
    "def sparse_rep_inner_training_step(model, graph_batch, graph_batch_indices, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = variational_wave_function_on_batch(model, graph_batch, graph_batch_indices)\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(\"output: \\n\", output)\n",
    "            \n",
    "        loss = evolving_function(output, Hamiltonian_tensor, beta)\n",
    "            \n",
    "    #print(\"model variables: \\n\",model.trainable_variables) -< those are fine and are all tf.variables\n",
    "    print(\"Is it lossing: \\n\", loss, type(loss))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(\"are model variables and gradients two lists?\", type(model.trainable_variables), type(gradients))\n",
    "\n",
    "    #print(\"model gradients: \\n\", gradients)\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"{var.name}: Gradient {'is None' if grad is None else 'exists'}\")\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "start=0\n",
    "for step in range(10):  # IT-SWO steps\n",
    "    # Compute phi once at the beginning of each outer step, this is the ITO of psi\n",
    "\n",
    "    graph_tuples_batch=full_graph_tuples\n",
    "    graph_tuples_batch_indices= full_configurations\n",
    "\n",
    "    psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "    \n",
    "    beta = 0.05 #This parameter determines the amount of imaginary time evolution at each outer step\n",
    "    phi_csr = psi_csr - beta * Hamiltonian.dot(psi_csr)\n",
    "    print(\"WE\", innerprod_sparse(phi_csr, phi_csr))\n",
    "    for innerstep in range(2):  # Inner loop iterations: here we let psi approximate its ITO phi\n",
    "\n",
    "        outputs, loss = sparse_rep_inner_training_step(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices, optimizer_snt)\n",
    "       \n",
    "        #print(outputs,loss)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "        \n",
    "        \n",
    "    # Update the start index for the next batch\n",
    "    start += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tf.Tensor((1+0j), shape=(), dtype=complex128) \n",
      " tf.Tensor((1+0j), shape=(), dtype=complex128)\n",
      "tf.Tensor(\n",
      "[-0.01-0.02j -0.  -0.03j -0.  -0.03j -0.  -0.03j -0.  -0.03j -0.  -0.03j\n",
      " -0.  -0.06j  0.  -0.02j -0.  -0.03j -0.  -0.06j -0.  -0.03j  0.  -0.02j\n",
      " -0.  -0.03j  0.  -0.02j  0.  -0.02j  0.  -0.01j], shape=(16,), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from compgraph.tensor_wave_functions import time_evoluted_wave_function_on_batch,variational_wave_function_on_batch\n",
    "\n",
    "\n",
    "a=time_evoluted_wave_function_on_batch(less_trivial_gnn, 0.05, full_graph_tuples, G, sublattice_encoding)\n",
    "b=variational_wave_function_on_batch(less_trivial_gnn, full_graph_tuples,full_configurations)\n",
    "norm_b=np.array(b.values)/tf.norm(b.values)\n",
    "norm_a=np.array(a.values)/tf.norm(a.values)\n",
    "\n",
    "print(tf.norm(norm_b), \"\\n\", tf.norm(norm_a))\n",
    "print(a.values-b.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(wave, Ham_tensor,beta):\n",
    "\n",
    "    wave=tf.sparse.reorder(wave)\n",
    "\n",
    "    auxphi= adjust_dtype_and_multiply(Ham_tensor,wave)\n",
    "    beta *= -1\n",
    "    phi=tf.sparse.map_values(tf.multiply,auxphi, beta)\n",
    "    phi=tf.sparse.add(wave,tf.stop_gradient(phi))\n",
    "    wave_with_0=tf.sparse.map_values(tf.multiply,phi, 0)\n",
    "    wave=tf.sparse.add(wave,wave_with_0)\n",
    "    psi_conj= tf.sparse.map_values(tf.math.conj, wave)\n",
    "    norm_wave = tf.norm(wave.values)\n",
    "    norm_ito_wave=tf.norm(phi.values)\n",
    "    norm_wave = 1/norm_wave\n",
    "    norm_ito_wave= 1/norm_ito_wave\n",
    "    normalized_wave=tf.sparse.map_values(tf.multiply,psi_conj,norm_wave)\n",
    "    normalized_ito_wave=tf.sparse.map_values(tf.multiply,phi,norm_ito_wave)\n",
    "    print('Are they normalized', tf.norm(normalized_wave.values), normalized_wave)\n",
    "    print(tf.norm(normalized_ito_wave.values), normalized_ito_wave)\n",
    "    normalization_factor=norm_wave*norm_ito_wave\n",
    "    overlap_n=tf.sparse.map_values(tf.multiply,psi_conj,phi)\n",
    "    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap_n,normalization_factor)\n",
    "\n",
    "    overlap=tf.sparse.map_values(tf.multiply,normalized_wave,normalized_ito_wave)\n",
    "    print('Final overlap', tf.norm(overlap.values),tf.norm(overlap_normalized.values))\n",
    "    overlap2= tf.sparse.map_values(tf.multiply, normalized_wave, normalized_ito_wave)\n",
    "    print(overlap, overlap2)\n",
    "#    print('wave norms', norm_wave, norm_ito_wave)\n",
    "#    normalization=1/((norm_wave*norm_ito_wave))\n",
    "    \n",
    "    #print(normalization, 'This is the coefficient at denominator')\n",
    "#    overlap_normalized=tf.sparse.map_values(tf.multiply,overlap,normalization)\n",
    "    #TODO NORMALIZATION check\n",
    "#    print('overlap', tf.norm(overlap.values)*normalization, norm_ito_wave*norm_wave)\n",
    "#    print(tf.norm(overlap_normalized.values), 'This should be 1')\n",
    "    return -overlap.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "beta 0.05\n",
      "  (0, 0)\t(0.02789507433772087+0.2535555064678192j)\n",
      "  (1, 0)\t(-0.019511831924319267+0.16419753432273865j)\n",
      "  (2, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (3, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (4, 0)\t(0.14807215332984924+0.2852272391319275j)\n",
      "  (5, 0)\t(0.14807215332984924+0.2852272391319275j)\n",
      "  (6, 0)\t(-0.0013693792279809713+0.26433518528938293j)\n",
      "  (7, 0)\t(-0.0013693792279809713+0.26433518528938293j)\n",
      "  (8, 0)\t(-0.019511831924319267+0.16419753432273865j)\n",
      "  (9, 0)\t(-0.0013693792279809713+0.26433518528938293j)\n",
      "  (10, 0)\t(-0.019511831924319267+0.16419753432273865j)\n",
      "  (11, 0)\t(-0.002699091797694564+0.28862276673316956j)\n",
      "  (12, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (13, 0)\t(0.02789507433772087+0.2535555064678192j)\n",
      "  (14, 0)\t(-0.020448261871933937+0.13112111389636993j)\n",
      "  (15, 0)\t(0.018267717212438583+0.24162350594997406j)\n",
      "  (0, 0)\t(0.020921305753290653+0.1901666298508644j)\n",
      "  (1, 0)\t(-0.019767962303012608+0.11818675883114338j)\n",
      "  (2, 0)\t(-0.010371121857315303+0.17636818140745164j)\n",
      "  (3, 0)\t(-0.003722334664780648+0.1868464931845665j)\n",
      "  (4, 0)\t(0.14539420148357748+0.22869266122579573j)\n",
      "  (5, 0)\t(0.17044796607224272+0.23240652531385422j)\n",
      "  (6, 0)\t(-0.005291366239544004+0.20136811286211015j)\n",
      "  (7, 0)\t(0.0020903735829051586+0.22455650568008423j)\n",
      "  (8, 0)\t(-0.019767962303012608+0.11818675883114338j)\n",
      "  (9, 0)\t(-0.005291366239544004+0.20136811286211015j)\n",
      "  (10, 0)\t(-0.030652816232759506+0.0871708795428276j)\n",
      "  (11, 0)\t(-0.007732665637740866+0.2280252579599619j)\n",
      "  (12, 0)\t(-0.003722334664780648+0.1868464931845665j)\n",
      "  (13, 0)\t(0.028980333724757657+0.18594454564154148j)\n",
      "  (14, 0)\t(-0.0208042855898384+0.06469962000846863j)\n",
      "  (15, 0)\t(0.013700787909328938+0.18121762946248055j)\n",
      "overlap tf.Tensor(\n",
      "[-0.07-0.j -0.03-0.j -0.06-0.j -0.06-0.j -0.11-0.j -0.11-0.j -0.07-0.j\n",
      " -0.07-0.j -0.03-0.j -0.07-0.j -0.03-0.j -0.08-0.j -0.06-0.j -0.07-0.j\n",
      " -0.02-0.j -0.06-0.j], shape=(16,), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "# First, load the autoreload extension if it hasn't been loaded already\n",
    "%load_ext autoreload\n",
    "# Set autoreload to reload modules automatically before executing code\n",
    "%autoreload 2\n",
    "print('beta', beta)\n",
    "\n",
    "\n",
    "psi_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "print(psi_csr)\n",
    "phi_csr = psi_csr - beta*Hamiltonian.dot(psi_csr)\n",
    "print(phi_csr)\n",
    "output = variational_wave_function_on_batch(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices)\n",
    "            \n",
    "overlap = evolving_function(output, Hamiltonian_tensor, 0)\n",
    "print(\"overlap\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[-3568114.08-4721685.35j -3842670.11-5085005.22j -3580321.28-4737839.14j\n",
      " -3580321.28-4737839.14j -3423218.55-4529945.15j -3423218.55-4529945.15j\n",
      " -3434165.18-4544430.83j -3434165.18-4544430.83j -3842670.11-5085005.22j\n",
      " -3434165.18-4544430.83j -3842670.11-5085005.22j -3703905.98-4901378.69j\n",
      " -3580321.28-4737839.14j -3568114.08-4721685.35j -3585136.52-4744211.15j\n",
      " -3580321.28-4737839.14j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64)) SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[67794167.47+89712021.62j 73010732.16+96615099.19j\n",
      " 68026104.4 +90018943.74j 68026104.4 +90018943.74j\n",
      " 65041152.37+86068957.9j  65041152.37+86068957.9j\n",
      " 65249138.39+86344185.77j 65249138.39+86344185.77j\n",
      " 73010732.16+96615099.19j 65249138.39+86344185.77j\n",
      " 73010732.16+96615099.19j 70374213.53+93126195.19j\n",
      " 68026104.4 +90018943.74j 67794167.47+89712021.62j\n",
      " 68117593.95+90140011.9j  68026104.4 +90018943.74j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64)) SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[-3.63e+15+1.28e+16j -4.21e+15+1.49e+16j -3.66e+15+1.29e+16j\n",
      " -3.66e+15+1.29e+16j -3.34e+15+1.18e+16j -3.34e+15+1.18e+16j\n",
      " -3.37e+15+1.19e+16j -3.37e+15+1.19e+16j -4.21e+15+1.49e+16j\n",
      " -3.37e+15+1.19e+16j -4.21e+15+1.49e+16j -3.92e+15+1.38e+16j\n",
      " -3.66e+15+1.29e+16j -3.63e+15+1.28e+16j -3.67e+15+1.29e+16j\n",
      " -3.66e+15+1.29e+16j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[-3.63e+15+1.28e+16j -4.21e+15+1.49e+16j -3.66e+15+1.29e+16j\n",
      " -3.66e+15+1.29e+16j -3.34e+15+1.18e+16j -3.34e+15+1.18e+16j\n",
      " -3.37e+15+1.19e+16j -3.37e+15+1.19e+16j -4.21e+15+1.49e+16j\n",
      " -3.37e+15+1.19e+16j -4.21e+15+1.49e+16j -3.92e+15+1.38e+16j\n",
      " -3.66e+15+1.29e+16j -3.63e+15+1.28e+16j -3.67e+15+1.29e+16j\n",
      " -3.66e+15+1.29e+16j], shape=(16,), dtype=complex128)\n",
      "tf.Tensor(\n",
      "[1.40e+16+0.j 1.62e+16+0.j 1.41e+16+0.j 1.41e+16+0.j 1.29e+16+0.j\n",
      " 1.29e+16+0.j 1.30e+16+0.j 1.30e+16+0.j 1.62e+16+0.j 1.30e+16+0.j\n",
      " 1.62e+16+0.j 1.51e+16+0.j 1.41e+16+0.j 1.40e+16+0.j 1.41e+16+0.j\n",
      " 1.41e+16+0.j], shape=(16,), dtype=complex128)\n",
      "  (0, 0)\t(356811400+472168520j)\n",
      "  (1, 0)\t(377448996+499478260j)\n",
      "  (2, 0)\t(350710872+464095720j)\n",
      "  (3, 0)\t(363528680+481057512j)\n",
      "  (4, 0)\t(363279096+480727240j)\n",
      "  (5, 0)\t(367677224+486547272j)\n",
      "  (6, 0)\t(350315220+463572136j)\n",
      "  (7, 0)\t(359531112+475767536j)\n",
      "  (8, 0)\t(377448996+499478260j)\n",
      "  (9, 0)\t(350315220+463572136j)\n",
      "  (10, 0)\t(334121096+442142472j)\n",
      "  (11, 0)\t(355642136+470621236j)\n",
      "  (12, 0)\t(363528680+481057512j)\n",
      "  (13, 0)\t(366505496+484996724j)\n",
      "  (14, 0)\t(347453416+459785104j)\n",
      "  (15, 0)\t(358032120+473783920j)\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64), values=tf.Tensor(\n",
      "[3.57e+08+4.72e+08j 3.77e+08+4.99e+08j 3.51e+08+4.64e+08j\n",
      " 3.64e+08+4.81e+08j 3.63e+08+4.81e+08j 3.68e+08+4.87e+08j\n",
      " 3.50e+08+4.64e+08j 3.60e+08+4.76e+08j 3.77e+08+4.99e+08j\n",
      " 3.50e+08+4.64e+08j 3.34e+08+4.42e+08j 3.56e+08+4.71e+08j\n",
      " 3.64e+08+4.81e+08j 3.67e+08+4.85e+08j 3.47e+08+4.60e+08j\n",
      " 3.58e+08+4.74e+08j], shape=(16,), dtype=complex128), dense_shape=tf.Tensor([16  1], shape=(2,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[[ 0  0]\n",
      " [ 1  0]\n",
      " [ 2  0]\n",
      " [ 3  0]\n",
      " [ 4  0]\n",
      " [ 5  0]\n",
      " [ 6  0]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  0]\n",
      " [10  0]\n",
      " [11  0]\n",
      " [12  0]\n",
      " [13  0]\n",
      " [14  0]\n",
      " [15  0]], shape=(16, 2), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 7.71+14.82j 11.41 +0.6j  13.96 -3.39j 13.63 +2.9j   9.01 -4.06j\n",
      "  1.35 -4.52j  2.98 +0.42j 10.42 -3.39j 11.41 +0.6j   2.98 +0.42j\n",
      "  3.92 -9.96j  7.19+11.68j 13.63 +2.9j  -0.94 -8.64j -1.21 +2.84j\n",
      "  8.41 -5.58j], shape=(16,), dtype=complex128)\n"
     ]
    }
   ],
   "source": [
    "import compgraph\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "indexing= np.column_stack((Hamiltonian.nonzero()))\n",
    "values_H= Hamiltonian.data\n",
    "shape_H = Hamiltonian.shape\n",
    "H_tensor= tf.sparse.SparseTensor(indexing,values_H,shape_H)\n",
    "H_tensor = tf.sparse.reorder(H_tensor)\n",
    "H2_tensor = convert_csr_to_sparse_tensor(Hamiltonian)\n",
    "output=  variational_wave_function_on_batch(less_trivial_gnn, graph_tuples_batch, graph_tuples_batch_indices)\n",
    "output_csr = compute_wave_function_csr(graph_tuples_batch, less_trivial_gnn, graph_tuples_batch_indices)\n",
    "auxphi=tf.sparse.map_values(tf.multiply,output, -0.05)\n",
    "auxphi2=tf.sparse.map_values(tf.add,output,auxphi)\n",
    "prod=tf.sparse.map_values(tf.multiply,output,auxphi2)\n",
    "print(auxphi, auxphi2,prod) \n",
    "print(prod.values[:])\n",
    "conj_output = tf.sparse.map_values(tf.math.conj, output)\n",
    "prod=tf.sparse.map_values(tf.multiply,conj_output,output)\n",
    "print(prod.values[:])\n",
    "\n",
    "phi_csr = Hamiltonian.dot(output_csr)\n",
    "print(phi_csr)\n",
    "phi_sparse_coo = phi_csr.tocoo()\n",
    "indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex128)\n",
    "print(phi_sparse_tf)\n",
    "\n",
    "phi= adjust_dtype_and_multiply(H2_tensor,tf.sparse.reorder(output))\n",
    "\n",
    "print(tf.sparse.reorder(output).indices)\n",
    "print(phi.values-phi_sparse_tf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t(0.2+0j)\n",
      "  (1, 0)\t(0.6+0.5j)\n",
      "  (2, 0)\t(0.8+0.7j) \n",
      " ARTURO [[2.+0.j  0.+0.j  1.+0.2j 0.+0.j ]\n",
      " [0.+0.j  0.+0.j  3.+0.j  0.+0.j ]\n",
      " [4.+0.j  5.+0.j  6.+0.j  0.+0.j ]\n",
      " [0.+0.j  0.+0.j  0.+0.j  0.+0.j ]]\n",
      "  (0, 0)\t(1.06+0.86j)\n",
      "  (1, 0)\t(2.4000000000000004+2.0999999999999996j)\n",
      "  (2, 0)\t(8.600000000000001+6.699999999999999j)\n",
      "[[2.+0.j  0.+0.j  1.+0.2j 0.+0.j ]\n",
      " [0.+0.j  0.+0.j  3.+0.j  0.+0.j ]\n",
      " [4.+0.j  5.+0.j  6.+0.j  0.+0.j ]\n",
      " [0.+0.j  0.+0.j  0.+0.j  0.+0.j ]]\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [2 0]], shape=(3, 2), dtype=int64), values=tf.Tensor([0.2+0.j  0.6+0.5j 0.8+0.7j], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([4 1], shape=(2,), dtype=int64))\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [2 0]], shape=(3, 2), dtype=int64), values=tf.Tensor([1.06+0.86j 2.4 +2.1j  8.6 +6.7j ], shape=(3,), dtype=complex128), dense_shape=tf.Tensor([4 1], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([2, 0, 2, 0, 1, 2])\n",
    "data = np.array([1+ 0.2j, 2, 3, 4, 5, 6])\n",
    "arturo=csr_matrix((data, (row, col)), shape=(4, 4))\n",
    "data = [0.2, 0.6+0.5j, 0.8+0.7j ]  # List to store the non-zero entries\n",
    "row_indices = [0,1,2]  # List to store the row indices\n",
    "col_indices = [0]*3\n",
    "gomez=csr_matrix((data, (row_indices, col_indices)), shape=(4, 1))\n",
    "print(gomez,'\\n ARTURO',arturo.toarray())\n",
    "print(arturo.dot(gomez))\n",
    "indexing= np.column_stack((arturo.nonzero()))\n",
    "values_arturo= arturo.data\n",
    "shape_arturo = arturo.shape\n",
    "arturo_tensor= tf.sparse.SparseTensor(indexing,values_arturo,shape_arturo)\n",
    "arturo_tensor = tf.sparse.reorder(arturo_tensor)\n",
    "arturo_tensor = convert_csr_to_sparse_tensor(arturo)\n",
    "print(arturo_tensor._numpy())\n",
    "phi_sparse_coo = gomez.tocoo()\n",
    "indices = np.column_stack((phi_sparse_coo.row, phi_sparse_coo.col))\n",
    "phi_sparse_tf = tf.cast(tf.sparse.SparseTensor(indices, phi_sparse_coo.data, phi_sparse_coo.shape), dtype=tf.complex128)\n",
    "print(phi_sparse_tf)\n",
    "out_tensor = adjust_dtype_and_multiply(arturo_tensor,phi_sparse_tf)\n",
    "print(out_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 In the previous section we managed to perform all the operations we wanted to do while preserving the gradient. Hurra! \n",
    "In this section we will include the montecarlo update and try to stabilize the parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnets2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
